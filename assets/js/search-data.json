{
  
    
        "post0": {
            "title": "Geospatial_05_exercise",
            "content": "import math import geopandas as gpd import pandas as pd from shapely.geometry import MultiPolygon import folium from folium import Choropleth, Marker from folium.plugins import HeatMap, MarkerCluster from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex5 import * . embed_map() 함수를 사용하여 지도를 시각화할 수 있습니다. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . 1) &#52649;&#46028; &#45936;&#51060;&#53552;&#47484; &#49884;&#44033;&#54868;&#54633;&#45768;&#45796;. . 아래 코드 셀을 실행하여 2013-2018년 주요 자동차 충돌을 추적하는 GeoDataFrame &quot;충돌&quot;을 로드하십시오. . collisions = gpd.read_file(&quot;data/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp&quot;) collisions.head() . DATE TIME BOROUGH ZIP CODE LATITUDE LONGITUDE LOCATION ON STREET CROSS STRE OFF STREET ... CONTRIBU_2 CONTRIBU_3 CONTRIBU_4 UNIQUE KEY VEHICLE TY VEHICLE _1 VEHICLE _2 VEHICLE _3 VEHICLE _4 geometry . 0 07/30/2019 | 0:00 | BRONX | 10464 | 40.841100 | -73.784960 | (40.8411, -73.78496) | None | None | 121 PILOT STREET | ... | Unspecified | None | None | 4180045 | Sedan | Station Wagon/Sport Utility Vehicle | Station Wagon/Sport Utility Vehicle | None | None | POINT (1043750.211 245785.815) | . 1 07/30/2019 | 0:10 | QUEENS | 11423 | 40.710827 | -73.770660 | (40.710827, -73.77066) | JAMAICA AVENUE | 188 STREET | None | ... | None | None | None | 4180007 | Sedan | Sedan | None | None | None | POINT (1047831.185 198333.171) | . 2 07/30/2019 | 0:25 | None | None | 40.880318 | -73.841286 | (40.880318, -73.841286) | BOSTON ROAD | None | None | ... | None | None | None | 4179575 | Sedan | Station Wagon/Sport Utility Vehicle | None | None | None | POINT (1028139.293 260041.178) | . 3 07/30/2019 | 0:35 | MANHATTAN | 10036 | 40.756744 | -73.984590 | (40.756744, -73.98459) | None | None | 155 WEST 44 STREET | ... | None | None | None | 4179544 | Box Truck | Station Wagon/Sport Utility Vehicle | None | None | None | POINT (988519.261 214979.320) | . 4 07/30/2019 | 10:00 | BROOKLYN | 11223 | 40.600090 | -73.965910 | (40.60009, -73.96591) | AVENUE T | OCEAN PARKWAY | None | ... | None | None | None | 4180660 | Station Wagon/Sport Utility Vehicle | Bike | None | None | None | POINT (993716.669 157907.212) | . 5 rows × 30 columns . &quot;LATITUDE&quot; 및 &quot;LONGITUDE&quot; 열을 사용하여 충돌 데이터를 시각화하는 대화형 맵을 만듭니다. 어떤 종류의 지도가 가장 효과적이라고 생각하세요? . m_1 = folium.Map(location=[40.7, -74], zoom_start=11) # Your code here: Visualize the collision data HeatMap(data=collisions[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m_1) # Get credit for your work after you have created a map q_1.check() # Show the map m_1 . Thank you for creating a map! . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 2) &#48337;&#50896;&#51032; &#51201;&#50857; &#48276;&#50948;&#47484; &#51060;&#54644;&#54633;&#45768;&#45796;. . 다음 코드 셀을 실행하여 병원 데이터를 로드합니다. . hospitals = gpd.read_file(&quot;data/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp&quot;) hospitals.head() . id name address zip factype facname capacity capname bcode xcoord ycoord latitude longitude geometry . 0 317000001H1178 | BRONX-LEBANON HOSPITAL CENTER - CONCOURSE DIVI... | 1650 Grand Concourse | 10457 | 3102 | Hospital | 415 | Beds | 36005 | 1008872.0 | 246596.0 | 40.843490 | -73.911010 | POINT (1008872.000 246596.000) | . 1 317000001H1164 | BRONX-LEBANON HOSPITAL CENTER - FULTON DIVISION | 1276 Fulton Ave | 10456 | 3102 | Hospital | 164 | Beds | 36005 | 1011044.0 | 242204.0 | 40.831429 | -73.903178 | POINT (1011044.000 242204.000) | . 2 317000011H1175 | CALVARY HOSPITAL INC | 1740-70 Eastchester Rd | 10461 | 3102 | Hospital | 225 | Beds | 36005 | 1027505.0 | 248287.0 | 40.848060 | -73.843656 | POINT (1027505.000 248287.000) | . 3 317000002H1165 | JACOBI MEDICAL CENTER | 1400 Pelham Pkwy | 10461 | 3102 | Hospital | 457 | Beds | 36005 | 1027042.0 | 251065.0 | 40.855687 | -73.845311 | POINT (1027042.000 251065.000) | . 4 317000008H1172 | LINCOLN MEDICAL &amp; MENTAL HEALTH CENTER | 234 E 149 St | 10451 | 3102 | Hospital | 362 | Beds | 36005 | 1005154.0 | 236853.0 | 40.816758 | -73.924478 | POINT (1005154.000 236853.000) | . &quot;위도&quot; 및 &quot;경도&quot; 열을 사용하여 병원 위치를 시각화합니다. . m_2 = folium.Map(location=[40.7, -74], zoom_start=11) # Your code here: Visualize the hospital locations for idx, row in hospitals.iterrows(): Marker([row[&#39;latitude&#39;], row[&#39;longitude&#39;]], popup=row[&#39;name&#39;]).add_to(m_2) # Get credit for your work after you have created a map q_2.check() # Show the map m_2 . Thank you for creating a map! . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 3) &#44032;&#51109; &#44032;&#44620;&#50868; &#48337;&#50896;&#51008; &#50616;&#51228; 10&#53420;&#47196;&#48120;&#53552; &#51060;&#49345; &#46504;&#50612;&#51256; &#51080;&#50632;&#45208;&#50836;? . 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌과 &#39;충돌&#39;의 모든 행을 포함하는 DataFrame &#39;outside_range&#39;를 만듭니다. . 병원과 충돌은 모두 EPSG 2263을 좌표계로 하고 EPSG 2263은 미터 단위를 가진다. . coverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000) my_union = coverage.geometry.unary_union outside_range = collisions.loc[~collisions[&quot;geometry&quot;].apply(lambda x: my_union.contains(x))] # Check your answer q_3.check() . Correct . 다음 코드 셀은 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌 비율을 계산합니다. . percentage = round(100*len(outside_range)/len(collisions), 2) print(&quot;Percentage of collisions more than 10 km away from the closest hospital: {}%&quot;.format(percentage)) . Percentage of collisions more than 10 km away from the closest hospital: 15.12% . 4) &#52628;&#52380;&#51088;&#47484; &#47564;&#46308;&#50612;&#46972;. . 먼 곳에서 충돌이 일어날 때, 부상자들이 가장 가까운 병원으로 이송되는 것이 훨씬 더 중요해진다. . 이 점을 고려하여 다음과 같은 권장 사항을 작성하기로 결정했습니다. . 충돌 위치(EPSG 2263)를 입력으로 사용합니다. | (EPSG 2263에서 거리 계산이 수행되는) 가장 가까운 병원을 찾는다. | 가장 가까운 병원의 이름을 반환합니다. | . def best_hospital(collision_location): idx_min = hospitals.geometry.distance(collision_location).idxmin() my_hospital = hospitals.iloc[idx_min] name = my_hospital[&quot;name&quot;] return name # Test your function: this should suggest CALVARY HOSPITAL INC print(best_hospital(outside_range.geometry.iloc[0])) # Check your answer q_4.check() . CALVARY HOSPITAL INC . Correct . 5) &#49688;&#50836;&#44032; &#44032;&#51109; &#47566;&#51008; &#48337;&#50896;&#51008; &#50612;&#46356;&#51077;&#45768;&#44620;? . outside_range 데이터 프레임의 충돌만 고려한다면 어느 병원을 가장 추천합니까? . 답변은 4)에서 만든 함수에 의해 반환된 병원 이름과 정확히 일치하는 Python 문자열이어야 합니다. . highest_demand = outside_range.geometry.apply(best_hospital).value_counts().idxmax() # Check your answer q_5.check() . Correct . 6) &#49884;&#45716; &#50612;&#46356;&#50640; &#49352; &#48337;&#50896;&#51012; &#51648;&#50612;&#50556; &#54616;&#45716;&#44032;? . 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌 외에도 다음 코드 셀(변경 사항 없이)을 실행하여 병원 위치를 시각화합니다. . m_6 = folium.Map(location=[40.7, -74], zoom_start=11) coverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000) folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6) HeatMap(data=outside_range[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m_6) folium.LatLngPopup().add_to(m_6) m_6 . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 지도의 아무 곳이나 클릭하면 해당 위치가 위도 및 경도로 표시된 팝업이 나타납니다. . 뉴욕시는 두 개의 새로운 병원을 위한 장소를 결정하는 데 도움을 요청했습니다. 이들은 특히 *3) 단계에서 계산된 백분율을 10% 미만으로 낮추기 위해 위치를 식별하는 데 도움을 필요로 합니다. 지도를 사용하여 (병원을 짓기 위해 구역법이나 어떤 잠재적인 건물을 제거해야 하는지에 대한 걱정 없이) 도시가 이 목표를 달성하는 데 도움이 되는 두 개의 위치를 식별할 수 있습니까? . 병원 1에 대해 제안된 위도와 경도를 각각 lat_1과 long_1에 넣는다. (병원 2에 대해서도 마찬가지로) . 그런 다음, 새 병원의 효과를 보기 위해 나머지 세포를 그대로 가동합니다. 두 개의 새 병원이 10% 미만으로 비율을 낮추면 정답으로 표시됩니다. . lat_1 = 40.6714 long_1 = -73.8492 # Your answer here: proposed location of hospital 2 lat_2 = 40.6702 long_2 = -73.7612 # Do not modify the code below this line try: new_df = pd.DataFrame( {&#39;Latitude&#39;: [lat_1, lat_2], &#39;Longitude&#39;: [long_1, long_2]}) new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude)) new_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} new_gdf = new_gdf.to_crs(epsg=2263) # get new percentage new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000) new_my_union = new_coverage.geometry.unary_union new_outside_range = outside_range.loc[~outside_range[&quot;geometry&quot;].apply(lambda x: new_my_union.contains(x))] new_percentage = round(100*len(new_outside_range)/len(collisions), 2) print(&quot;(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%&quot;.format(new_percentage)) # Did you help the city to meet its goal? q_6.check() # make the map m = folium.Map(location=[40.7, -74], zoom_start=11) folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m) folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m) for idx, row in new_gdf.iterrows(): Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]]).add_to(m) HeatMap(data=new_outside_range[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m) folium.LatLngPopup().add_to(m) display(m) except: q_6.hint() . c: Users User anaconda3 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . (NEW) Percentage of collisions more than 10 km away from the closest hospital: 9.12% . Correct . Make this Notebook Trusted to load map: File -&gt; Trust Notebook Congratulations! . You have just completed the Geospatial Analysis micro-course! Great job! .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/2022/05/28/_Geospatial05.html",
            "relUrl": "/2022/05/28/_Geospatial05.html",
            "date": " • May 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Python 기본",
            "content": "Python Language Basics . Python Language Basics . Language Semantics . &#51473;&#44292;&#54840; &#50500;&#45772; &#46308;&#50668;&#50416;&#44592; . for x in array: if x &lt; privot: less.append(x) else: greater.append(x) . a = 5; b = 6; c = 7 . c . 7 . &#54632;&#49688; &#48143; &#44061;&#52404; method &#54840;&#52636; . 괄호를 사용하여 함수를 호출하고 0 이상의 인수를 전달합니다.옵션으로 반환된 값을 변수에 할당합니다. . result = f(x,y,z) g() . Python의 거의 모든 Object에는 Object의 내부 콘텐츠에 접근할 수 있는 Method라고 불리는 부가 함수가 있습니다. 다음 구문을 사용하여 호출할 수 있습니다. . obj.some_method(x,y,z) . &#48320;&#49688; &#48143; &#51064;&#49688; &#51204;&#45804; . a = [1,2,3] . b = a . a.append(4) # a라는 object에 4를 추가한다. #append는 a라는 object가 가지고 있는 함수 b . [1, 2, 3, 4] . &#45936;&#51060;&#53552; &#53440;&#51077; . a = 5 type(a) . int . a = &#39;foo&#39; type(a) . str . a = 4.5 type(a) . float . type(a) . float . Attributes and methods . python의 object는 일반적으로 attributes(다른 python object가 object를 &quot;저장&quot;하고 있는 것)과 method(&#39;object의 내부 데이터에 접근할 수 있는 objcet와 관련된 것)를 모두 가지고 있습니다. . a = &#39;foo&#39; . #string type으로 객체를 할당하면 .을 찍으면 사용할 수 있는 함수들이 나온다. . a.capitalize() #capitalize() : 문자열 str의 첫 번째 알파벳을 대문자로 바꾸고, 다른 알파벳은 모두 소문자로 변경한다. . &#39;Foo&#39; . a.upper() #upper() : 문자열 str의 모든 알파벳을 대문자로 변경한다. . &#39;FOO&#39; . Duck Typing . def isiterable(obj): try: iter(obj) return True except TypeError: #not iterable return False . isiterable(&#39;a string&#39;) . True . isiterable([1,2,3]) . True . Imports . Python에서 모듈은 단순히 Python 코드를 포함하는 .py 확장자를 가진 파일입니다. 다음 모듈이 있다고 가정합니다. . ##### some_module.py ##### PI = 3.14159 def f(x): return x + 2 def g(a,b): return a + b . 같은 디렉토리에 있는 다른 파일에서 some_moduel.py에 정의된 변수와 함수에 액세스하려면 다음을 수행합니다. . import some_module result = some_module.f(5) result . 7 . pi = some_module.PI #some_module에 ctrl을 누르면 some_module에 정의된 변수와 함수를 볼 수 있음 pi . 3.14159 . 매번 앞에 some_module을 쓰는 게 힘들 때 함수를 바로 불러오는 법 . from some_module import f, g, PI #some_module에서 direct로 f,g함수와 PI변수를 input 해줌 result = g(5,PI) result . 8.14159 . as 키워드를 사용하면 모듈이름과, 함수와 변수의 이름들을 간단하게 줄일 수 있음 . ex . import pandas ad pd . import some_module as sm from some_module import PI as pi, g as gf r1 = sm.f(pi) r2 = gf(6,pi) . r1 . 5.14159 . r2 . 9.14159 . &#48320;&#44221; &#44032;&#45733;&#54620; object&#50752; &#48520;&#44032;&#45733;&#54620; object . lists, dicts, NumPy arrays 및 대부분의 사용자 정의 유형(classes)과 같은 Python의 대부분의 object는 변경 가능합니다. 즉, 포함된 object 또는 값(value)을 변경할 수 있습니다. . a_list = [&#39;foo&#39;,2,[4,5]] a_list[2] = (3,4) a_list . [&#39;foo&#39;, 2, (3, 4)] . 반면에, 문자열과 tuple은 변경 불가능합니다. 단, 속도는 빠름 . Tuple . # a_tuple[1] = &#39;four&#39; . String . # a[10] = &#39;f&#39; # #string이기 때문에 변경 불가능 . b = a.replace(&#39;string&#39;,&#39;longer string&#39;) b . &#39;this is a longer string&#39; . 문자열은 일련의 유니코드 문자이므로 list나 tuple 등 다른 시퀀스와 동일하게 취급할 수 있습니다 . s = &#39;python&#39; list(s) . [&#39;p&#39;, &#39;y&#39;, &#39;t&#39;, &#39;h&#39;, &#39;o&#39;, &#39;n&#39;] . s[:3] . &#39;pyt&#39; . 두 개의 문자열을 함께 추가하여 그것들을 연결하고 새로운 문자열을 생성합니다. . a = &#39;this is the first half&#39; b = &#39;and this is the second half&#39; a + b . &#39;this is the first halfand this is the second half&#39; . file_dir = &#39;G:/tmp/&#39; file_name = &#39;test.csv&#39; file_dir + file_name . &#39;G:/tmp/test.csv&#39; . 문자열 object에는 포맷된 인수를 문자열로 대체하여 새로운 문자열을 생성하기 위해 사용할 수 있는 format method가 있습니다. . template = &#39;{0:.2f} {1:s} are worth US${2:d}&#39; template . &#39;{0:.2f} {1:s} are worth US${2:d}&#39; . {0:2f} 는 첫 번째 인수의 형식을 소수점 2자리 부동 소수점 숫자로 지정합니다. | {1:s} 는 두 번째 인수의 형식을 문자열로 지정합니다. | {2:d} 는 세 번째 인수의 형식을 정확한 정수로 지정합니다. | . template.format(4.5560, &#39;Argentine Pesos&#39;,1) . &#39;4.56 Argentine Pesos are worth US$1&#39; . template.format(1263.23,&#39;won&#39;,1) . &#39;1263.23 won are worth US$1&#39; . None . Python에서 변수에 아무 값도 넣고 싶지 않을 때 . a = None a is None . True . a #값은 없지만 object로 할당은 되었음 . None은 함수 인수의 공통 기본값이기도 합니다. . def add_and_maybe_multiply(a,b,c=None): result = a + b if c is not None: result = result * c return result . add_and_maybe_multiply(5,3) . 8 . add_and_maybe_multiply(5,3,10) . 80 . &#45216;&#51676;&#50752; &#49884;&#44036; . 내장된 Python datetime 모듈은 datetime, date 및 time types을 제공합니다. datetime 유형은 예상대로 날짜와 시간에 저장된 정보를 조합하여 가장 일반적으로 사용됩니다. . from datetime import datetime, date, time dt = datetime(2011,10,29,20,30,21) dt . datetime.datetime(2011, 10, 29, 20, 30, 21) . dt.day . 29 . datetime 인스턴스를 지정하면 동일한 이름의 datetime method를 호출하여 동일한 날짜 및 시간 개체를 추출할 수 있습니다. . dt.date() . datetime.date(2011, 10, 29) . dt.time() . datetime.time(20, 30, 21) . strftime 메서드는 datetime을 string으로 포맷합니다. . dt.strftime(&#39;%m %d %Y %H : %M&#39;) . &#39;10 29 2011 20 : 30&#39; . dt.strftime(&#39;%Y/%m/%d %H:%M&#39;) . &#39;2011/10/29 20:30&#39; . strptime method는 datetime을 string으로 포맷합니다. . datetime.strptime(&#39;20091031&#39;,&#39;%Y%m%d&#39;) . datetime.datetime(2009, 10, 31, 0, 0) . 시계열 데이터를 집계하거나 그룹화할 때, 분 및 초 field를 0으로 바꾸는 것과 같이 일련의 데이터 시간 field를 바꾸는 것이 유용할 수 있습니다. . dt.replace(minute=0,second=0) . datetime.datetime(2011, 10, 29, 20, 0) . dt2 = datetime(2011,11,15,22,30) delta = dt2- dt delta . datetime.timedelta(days=17, seconds=7179) . type(delta) . datetime.timedelta . &#49340;&#54637; &#50672;&#49328;&#51088;(Ternary Operator) . value = true-expr if condition else false-expr . 여기서 true-expr 및 false-expr은 임의의 Python 식입니다. 자세한 내용은 다음과 같습니다. . if condition: value = true-expr else: value = false-expr . x = 5 &#39;None-negative&#39; if x &gt;= 0 else &#39;Negative&#39; . &#39;None-negative&#39; . x = 5 a = 100 if x &gt;= 0 else -100 a . 100 . &#51228;&#50612;&#47928; . python은 다른 프로그래밍 언어에서 볼 수 있는 조건부 논리, 루프 및 기타 표준 제어 흐름 개념을 위한 여러 bulit-in 키워드를 가지고 있습니다. . if , elif and else . if 문은 가장 잘 알려진 제어 흐름문 유형 중 하나입니다. True일 경우 다음 블록의 코드를 평가하는 조건을 체크합니다. . x = -5 if x &lt; 0: print(&#39;It is negative&#39;) . It is negative . if 문 뒤에 옵션으로 하나 이상의 elif 블록과 모든 조건이 false인 경우 catch all other 블록을 사용할 수 있습니다. . x = -5 if x &lt; 0: print(&#39;It is negative&#39;) elif x == 0: print(&#39;Equal to zero&#39;) elif 0 &lt; x &lt; 5: print(&#39;Positive but smaller than 5&#39;) else: print(&#39;Positive and larger than or equl to 5&#39;) . It is negative . 어느 하나의 조건이 참일 경우 더 이상의 elif 또는 블록에 도달하지 않습니다. and 또는 or 를 사용하는 복합 조건에서는 조건이 왼쪽에서 오른쪽으로 평가되어 단락됩니다. . a = 5; b = 7 c = 8; d = 4 if a &lt; b or c &gt; d: print(&#39;Made it&#39;) . Made it . 이 예에서는 첫 번째 비교가 True였기 때문에 c&gt;d 비교는 평가되지 않습니다. 연쇄 비교도 가능합니다. . 4 &gt; 3 &gt; 2 &gt; 1 . True . 3&gt;5 or 2&gt;1 . True . 3&gt;5&gt;2&gt;1 . False . Pass . pass는 Python에서 &quot;no-op&quot;(&quot;No operation&quot;) 문입니다. 액션이 수행되지 않는 블록(또는 아직 구현되지 않은 코드의 자리 표시자)에서 사용할 수 있습니다. Python은 블록을 구분하기 위해 공백을 사용하기 때문에 필요합니다. . x = -1 if x &lt; 0: print(&quot;negative!&quot;) elif x == 0: # TODO: put something smart here pass else: print(&quot;positive!&quot;) . negative! . For loops . for loop는 컬렉션(list나 tuple 등) 또는 반복기로 반복하기 위한 것입니다. for loop의 표준 구문은 다음과 같습니다. . for value in collection: #do something with value . continue 키워드를 사용하여 for loop를 다음 반복으로 진행하고 나머지 블록을 건너뛸 수 있습니다. 이 코드는 list의 int를 집계하고 None 값을 건너뜁니다. . # total = 0 # for value in sequence: # total += value . sequence = [1,2, None, 4, None, 5] total = 0 for value in sequence: if value is None: continue total += value . total . 12 . for loop는 break 키워드를 사용하여 모두 종료할 수 있습니다. 이 코드는 5에 도달할 때까지 목록의 요소를 집계합니다. . sequence = [1,2,0,4,6,5,2,1] total_until_5 = 0 for value in sequence: if value == 5: break total_until_5 += value . total_until_5 . 13 . break 키워드는 가장 안쪽의 for loop만 종료합니다.outer for loop는 계속 실행됩니다. . for i in range(4): for j in range(4): if j &gt; i: break print((i,j)) . (0, 0) (1, 0) (1, 1) (2, 0) (2, 1) (2, 2) (3, 0) (3, 1) (3, 2) (3, 3) . ? 컬렉션 또는 반복기 내의 요소가 시퀀스(튜플 또는 리스트 등)인 경우 쉽게 for loop 스테이트먼트의 변수로 압축 해제할 수 있습니다. . for a, b, c in iterator: # do something . for a, b, c in [[1,2,3],[4,5,6],[7,8,9]]: print(a,b,c) . 1 2 3 4 5 6 7 8 9 . While loops . while loop문은 조건이 False로 평가되거나 루프가 명시적으로 break으로 종료될 때까지 실행되는 조건 및 코드 블록을 지정합니다. . x = 256 total = 0 while x &gt; 0: if total &gt; 500: break total += x x = x // 2 . total . 504 . x . 4 . Range . range 함수는 균일한 간격의 정수 시퀀스를 생성하는 반복기를 반환합니다. . range(10) . range(0, 10) . list(range(10)) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . 시작, 종료 및 단계(음수일 수 있음)를 모두 지정할 수 있습니다. . list(range(0,20,2)) . [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] . list(range(5,0,-1)) . [5, 4, 3, 2, 1] . 보시다시피 range는 끝점을 포함하지 않는 정수를 생성합니다.range의 일반적인 용도는 인덱스로 시퀀스를 반복하는 것입니다. . seq = [1,2,3,4] for i in range(len(seq)): val = seq[i] . val . 4 . list와 같은 함수를 사용하여 범위별로 생성된 모든 정수를 다른 데이터 구조에 저장할 수 있지만, 종종 기본 반복자 형식이 원하는 것이 됩니다. 이 스니펫에서는 3 또는5의 배수인0 ~ 99,999 의 모든 수치를 집계하고 있습니다. . sum = 0 for i in range(100000): # % is the modulo operator if i % 3 == 0 or i % 5 == 0: sum += i .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/jupyter/python/2022/03/20/python_basic.html",
            "relUrl": "/jupyter/python/2022/03/20/python_basic.html",
            "date": " • Mar 20, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Pandas 기본",
            "content": "도구 - 판다스(pandas) . pandas 라이브러리는 사용하기 쉬운 고성능 데이터 구조와 데이터 분석 도구를 제공합니다. 주 데이터 구조는 DataFrame입니다. 이를 인-메모리(in-memory) 2D 테이블로 생각할 수 있습니다(열 이름과 행 레이블이 있는 스프레드시트와 비슷합니다). 엑셀에 있는 많은 기능을 프로그램에서 사용할 수 있습니다. 여기에는 피봇 테이블이나 다른 열을 기반으로 열을 계산하고 그래프 출력하는 기능 등이 포함됩니다. 열 값으로 행을 그룹핑할 수도 있습니다. 또한 SQL과 비슷하게 테이블을 조인할 수 있습니다. 판다스는 시계열 데이터를 다루는데도 뛰어납니다. . 필요 라이브러리: . 넘파이(NumPy) – 넘파이에 익숙하지 않다면 지금 넘파이 튜토리얼을 둘러 보세요. | . 구글 코랩에서 실행하기 | &#49444;&#51221; . 먼저 pandas를 임포트합니다. 보통 pd로 임포트합니다: . import pandas as pd import numpy as np . Series &#44061;&#52404; . pandas 라이브러리는 다음과 같은 유용한 데이터 구조를 포함하고 있습니다: . Series 객체를 곧 이어서 설명하겠습니다. Series 객체는 1D 배열입니다. (열 이름과 행 레이블을 가진) 스프레드시트의 열과 비슷합니다. | DataFrame 객체는 2D 테이블입니다. (열 이름과 행 레이블을 가진) 스프레드시트와 비슷합니다. | . Series &#47564;&#46308;&#44592; . 첫 번째 Series 객체를 만들어 보죠! . import numpy as np np.array([2,-1,3,5]) . array([ 2, -1, 3, 5]) . s = pd.Series([2,-1,3,5]) s . 0 2 1 -1 2 3 3 5 dtype: int64 . 1D ndarray&#50752; &#48708;&#49847;&#54633;&#45768;&#45796; . Series 객체는 넘파이 ndarray와 비슷하게 동작합니다. 넘파이 함수에 매개변수로 종종 전달할 수 있습니다: . import numpy as np np.exp(s) . 0 7.389056 1 0.367879 2 20.085537 3 148.413159 dtype: float64 . Series 객체에 대한 산술 연산도 가능합니다. ndarray와 비슷하게 원소별로 적용됩니다: . s + [1000,2000,3000,4000] . 0 1002 1 1999 2 3003 3 4005 dtype: int64 . 넘파이와 비슷하게 Series에 하나의 숫자를 더하면 Series에 있는 모든 원소에 더해집니다. 이를 브로드캐스팅(broadcasting)이라고 합니다: . s + 1000 . 0 1002 1 999 2 1003 3 1005 dtype: int64 . *나 / 같은 모든 이항 연산과 심지어 조건 연산에서도 마찬가지입니다: . s &lt; 0 . 0 False 1 True 2 False 3 False dtype: bool . s[s&lt;0] . 1 -1 dtype: int64 . &#51064;&#45937;&#49828; &#47112;&#51060;&#48660; . Series 객체에 있는 각 원소는 인덱스 레이블(index label)이라 불리는 고유한 식별자를 가지고 있습니다. 기본적으로 Series에 있는 원소의 순서입니다(0에서 시작합니다). 하지만 수동으로 인덱스 레이블을 지정할 수도 있습니다: . s2 = pd.Series([68, 83, 112, 68], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;, &quot;darwin&quot;]) s2 . alice 68 bob 83 charles 112 darwin 68 dtype: int64 . 그다음 dict처럼 Series를 사용할 수 있습니다: . s2[&quot;bob&quot;] . 83 . 일반 배열처럼 정수 인덱스를 사용하여 계속 원소에 접근할 수 있습니다: . s2[1] . 83 . 레이블이나 정수를 사용해 접근할 때 명확하게 하기 위해 레이블은 loc 속성을 사용하고 정수는 iloc 속성을 사용하는 것이 좋습니다: . s2.loc[&quot;bob&quot;] . 83 . s2.iloc[1] #정수는 integer loc . 83 . Series는 인덱스 레이블을 슬라이싱할 수도 있습니다: . s2.iloc[1:3] . bob 83 charles 112 dtype: int64 . 기본 정수 레이블을 사용할 때 예상 외의 결과를 만들 수 있기 때문에 주의해야 합니다: . surprise = pd.Series([1000, 1001, 1002, 1003]) surprise . 0 1000 1 1001 2 1002 3 1003 dtype: int64 . surprise_slice = surprise[2:] surprise_slice # index 레이블이 0부터 시작해야하는데 2부터 시작함 . 2 1002 3 1003 dtype: int64 . #surprise_slice.[0] . surprise_slice.iloc[0] . 1002 . 보세요. 첫 번째 원소의 인덱스 레이블이 2입니다. 따라서 슬라이싱 결과에서 인덱스 레이블 0인 원소는 없습니다: . try: surprise_slice[0] except KeyError as e: print(&quot;키 에러:&quot;, e) . 키 에러: 0 . 하지만 iloc 속성을 사용해 정수 인덱스로 원소에 접근할 수 있습니다. Series 객체를 사용할 때 loc와 iloc를 사용하는 것이 좋은 이유입니다: . surprise_slice.iloc[0] . 1002 . dict&#50640;&#49436; &#52488;&#44592;&#54868; . dict에서 Series 객체를 만들 수 있습니다. 키는 인덱스 레이블로 사용됩니다: . weights = {&quot;alice&quot;: 68, &quot;bob&quot;: 83, &quot;colin&quot;: 86, &quot;darwin&quot;: 68} s3 = pd.Series(weights) s3 . alice 68 bob 83 colin 86 darwin 68 dtype: int64 . Series에 포함할 원소를 제어하고 index를 지정하여 명시적으로 순서를 결정할 수 있습니다: . s4 = pd.Series(weights, index = [&quot;colin&quot;, &quot;alice&quot;]) s4 . colin 86 alice 68 dtype: int64 . &#51088;&#46041; &#51221;&#47148; . 여러 개의 Series 객체를 다룰 때 pandas는 자동으로 인덱스 레이블에 따라 원소를 정렬합니다. . s2 . alice 68 bob 83 charles 112 darwin 68 dtype: int64 . s3 . alice 68 bob 83 colin 86 darwin 68 dtype: int64 . # s2에는 colin이 s3 에는 charles가 없기 때문에 NaN값. print(s2.keys()) print(s3.keys()) s2 + s3 . Index([&#39;alice&#39;, &#39;bob&#39;, &#39;charles&#39;, &#39;darwin&#39;], dtype=&#39;object&#39;) Index([&#39;alice&#39;, &#39;bob&#39;, &#39;colin&#39;, &#39;darwin&#39;], dtype=&#39;object&#39;) . alice 136.0 bob 166.0 charles NaN colin NaN darwin 136.0 dtype: float64 . 만들어진 Series는 s2와 s3의 인덱스 레이블의 합집합을 담고 있습니다. s2에 &quot;colin&quot;이 없고 s3에 &quot;charles&quot;가 없기 때문에 이 원소는 NaN 값을 가집니다(Not-a-Number는 누락이란 의미입니다). . 자동 정렬은 구조가 다고 누락된 값이 있는 여러 데이터를 다룰 때 매우 편리합니다. 하지만 올바른 인덱스 레이블을 지정하는 것을 잊는다면 원치않는 결과를 얻을 수 있습니다: . s5 = pd.Series([1000,1000,1000,1000]) print(&quot;s2 =&quot;, s2.values) print(&quot;s5 =&quot;, s5.values) s2 + s5 . s2 = [ 68 83 112 68] s5 = [1000 1000 1000 1000] . alice NaN bob NaN charles NaN darwin NaN 0 NaN 1 NaN 2 NaN 3 NaN dtype: float64 . 레이블이 하나도 맞지 않기 때문에 판다스가 이 Series를 정렬할 수 없습니다. 따라서 모두 NaN이 되었습니다. . &#49828;&#52860;&#46972;&#47196; &#52488;&#44592;&#54868; . 스칼라와 인덱스 레이블의 리스트로 Series 객체를 초기화할 수도 있습니다: 모든 원소가 이 스칼라 값으로 설정됩니다. . meaning = pd.Series(42, [&quot;life&quot;, &quot;universe&quot;, &quot;everything&quot;]) #meaning = pd.Series([42,42,42], [&quot;life&quot;, &quot;universe&quot;, &quot;everything&quot;]) meaning . life 42 universe 42 everything 42 dtype: int64 . Series &#51060;&#47492; . Series는 name을 가질 수 있습니다: . s6 = pd.Series([83, 68], index=[&quot;bob&quot;, &quot;alice&quot;], name=&quot;weights&quot;) s6 . bob 83 alice 68 Name: weights, dtype: int64 . Series &#44536;&#47000;&#54532; &#52636;&#47141; . 맷플롯립을 사용해 Series 데이터를 쉽게 그래프로 출력할 수 있습니다(맷플롯립에 대한 자세한 설명은 맷플롯립 튜토리얼을 참고하세요). 맷플롯립을 임포트하고 plot() 메서드를 호출하면 끝입니다: . %matplotlib inline import matplotlib.pyplot as plt temperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5] s7 = pd.Series(temperatures, name=&quot;Temperature&quot;) s7.plot() plt.show() . 데이터를 그래프로 출력하는데 많은 옵션이 있습니다. 여기에서 모두 나열할 필요는 없습니다. 특정 종류의 그래프(히스토그램, 파이 차트 등)가 필요하면 판다스 문서의 시각화 섹션에서 예제 코드를 참고하세요. . &#49884;&#44036; &#45796;&#47336;&#44592; . 많은 데이터셋에 타임스탬프가 포함되어 있습니다. 판다스는 이런 데이터를 다루는데 뛰어납니다: . (2016Q3 같은) 기간과 (&quot;monthly&quot; 같은) 빈도를 표현할 수 있습니다. | 기간을 실제 타임스탬프로 변환하거나 그 반대로 변환할 수 있습니다. | 데이터를 리샘플링하고 원하는 방식으로 값을 모을 수 있습니다. | 시간대를 다룰 수 있습니다. | . &#49884;&#44036; &#48276;&#50948; . 먼저 pd.date_range()를 사용해 시계열을 만들어 보죠. 이 함수는 2016년 10월 29일 5:30pm에서 시작하여 12시간마다 하나의 datetime을 담고 있는 DatetimeIndex를 반환합니다. . dates = pd.date_range(&#39;2016/10/29 5:30pm&#39;, periods=12, freq=&#39;H&#39;) #freq=뽑는 간격 dates . DatetimeIndex([&#39;2016-10-29 17:30:00&#39;, &#39;2016-10-29 18:30:00&#39;, &#39;2016-10-29 19:30:00&#39;, &#39;2016-10-29 20:30:00&#39;, &#39;2016-10-29 21:30:00&#39;, &#39;2016-10-29 22:30:00&#39;, &#39;2016-10-29 23:30:00&#39;, &#39;2016-10-30 00:30:00&#39;, &#39;2016-10-30 01:30:00&#39;, &#39;2016-10-30 02:30:00&#39;, &#39;2016-10-30 03:30:00&#39;, &#39;2016-10-30 04:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;H&#39;) . pd.date_range(&#39;2016/10/29 5:30pm&#39;, periods=12, freq=&#39;2H&#39;) #freq=뽑는 간격 . DatetimeIndex([&#39;2016-10-29 17:30:00&#39;, &#39;2016-10-29 19:30:00&#39;, &#39;2016-10-29 21:30:00&#39;, &#39;2016-10-29 23:30:00&#39;, &#39;2016-10-30 01:30:00&#39;, &#39;2016-10-30 03:30:00&#39;, &#39;2016-10-30 05:30:00&#39;, &#39;2016-10-30 07:30:00&#39;, &#39;2016-10-30 09:30:00&#39;, &#39;2016-10-30 11:30:00&#39;, &#39;2016-10-30 13:30:00&#39;, &#39;2016-10-30 15:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;2H&#39;) . pd.date_range(&#39;2016/10/29 5:30pm&#39;, periods=6, freq=&#39;2H&#39;) #freq=뽑는 간격 . DatetimeIndex([&#39;2016-10-29 17:30:00&#39;, &#39;2016-10-29 19:30:00&#39;, &#39;2016-10-29 21:30:00&#39;, &#39;2016-10-29 23:30:00&#39;, &#39;2016-10-30 01:30:00&#39;, &#39;2016-10-30 03:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;2H&#39;) . pd.date_range(&#39;2020-10-07&#39;, &#39;2020-10-20&#39;, freq = &#39;D&#39;) # 시작날짜와 끝나는 날짜를 지정 . DatetimeIndex([&#39;2020-10-07&#39;, &#39;2020-10-08&#39;, &#39;2020-10-09&#39;, &#39;2020-10-10&#39;, &#39;2020-10-11&#39;, &#39;2020-10-12&#39;, &#39;2020-10-13&#39;, &#39;2020-10-14&#39;, &#39;2020-10-15&#39;, &#39;2020-10-16&#39;, &#39;2020-10-17&#39;, &#39;2020-10-18&#39;, &#39;2020-10-19&#39;, &#39;2020-10-20&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) . 이 DatetimeIndex를 Series의 인덱스로 사용할수 있습니다: . temp_series = pd.Series(temperatures, dates) temp_series . 2016-10-29 17:30:00 4.4 2016-10-29 18:30:00 5.1 2016-10-29 19:30:00 6.1 2016-10-29 20:30:00 6.2 2016-10-29 21:30:00 6.1 2016-10-29 22:30:00 6.1 2016-10-29 23:30:00 5.7 2016-10-30 00:30:00 5.2 2016-10-30 01:30:00 4.7 2016-10-30 02:30:00 4.1 2016-10-30 03:30:00 3.9 2016-10-30 04:30:00 3.5 Freq: H, dtype: float64 . 이 시리즈를 그래프로 출력해 보죠: . temp_series.plot(kind=&quot;bar&quot;) plt.grid(True) plt.show() . &#47532;&#49368;&#54540;&#47553; . 판다스는 매우 간단하게 시계열을 리샘플링할 수 있습니다. resample() 메서드를 호출하고 새로운 주기를 지정하면 됩니다: . temp_series_freq_2H = temp_series.resample(&quot;2H&quot;) temp_series_freq_2H . &lt;pandas.core.resample.DatetimeIndexResampler object at 0x000001620BB0FC10&gt; . 리샘플링 연산은 사실 지연된 연산입니다. 그래서 Series 객체 대신 DatetimeIndexResampler 객체가 반환됩니다. 실제 리샘플링 연산을 수행하려면 mean() 같은 메서드를 호출할 수 있습니다. 이 메서드는 연속적인 시간 쌍에 대해 평균을 계산합니다: . temp_series_freq_2H = temp_series_freq_2H.mean() . temp_series_freq_2H . 2016-10-29 16:00:00 4.40 2016-10-29 18:00:00 5.60 2016-10-29 20:00:00 6.15 2016-10-29 22:00:00 5.90 2016-10-30 00:00:00 4.95 2016-10-30 02:00:00 4.00 2016-10-30 04:00:00 3.50 Freq: 2H, dtype: float64 . 결과를 그래프로 출력해 보죠: . temp_series_freq_2H.plot(kind=&quot;bar&quot;) plt.show() . 2시간 간격으로 어떻게 값이 수집되었는지 확인해 보세요. 예를 들어 6-8pm 간격을 보면 6:30pm에서 5.1이고 7:30pm에서 6.1입니다. 리샘플링 후에 5.1과 6.1의 평균인 5.6 하나를 얻었습니다. 평균말고 어떤 집계 함수(aggregation function)도 사용할 수 있습니다. 예를 들어 각 기간에서 최솟값을 찾을 수 있습니다: . temp_series_freq_2H = temp_series.resample(&quot;2H&quot;).min() temp_series_freq_2H . 2016-10-29 16:00:00 4.4 2016-10-29 18:00:00 5.1 2016-10-29 20:00:00 6.1 2016-10-29 22:00:00 5.7 2016-10-30 00:00:00 4.7 2016-10-30 02:00:00 3.9 2016-10-30 04:00:00 3.5 Freq: 2H, dtype: float64 . 또는 동일한 효과를 내는 apply() 메서드를 사용할 수 있습니다: . temp_series_freq_2H = temp_series.resample(&quot;2H&quot;).apply(np.min) temp_series_freq_2H . 2016-10-29 16:00:00 4.4 2016-10-29 18:00:00 5.1 2016-10-29 20:00:00 6.1 2016-10-29 22:00:00 5.7 2016-10-30 00:00:00 4.7 2016-10-30 02:00:00 3.9 2016-10-30 04:00:00 3.5 Freq: 2H, dtype: float64 . &#50629;&#49368;&#54540;&#47553;&#44284; &#48372;&#44036; . 다운샘플링의 예를 보았습니다. 하지만 업샘플링(즉, 빈도를 높입니다)도 할 수 있습니다. 하지만 데이터에 구멍을 만듭니다: . temp_series_freq_15min = temp_series.resample(&quot;2H&quot;).mean() temp_series_freq_15min . 2016-10-29 16:00:00 4.40 2016-10-29 18:00:00 5.60 2016-10-29 20:00:00 6.15 2016-10-29 22:00:00 5.90 2016-10-30 00:00:00 4.95 2016-10-30 02:00:00 4.00 2016-10-30 04:00:00 3.50 Freq: 2H, dtype: float64 . temp_series_freq_15min = temp_series.resample(&quot;15Min&quot;).mean() temp_series_freq_15min.head(n=10) # 간격을 줄이면 없는 값이 NaN으로 출력된다. . 2016-10-29 17:30:00 4.4 2016-10-29 17:45:00 NaN 2016-10-29 18:00:00 NaN 2016-10-29 18:15:00 NaN 2016-10-29 18:30:00 5.1 2016-10-29 18:45:00 NaN 2016-10-29 19:00:00 NaN 2016-10-29 19:15:00 NaN 2016-10-29 19:30:00 6.1 2016-10-29 19:45:00 NaN Freq: 15T, dtype: float64 . 한가지 방법은 보간으로 사이를 채우는 것입니다. 이렇게 하려면 interpolate() 메서드를 호출합니다. 기본값은 선형 보간이지만 3차 보간(cubic interpolation) 같은 다른 방법을 선택할 수 있습니다: . temp_series_freq_15min = temp_series.resample(&quot;15Min&quot;).interpolate(method=&quot;cubic&quot;) temp_series_freq_15min.head(n=10) . 2016-10-29 17:30:00 4.400000 2016-10-29 17:45:00 4.452911 2016-10-29 18:00:00 4.605113 2016-10-29 18:15:00 4.829758 2016-10-29 18:30:00 5.100000 2016-10-29 18:45:00 5.388992 2016-10-29 19:00:00 5.669887 2016-10-29 19:15:00 5.915839 2016-10-29 19:30:00 6.100000 2016-10-29 19:45:00 6.203621 Freq: 15T, dtype: float64 . temp_series.plot(label=&quot;Period: 1 hour&quot;) temp_series_freq_15min.plot(label=&quot;Period: 15 minutes&quot;) plt.legend() plt.show() . &#49884;&#44036;&#45824; . 기본적으로 datetime은 단순합니다. 시간대(timezone)을 고려하지 않죠. 따라서 2016-10-30 02:30는 파리나 뉴욕이나 2016년 10월 30일 2:30pm입니다. tz_localize() 메서드로 시간대를 고려한 datetime을 만들 수 있습니다: . temp_series_ny = temp_series.tz_localize(&quot;America/New_York&quot;) temp_series_ny . 2016-10-29 17:30:00-04:00 4.4 2016-10-29 18:30:00-04:00 5.1 2016-10-29 19:30:00-04:00 6.1 2016-10-29 20:30:00-04:00 6.2 2016-10-29 21:30:00-04:00 6.1 2016-10-29 22:30:00-04:00 6.1 2016-10-29 23:30:00-04:00 5.7 2016-10-30 00:30:00-04:00 5.2 2016-10-30 01:30:00-04:00 4.7 2016-10-30 02:30:00-04:00 4.1 2016-10-30 03:30:00-04:00 3.9 2016-10-30 04:30:00-04:00 3.5 dtype: float64 . temp_series_seoul = temp_series.tz_localize(&quot;Asia/Seoul&quot;) temp_series_seoul . 2016-10-29 17:30:00+09:00 4.4 2016-10-29 18:30:00+09:00 5.1 2016-10-29 19:30:00+09:00 6.1 2016-10-29 20:30:00+09:00 6.2 2016-10-29 21:30:00+09:00 6.1 2016-10-29 22:30:00+09:00 6.1 2016-10-29 23:30:00+09:00 5.7 2016-10-30 00:30:00+09:00 5.2 2016-10-30 01:30:00+09:00 4.7 2016-10-30 02:30:00+09:00 4.1 2016-10-30 03:30:00+09:00 3.9 2016-10-30 04:30:00+09:00 3.5 dtype: float64 . 모든 datetime에 -04:00이 추가됩니다. 즉 모든 시간은 UTC - 4시간을 의미합니다. . 다음처럼 파리 시간대로 바꿀 수 있습니다: . temp_series_paris = temp_series_ny.tz_convert(&quot;Europe/Paris&quot;) temp_series_paris . 2016-10-29 23:30:00+02:00 4.4 2016-10-30 00:30:00+02:00 5.1 2016-10-30 01:30:00+02:00 6.1 2016-10-30 02:30:00+02:00 6.2 2016-10-30 02:30:00+01:00 6.1 2016-10-30 03:30:00+01:00 6.1 2016-10-30 04:30:00+01:00 5.7 2016-10-30 05:30:00+01:00 5.2 2016-10-30 06:30:00+01:00 4.7 2016-10-30 07:30:00+01:00 4.1 2016-10-30 08:30:00+01:00 3.9 2016-10-30 09:30:00+01:00 3.5 dtype: float64 . UTC와의 차이가 +02:00에서 +01:00으로 바뀐 것을 알 수 있습니다. 이는 프랑스가 10월 30일 3am에 겨울 시간으로 바꾸기 때문입니다(2am으로 바뀝니다). 따라서 2:30am이 두 번 등장합니다! 시간대가 없는 표현으로 돌아가 보죠(시간대가 없이 지역 시간으로 매시간 로그를 기록하는 경우 이와 비슷할 것입니다): . temp_series_paris_naive = temp_series_paris.tz_localize(None) temp_series_paris_naive . 2016-10-29 23:30:00 4.4 2016-10-30 00:30:00 5.1 2016-10-30 01:30:00 6.1 2016-10-30 02:30:00 6.2 2016-10-30 02:30:00 6.1 2016-10-30 03:30:00 6.1 2016-10-30 04:30:00 5.7 2016-10-30 05:30:00 5.2 2016-10-30 06:30:00 4.7 2016-10-30 07:30:00 4.1 2016-10-30 08:30:00 3.9 2016-10-30 09:30:00 3.5 dtype: float64 . 이렇게 되면 02:30이 정말 애매합니다. 시간대가 없는 datetime을 파리 시간대로 바꿀 때 에러가 발생합니다: . try: temp_series_paris_naive.tz_localize(&quot;Europe/Paris&quot;) except Exception as e: print(type(e)) print(e) . &lt;class &#39;NameError&#39;&gt; name &#39;temp_series_paris_naive&#39; is not defined . 다행히 ambiguous 매개변수를 사용하면 판다스가 타임스탬프의 순서를 기반으로 적절한 DST(일광 절약 시간제)를 추측합니다: . temp_series_paris_naive.tz_localize(&quot;Europe/Paris&quot;, ambiguous=&quot;infer&quot;) . 2016-10-29 23:30:00+02:00 4.4 2016-10-30 00:30:00+02:00 5.1 2016-10-30 01:30:00+02:00 6.1 2016-10-30 02:30:00+02:00 6.2 2016-10-30 02:30:00+01:00 6.1 2016-10-30 03:30:00+01:00 6.1 2016-10-30 04:30:00+01:00 5.7 2016-10-30 05:30:00+01:00 5.2 2016-10-30 06:30:00+01:00 4.7 2016-10-30 07:30:00+01:00 4.1 2016-10-30 08:30:00+01:00 3.9 2016-10-30 09:30:00+01:00 3.5 dtype: float64 . &#44592;&#44036; . pd.period_range() 함수는 DatetimeIndex가 아니라 PeriodIndex를 반환합니다. 예를 들어 2016과 2017년의 전체 분기를 가져와 보죠: . quarters = pd.period_range(&#39;2016Q1&#39;, periods=8, freq=&#39;Q&#39;) #&#39;Q&#39; = quarter분기 #&#39;M&#39; = 한달 quarters . PeriodIndex([&#39;2016Q1&#39;, &#39;2016Q2&#39;, &#39;2016Q3&#39;, &#39;2016Q4&#39;, &#39;2017Q1&#39;, &#39;2017Q2&#39;, &#39;2017Q3&#39;, &#39;2017Q4&#39;], dtype=&#39;period[Q-DEC]&#39;) . PeriodIndex에 숫자 N을 추가하면 PeriodIndex 빈도의 N 배만큼 이동시킵니다: . quarters + 3 # 단위가 quarter이므로 3quarter씩 더해줌. . PeriodIndex([&#39;2016Q4&#39;, &#39;2017Q1&#39;, &#39;2017Q2&#39;, &#39;2017Q3&#39;, &#39;2017Q4&#39;, &#39;2018Q1&#39;, &#39;2018Q2&#39;, &#39;2018Q3&#39;], dtype=&#39;period[Q-DEC]&#39;) . asfreq() 메서드를 사용하면 PeriodIndex의 빈도를 바꿀 수 있습니다. 모든 기간이 늘어나거나 줄어듭니다. 예를 들어 분기 기간을 모두 월별 기간으로 바꾸어 보죠: . quarters.asfreq(&quot;M&quot;) . PeriodIndex([&#39;2016-03&#39;, &#39;2016-06&#39;, &#39;2016-09&#39;, &#39;2016-12&#39;, &#39;2017-03&#39;, &#39;2017-06&#39;, &#39;2017-09&#39;, &#39;2017-12&#39;], dtype=&#39;period[M]&#39;) . quarters . PeriodIndex([&#39;2016Q1&#39;, &#39;2016Q2&#39;, &#39;2016Q3&#39;, &#39;2016Q4&#39;, &#39;2017Q1&#39;, &#39;2017Q2&#39;, &#39;2017Q3&#39;, &#39;2017Q4&#39;], dtype=&#39;period[Q-DEC]&#39;) . 기본적으로 asfreq는 각 기간의 끝에 맞춥니다. 기간의 시작에 맞추도록 변경할 수 있습니다: . quarters.asfreq(&quot;M&quot;, how=&quot;start&quot;) . PeriodIndex([&#39;2016-01&#39;, &#39;2016-04&#39;, &#39;2016-07&#39;, &#39;2016-10&#39;, &#39;2017-01&#39;, &#39;2017-04&#39;, &#39;2017-07&#39;, &#39;2017-10&#39;], dtype=&#39;period[M]&#39;) . 간격을 늘릴 수도 있습니다: . quarters.asfreq(&quot;A&quot;) # &#39;A&#39; = 연도 . PeriodIndex([&#39;2016&#39;, &#39;2016&#39;, &#39;2016&#39;, &#39;2016&#39;, &#39;2017&#39;, &#39;2017&#39;, &#39;2017&#39;, &#39;2017&#39;], dtype=&#39;period[A-DEC]&#39;) . 물론 PeriodIndex로 Series를 만들 수 있습니다: . quarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters) quarterly_revenue . 2016Q1 300 2016Q2 320 2016Q3 290 2016Q4 390 2017Q1 320 2017Q2 360 2017Q3 310 2017Q4 410 Freq: Q-DEC, dtype: int64 . quarterly_revenue.plot(kind=&quot;line&quot;) plt.show() . to_timestamp를 호출해서 기간을 타임스탬프로 변경할 수 있습니다. 기본적으로 기간의 첫 번째 날을 반환합니다. 하지만 how와 freq를 지정해서 기간의 마지막 시간을 얻을 수 있습니다: . quarterly_revenue . 2016Q1 300 2016Q2 320 2016Q3 290 2016Q4 390 2017Q1 320 2017Q2 360 2017Q3 310 2017Q4 410 Freq: Q-DEC, dtype: int64 . last_hours = quarterly_revenue.to_timestamp(how=&quot;end&quot;, freq=&quot;H&quot;) last_hours . 2016-03-31 23:59:59.999999999 300 2016-06-30 23:59:59.999999999 320 2016-09-30 23:59:59.999999999 290 2016-12-31 23:59:59.999999999 390 2017-03-31 23:59:59.999999999 320 2017-06-30 23:59:59.999999999 360 2017-09-30 23:59:59.999999999 310 2017-12-31 23:59:59.999999999 410 dtype: int64 . to_peroid를 호출하면 다시 기간으로 돌아갑니다: . last_hours.to_period() #분기로 다시 바꿔줌 . 2016Q1 300 2016Q2 320 2016Q3 290 2016Q4 390 2017Q1 320 2017Q2 360 2017Q3 310 2017Q4 410 Freq: Q-DEC, dtype: int64 . 판다스는 여러 가지 시간 관련 함수를 많이 제공합니다. 온라인 문서를 확인해 보세요. 예를 하나 들면 2016년 매월 마지막 업무일의 9시를 얻는 방법은 다음과 같습니다: . months_2022 = pd.period_range(&quot;2022&quot;, periods=12, freq = &#39;M&#39;) one_day_after_last_days = months_2022.asfreq(&quot;D&quot;)+1 last_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay() last_bdays.to_period(&quot;H&quot;)+9 . PeriodIndex([&#39;2022-01-31 09:00&#39;, &#39;2022-02-28 09:00&#39;, &#39;2022-03-31 09:00&#39;, &#39;2022-04-29 09:00&#39;, &#39;2022-05-31 09:00&#39;, &#39;2022-06-30 09:00&#39;, &#39;2022-07-29 09:00&#39;, &#39;2022-08-31 09:00&#39;, &#39;2022-09-30 09:00&#39;, &#39;2022-10-31 09:00&#39;, &#39;2022-11-30 09:00&#39;, &#39;2022-12-30 09:00&#39;], dtype=&#39;period[H]&#39;) . BDay()는 business day 주말이면 2일을 빼주고 평일이면 하루를 빼준다. . months_2016 = pd.period_range(&quot;2016&quot;, periods=12, freq=&quot;M&quot;) one_day_after_last_days = months_2016.asfreq(&quot;D&quot;) + 1 #매월의 첫날 last_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay(n=1) last_bdays.to_period(&quot;H&quot;) + 9 . PeriodIndex([&#39;2016-01-29 09:00&#39;, &#39;2016-02-29 09:00&#39;, &#39;2016-03-31 09:00&#39;, &#39;2016-04-29 09:00&#39;, &#39;2016-05-31 09:00&#39;, &#39;2016-06-30 09:00&#39;, &#39;2016-07-29 09:00&#39;, &#39;2016-08-31 09:00&#39;, &#39;2016-09-30 09:00&#39;, &#39;2016-10-31 09:00&#39;, &#39;2016-11-30 09:00&#39;, &#39;2016-12-30 09:00&#39;], dtype=&#39;period[H]&#39;) . DataFrame &#44061;&#52404; . 데이터프레임 객체는 스프레드시트를 표현합니다. 셀 값, 열 이름, 행 인덱스 레이블을 가집니다. 다른 열을 바탕으로 열을 계산하는 식을 쓸 수 있고 피봇 테이블을 만들고, 행을 그룹핑하고, 그래프를 그릴 수 있습니다. DataFrame을 Series의 딕셔너리로 볼 수 있습니다. . DataFrame &#47564;&#46308;&#44592; . Series 객체의 딕셔너리를 전달하여 데이터프레임을 만들 수 있습니다: . people_dict = { &quot;weight&quot;: pd.Series([68, 83, 112], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;]), &quot;birthyear&quot;: pd.Series([1984, 1985, 1992], index=[&quot;bob&quot;, &quot;alice&quot;, &quot;charles&quot;], name=&quot;year&quot;), &quot;children&quot;: pd.Series([0, 3], index=[&quot;charles&quot;, &quot;bob&quot;]), &quot;hobby&quot;: pd.Series([&quot;Biking&quot;, &quot;Dancing&quot;], index=[&quot;alice&quot;, &quot;bob&quot;]), } people = pd.DataFrame(people_dict) people . weight birthyear children hobby . alice 68 | 1985 | NaN | Biking | . bob 83 | 1984 | 3.0 | Dancing | . charles 112 | 1992 | 0.0 | NaN | . people_dict . {&#39;weight&#39;: alice 68 bob 83 charles 112 dtype: int64, &#39;birthyear&#39;: bob 1984 alice 1985 charles 1992 Name: year, dtype: int64, &#39;children&#39;: charles 0 bob 3 dtype: int64, &#39;hobby&#39;: alice Biking bob Dancing dtype: object} . people_dict[&#39;weight&#39;] . alice 68 bob 83 charles 112 dtype: int64 . 몇가지 알아 두어야 할 것은 다음과 같습니다: . Series는 인덱스를 기반으로 자동으로 정렬됩니다. | 누란된 값은 NaN으로 표현됩니다. | Series 이름은 무시됩니다(&quot;year&quot;란 이름은 삭제됩니다). | DataFrame은 주피터 노트북에서 멋지게 출력됩니다! | . 예상하는 방식으로 열을 참조할 수 있고 Serires 객체가 반환됩니다: . people[&quot;birthyear&quot;] . alice 1985 bob 1984 charles 1992 Name: birthyear, dtype: int64 . 동시에 여러 개의 열을 선택할 수 있습니다: . people[[&quot;birthyear&quot;, &quot;hobby&quot;]] . birthyear hobby . alice 1985 | Biking | . bob 1984 | Dancing | . charles 1992 | NaN | . 열 리스트나 행 인덱스 레이블을 DataFrame 생성자에 전달하면 해당 열과 행으로 채워진 데이터프레임이 반환됩니다. 예를 들면: . people_dict . {&#39;weight&#39;: alice 68 bob 83 charles 112 dtype: int64, &#39;birthyear&#39;: bob 1984 alice 1985 charles 1992 Name: year, dtype: int64, &#39;children&#39;: charles 0 bob 3 dtype: int64, &#39;hobby&#39;: alice Biking bob Dancing dtype: object} . d2 = pd.DataFrame( people_dict, columns=[&quot;birthyear&quot;, &quot;weight&quot;, &quot;height&quot;], index=[&quot;bob&quot;, &quot;alice&quot;, &quot;eugene&quot;] ) d2 . birthyear weight height . bob 1984.0 | 83.0 | NaN | . alice 1985.0 | 68.0 | NaN | . eugene NaN | NaN | NaN | . DataFrame을 만드는 또 다른 편리한 방법은 ndarray나 리스트의 리스트로 모든 값을 생성자에게 전달하고 열 이름과 행 인덱스 레이블을 각기 지정하는 것입니다: . values = [ [1985, np.nan, &quot;Biking&quot;, 68], [1984, 3, &quot;Dancing&quot;, 83], [1992, 0, np.nan, 112] ] d3 = pd.DataFrame( values, columns=[&quot;birthyear&quot;, &quot;children&quot;, &quot;hobby&quot;, &quot;weight&quot;], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;] ) d3 . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3.0 | Dancing | 83 | . charles 1992 | 0.0 | NaN | 112 | . 누락된 값을 지정하려면 np.nan이나 넘파이 마스크 배열을 사용합니다: . masked_array = np.ma.asarray(values, dtype=np.object) masked_array[(0, 2), (1, 2)] = np.ma.masked #NaN값으로 들어감 . d3 = pd.DataFrame( masked_array, columns=[&quot;birthyear&quot;, &quot;children&quot;, &quot;hobby&quot;, &quot;weight&quot;], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;] ) d3 . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3 | Dancing | 83 | . charles 1992 | 0 | NaN | 112 | . ndarray 대신에 DataFrame 객체를 전달할 수도 있습니다: . d4 = pd.DataFrame( d3, columns=[&quot;hobby&quot;, &quot;children&quot;], index=[&quot;alice&quot;, &quot;bob&quot;] ) d4 . hobby children . alice Biking | NaN | . bob Dancing | 3 | . 딕셔너리의 딕셔너리(또는 리스트의 리스트)로 DataFrame을 만들 수 있습니다: . people = pd.DataFrame({ &quot;birthyear&quot;: {&quot;alice&quot;:1985, &quot;bob&quot;: 1984, &quot;charles&quot;: 1992}, &quot;hobby&quot;: {&quot;alice&quot;:&quot;Biking&quot;, &quot;bob&quot;: &quot;Dancing&quot;}, &quot;weight&quot;: {&quot;alice&quot;:68, &quot;bob&quot;: 83, &quot;charles&quot;: 112}, &quot;children&quot;: {&quot;bob&quot;: 3, &quot;charles&quot;: 0} }) people . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . charles 1992 | NaN | 112 | 0.0 | . &#47680;&#54000; &#51064;&#45937;&#49905; . 모든 열이 같은 크기의 튜플이면 멀티 인덱스로 인식합니다. 열 인덱스 레이블에도 같은 방식이 적용됩니다. 예를 들면: . d5 = pd.DataFrame( { (&quot;public&quot;, &quot;birthyear&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):1985, (&quot;Paris&quot;,&quot;bob&quot;): 1984, (&quot;London&quot;,&quot;charles&quot;): 1992}, (&quot;public&quot;, &quot;hobby&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):&quot;Biking&quot;, (&quot;Paris&quot;,&quot;bob&quot;): &quot;Dancing&quot;}, (&quot;private&quot;, &quot;weight&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):68, (&quot;Paris&quot;,&quot;bob&quot;): 83, (&quot;London&quot;,&quot;charles&quot;): 112}, (&quot;private&quot;, &quot;children&quot;): {(&quot;Paris&quot;, &quot;alice&quot;):np.nan, (&quot;Paris&quot;,&quot;bob&quot;): 3, (&quot;London&quot;,&quot;charles&quot;): 0} } ) d5 . public private . birthyear hobby weight children . Paris alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . London charles 1992 | NaN | 112 | 0.0 | . 이제 &quot;public&quot; 열을 모두 담은 DataFrame을 손쉽게 만들 수 있습니다: . d5[&quot;public&quot;] . birthyear hobby . Paris alice 1985 | Biking | . bob 1984 | Dancing | . London charles 1992 | NaN | . d5[&quot;public&quot;, &quot;hobby&quot;] # d5[&quot;public&quot;][&quot;hobby&quot;]와 같습니다. . Paris alice Biking bob Dancing London charles NaN Name: (public, hobby), dtype: object . &#47112;&#48296; &#45230;&#52628;&#44592; . d5를 다시 확인해 보죠: . d5 . public private . birthyear hobby weight children . Paris alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . London charles 1992 | NaN | 112 | 0.0 | . 열의 레벨(level)이 2개이고 인덱스 레벨이 2개입니다. droplevel()을 사용해 열 레벨을 낮출 수 있습니다(인덱스도 마찬가지입니다): . d5.columns = d5.columns.droplevel(level = 0) d5 . birthyear hobby weight children . Paris alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . London charles 1992 | NaN | 112 | 0.0 | . #d5.columns = d5.columns.droplevel(level=1) #d5 . public public private private . Paris alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . London charles 1992 | NaN | 112 | 0.0 | . # d5.index = d5.index.droplevel(level = 0) # d5 . public private . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . charles 1992 | NaN | 112 | 0.0 | . &#51204;&#52824; . T 속성을 사용해 열과 인덱스를 바꿀 수 있습니다: . d6 = d5.T d6 . Paris London . alice bob charles . birthyear 1985 | 1984 | 1992 | . hobby Biking | Dancing | NaN | . weight 68 | 83 | 112 | . children NaN | 3.0 | 0.0 | . &#47112;&#48296; &#49828;&#53469;&#44284; &#50616;&#49828;&#53469; . stack() 메서드는 가장 낮은 열 레벨을 가장 낮은 인덱스 뒤에 추가합니다: unstack() 메서드는 가장 낮은 인덱스 레벨을 가장 낮은 열 뒤에 추가합니다: . d7 = d6.stack() d7 . London Paris . birthyear alice NaN | 1985 | . bob NaN | 1984 | . charles 1992 | NaN | . hobby alice NaN | Biking | . bob NaN | Dancing | . weight alice NaN | 68 | . bob NaN | 83 | . charles 112 | NaN | . children bob NaN | 3.0 | . charles 0.0 | NaN | . d7.index . MultiIndex([(&#39;birthyear&#39;, &#39;alice&#39;), (&#39;birthyear&#39;, &#39;bob&#39;), (&#39;birthyear&#39;, &#39;charles&#39;), ( &#39;hobby&#39;, &#39;alice&#39;), ( &#39;hobby&#39;, &#39;bob&#39;), ( &#39;weight&#39;, &#39;alice&#39;), ( &#39;weight&#39;, &#39;bob&#39;), ( &#39;weight&#39;, &#39;charles&#39;), ( &#39;children&#39;, &#39;bob&#39;), ( &#39;children&#39;, &#39;charles&#39;)], ) . NaN 값이 생겼습니다. 이전에 없던 조합이 생겼기 때문입니다(예를 들어 London에 bob이 없었습니다). . unstack()을 호출하면 반대가 됩니다. 여기에서도 많은 NaN 값이 생성됩니다. . d8 = d7.unstack() d8 . London Paris . alice bob charles alice bob charles . birthyear NaN | NaN | 1992 | 1985 | 1984 | NaN | . children NaN | NaN | 0.0 | NaN | 3.0 | NaN | . hobby NaN | NaN | NaN | Biking | Dancing | NaN | . weight NaN | NaN | 112 | 68 | 83 | NaN | . unstack을 다시 호출하면 Series 객체가 만들어 집니다: . d9 = d8.unstack() d9 . London alice birthyear NaN children NaN hobby NaN weight NaN bob birthyear NaN children NaN hobby NaN weight NaN charles birthyear 1992 children 0.0 hobby NaN weight 112 Paris alice birthyear 1985 children NaN hobby Biking weight 68 bob birthyear 1984 children 3.0 hobby Dancing weight 83 charles birthyear NaN children NaN hobby NaN weight NaN dtype: object . stack()과 unstack() 메서드를 사용할 때 스택/언스택할 level을 선택할 수 있습니다. 심지어 한 번에 여러 개의 레벨을 스택/언스택할 수도 있습니다: . d10 = d9.unstack(level = (0,1)) d10 . London Paris . alice bob charles alice bob charles . birthyear NaN | NaN | 1992 | 1985 | 1984 | NaN | . children NaN | NaN | 0.0 | NaN | 3.0 | NaN | . hobby NaN | NaN | NaN | Biking | Dancing | NaN | . weight NaN | NaN | 112 | 68 | 83 | NaN | . &#45824;&#48512;&#48516;&#51032; &#47700;&#49436;&#46300;&#45716; &#49688;&#51221;&#46108; &#48373;&#49324;&#48376;&#51012; &#48152;&#54872;&#54633;&#45768;&#45796; . 눈치챘겠지만 stack()과 unstack() 메서드는 객체를 수정하지 않습니다. 대신 복사본을 만들어 반환합니다. 판다스에 있는 대부분의 메서드들이 이렇게 동작합니다. . Pivot . import pandas._testing as tm def unpivot(frame): N, K = frame.shape data = { &quot;value&quot;: frame.to_numpy().ravel(&quot;F&quot;), &quot;variable&quot;: np.asarray(frame.columns).repeat(N), &quot;date&quot;: np.tile(np.asarray(frame.index), K), } return pd.DataFrame(data, columns=[&quot;date&quot;, &quot;variable&quot;, &quot;value&quot;]) df = unpivot(tm.makeTimeDataFrame(3)) df . date variable value . 0 2000-01-03 | A | 0.956514 | . 1 2000-01-04 | A | -0.711841 | . 2 2000-01-05 | A | 0.496248 | . 3 2000-01-03 | B | 0.092486 | . 4 2000-01-04 | B | -1.625856 | . 5 2000-01-05 | B | 0.801648 | . 6 2000-01-03 | C | 1.164039 | . 7 2000-01-04 | C | -1.194786 | . 8 2000-01-05 | C | -0.080289 | . 9 2000-01-03 | D | -0.842007 | . 10 2000-01-04 | D | -0.934912 | . 11 2000-01-05 | D | 0.711753 | . filtered = df[df[&quot;variable&quot;] == &quot;A&quot;] filtered . date variable value . 0 2000-01-03 | A | 0.162390 | . 1 2000-01-04 | A | -0.994437 | . 2 2000-01-05 | A | 0.747082 | . pivoted = df.pivot(index=&quot;date&quot;, columns=&quot;variable&quot;, values=&quot;value&quot;) pivoted . variable A B C D . date . 2000-01-03 0.956514 | 0.092486 | 1.164039 | -0.842007 | . 2000-01-04 -0.711841 | -1.625856 | -1.194786 | -0.934912 | . 2000-01-05 0.496248 | 0.801648 | -0.080289 | 0.711753 | . df[&quot;value2&quot;] = df[&quot;value&quot;] * 2 pivoted = df.pivot(index=&quot;date&quot;, columns=&quot;variable&quot;) pivoted . value value2 . variable A B C D A B C D . date . 2000-01-03 0.956514 | 0.092486 | 1.164039 | -0.842007 | 1.913028 | 0.184971 | 2.328077 | -1.684013 | . 2000-01-04 -0.711841 | -1.625856 | -1.194786 | -0.934912 | -1.423682 | -3.251711 | -2.389572 | -1.869825 | . 2000-01-05 0.496248 | 0.801648 | -0.080289 | 0.711753 | 0.992497 | 1.603296 | -0.160578 | 1.423505 | . pivoted[&quot;value2&quot;] . variable A B C D . date . 2000-01-03 1.913028 | 0.184971 | 2.328077 | -1.684013 | . 2000-01-04 -1.423682 | -3.251711 | -2.389572 | -1.869825 | . 2000-01-05 0.992497 | 1.603296 | -0.160578 | 1.423505 | . &#54665; &#52280;&#51312;&#54616;&#44592; . people DataFrame으로 돌아가 보죠: . people . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . charles 1992 | NaN | 112 | 0.0 | . loc 속성으로 열 대신 행을 참조할 수 있습니다. DataFrame의 열 이름이 행 인덱스 레이블로 매핑된 Series 객체가 반환됩니다: . people[&#39;birthyear&#39;] . alice 1985 bob 1984 charles 1992 Name: birthyear, dtype: int64 . people.loc[&quot;charles&quot;] . birthyear 1992 hobby NaN weight 112 children 0.0 Name: charles, dtype: object . iloc 속성을 사용해 정수 인덱스로 행을 참조할 수 있습니다: . people.iloc[2] . birthyear 1992 hobby NaN weight 112 children 0.0 Name: charles, dtype: object . 행을 슬라이싱할 수 있으며 DataFrame 객체가 반환됩니다: . people.iloc[1:3] . birthyear hobby weight children . bob 1984 | Dancing | 83 | 3.0 | . charles 1992 | NaN | 112 | 0.0 | . 마자믹으로 불리언 배열을 전달하여 해당하는 행을 가져올 수 있습니다: . people[np.array([True, False, True])] . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . charles 1992 | NaN | 112 | 0.0 | . 불리언 표현식을 사용할 때 아주 유용합니다: . people[people[&quot;birthyear&quot;] &lt; 1990] . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . &#50676; &#52628;&#44032;, &#49325;&#51228; . DataFrame을 Series의 딕셔너리처럼 다룰 수 있습니다. 따라서 다음 같이 쓸 수 있습니다: . people . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . charles 1992 | NaN | 112 | 0.0 | . people[&quot;age&quot;] = 2018 - people[&quot;birthyear&quot;] # &quot;age&quot; 열을 추가합니다 people[&quot;over 30&quot;] = people[&quot;age&quot;] &gt; 30 # &quot;over 30&quot; 열을 추가합니다 birthyears = people.pop(&quot;birthyear&quot;) #pop은 뽑다, birthyear은 제거된다 del people[&quot;children&quot;] people . hobby weight age over 30 . alice Biking | 68 | 33 | True | . bob Dancing | 83 | 34 | True | . charles NaN | 112 | 26 | False | . birthyears . alice 1985 bob 1984 charles 1992 Name: birthyear, dtype: int64 . weights = {&quot;alice&quot;:68,&quot;bob&quot;:83,&quot;colin&quot;:86,&quot;darwin&quot;:68} . weights.pop(&quot;alice&quot;) . 68 . weights . {&#39;bob&#39;: 83, &#39;colin&#39;: 86, &#39;darwin&#39;: 68} . del weights[&quot;bob&quot;] . weights . {&#39;colin&#39;: 86, &#39;darwin&#39;: 68} . 새로운 열을 추가할 때 행의 개수는 같아야 합니다. 누락된 행은 NaN으로 채워지고 추가적인 행은 무시됩니다: . people[&quot;pets&quot;] = pd.Series({&quot;bob&quot;: 0, &quot;charles&quot;: 5, &quot;eugene&quot;:1}) # alice 누락됨, eugene은 무시됨 people . hobby weight age over 30 pets . alice Biking | 68 | 33 | True | NaN | . bob Dancing | 83 | 34 | True | 0.0 | . charles NaN | 112 | 26 | False | 5.0 | . 새로운 열을 추가할 때 기본적으로 (오른쪽) 끝에 추가됩니다. insert() 메서드를 사용해 다른 곳에 열을 추가할 수 있습니다: . . people.insert(1, &quot;height&quot;, [172, 181, 185]) people . hobby height weight age over 30 pets . alice Biking | 172 | 68 | 33 | True | NaN | . bob Dancing | 181 | 83 | 34 | True | 0.0 | . charles NaN | 185 | 112 | 26 | False | 5.0 | . &#49352;&#47196;&#50868; &#50676; &#54624;&#45817;&#54616;&#44592; . assign() 메서드를 호출하여 새로운 열을 만들 수도 있습니다. 이는 새로운 DataFrame 객체를 반환하며 원본 객체는 변경되지 않습니다: . people.assign( body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2, has_pets = people[&quot;pets&quot;] &gt; 0 ) . hobby height weight age over 30 pets body_mass_index has_pets . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . people[&quot;body_mass_index&quot;] = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100)*2 people . hobby height weight age over 30 pets body_mass_index . alice Biking | 172 | 68 | 33 | True | NaN | 79.069767 | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 91.712707 | . charles NaN | 185 | 112 | 26 | False | 5.0 | 121.081081 | . . 할당문 안에서 만든 열은 접근할 수 없습니다: . try: people.assign( body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2, overweight = people[&quot;body_mass_index&quot;] &gt; 25 ) except KeyError as e: print(&quot;키 에러:&quot;, e) . 키 에러: &#39;body_mass_index&#39; . 해결책은 두 개의 연속된 할당문으로 나누는 것입니다: . # 비효율적 d6 = people.assign(body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2) d6.assign(overweight = d6[&quot;body_mass_index&quot;] &gt; 25) . hobby height weight age over 30 pets body_mass_index overweight . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | True | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . 임시 변수 d6를 만들면 불편합니다. assign() 메서드를 연결하고 싶겠지만 people 객체가 첫 번째 할당문에서 실제로 수정되지 않기 때문에 작동하지 않습니다: . try: (people .assign(body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2) .assign(overweight = people[&quot;body_mass_index&quot;] &gt; 25) ) except KeyError as e: print(&quot;키 에러:&quot;, e) . 키 에러: &#39;body_mass_index&#39; . 하지만 걱정하지 마세요. 간단한 방법이 있습니다. assign() 메서드에 함수(전형적으로 lambda 함수)를 전달하면 DataFrame을 매개변수로 이 함수를 호출할 것입니다: . (people .assign(body_mass_index = lambda df: df[&quot;weight&quot;] / (df[&quot;height&quot;] / 100) ** 2) .assign(overweight = lambda df: df[&quot;body_mass_index&quot;] &gt; 25) ) . hobby height weight age over 30 pets body_mass_index overweight . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | True | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . people[&quot;body_mass_index&quot;] = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100)*2 . 문제가 해결되었군요! . &#54364;&#54788;&#49885; &#54217;&#44032; . 판다스가 제공하는 뛰어난 기능 하나는 표현식 평가입니다. 이는 numexpr 라이브러리에 의존하기 때문에 설치가 되어 있어야 합니다. . people.eval(&quot;weight / (height/100) ** 2 &gt; 25&quot;) . alice False bob True charles True dtype: bool . 할당 표현식도 지원됩니다. inplace=True로 지정하면 수정된 복사본을 만들지 않고 바로 DataFrame을 변경합니다: . people.eval(&quot;body_mass_index = weight / (height/100) ** 2&quot;, inplace=True) people . hobby height weight age over 30 pets body_mass_index . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | . &#39;@&#39;를 접두어로 사용하여 지역 변수나 전역 변수를 참조할 수 있습니다: . overweight_threshold = 30 people.eval(&quot;overweight = body_mass_index &gt; @overweight_threshold&quot;, inplace=True) people . hobby height weight age over 30 pets body_mass_index overweight . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . DataFrame &#53244;&#47532;&#54616;&#44592; . query() 메서드를 사용하면 쿼리 표현식에 기반하여 DataFrame을 필터링할 수 있습니다: . people.query(&quot;age &gt; 30 and pets == 0&quot;) . hobby height weight age over 30 pets body_mass_index overweight . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . people[(people[&quot;age&quot;]&gt;30) &amp; (people[&quot;pets&quot;]==0)] . hobby height weight age over 30 pets body_mass_index overweight . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . mask = (people[&quot;age&quot;]&gt;30) &amp; (people[&quot;pets&quot;]==0) people[mask] . hobby height weight age over 30 pets body_mass_index overweight . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . DataFrame &#51221;&#47148; . sort_index 메서드를 호출하여 DataFrame을 정렬할 수 있습니다. 기본적으로 인덱스 레이블을 기준으로 오름차순으로 행을 정렬합니다. 여기에서는 내림차순으로 정렬해 보죠: . people.sort_index(ascending=False) . hobby height weight age over 30 pets body_mass_index overweight . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . sort_index는 DataFrame의 정렬된 복사본을 반환합니다. people을 직접 수정하려면 inplace 매개변수를 True로 지정합니다. 또한 axis=1로 지정하여 열 대신 행을 정렬할 수 있습니다: . people.sort_index(axis=1, inplace=True) people . age body_mass_index height hobby over 30 overweight pets weight . alice 33 | 22.985398 | 172 | Biking | True | False | NaN | 68 | . bob 34 | 25.335002 | 181 | Dancing | True | False | 0.0 | 83 | . charles 26 | 32.724617 | 185 | NaN | False | True | 5.0 | 112 | . 레이블이 아니라 값을 기준으로 DataFrame을 정렬하려면 sort_values에 정렬하려는 열을 지정합니다: . people.sort_values(by=&quot;age&quot;, inplace=True) people . age body_mass_index height hobby over 30 overweight pets weight . charles 26 | 32.724617 | 185 | NaN | False | True | 5.0 | 112 | . alice 33 | 22.985398 | 172 | Biking | True | False | NaN | 68 | . bob 34 | 25.335002 | 181 | Dancing | True | False | 0.0 | 83 | . DataFrame &#44536;&#47000;&#54532; &#44536;&#47532;&#44592; . Series와 마찬가지로 판다스는 DataFrame 기반으로 멋진 그래프를 손쉽게 그릴 수 있습니다. . 예를 들어 plot 메서드를 호출하여 DataFrame의 데이터에서 선 그래프를 쉽게 그릴 수 있습니다: . people.plot.line(x = &quot;body_mass_index&quot;, y = [&quot;height&quot;, &quot;weight&quot;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b72761dc0&gt; . people.plot(kind = &quot;line&quot;, x = &quot;body_mass_index&quot;, y = [&quot;height&quot;, &quot;weight&quot;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b727e82b0&gt; . 맷플롯립의 함수가 지원하는 다른 매개변수를 사용할 수 있습니다. 예를 들어, 산점도를 그릴 때 맷플롯립의 scatter() 함수의 s 매개변수를 사용해 크기를 지정할 수 있습니다: . people.plot(kind = &quot;scatter&quot;, x = &quot;height&quot;, y = &quot;weight&quot;, s=[40, 120, 200]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b72827100&gt; . df4 = pd.DataFrame( { &quot;a&quot;:np.random.randn(1000)+1, &quot;b&quot;:np.random.randn(1000), &quot;c&quot;:np.random.randn(1000)-1, }, columns = [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;], ) #plt.figure() df4.plot.hist(alpha=0.5) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b7390e940&gt; . df4[&quot;a&quot;].plot.hist(alpha=0.5) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b739a0880&gt; . df = pd.DataFrame(np.random.rand(10,5), columns = [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;]) df.plot.box() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b73a264f0&gt; . df = pd.DataFrame(np.random.rand(10,2),columns=[&quot;Col1&quot;,&quot;Col2&quot;]) df[&quot;X&quot;] = pd.Series([&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;]) df . Col1 Col2 X . 0 0.573811 | 0.904532 | A | . 1 0.456202 | 0.791875 | A | . 2 0.027375 | 0.962137 | A | . 3 0.155703 | 0.116007 | A | . 4 0.646041 | 0.749334 | A | . 5 0.168754 | 0.928601 | B | . 6 0.872871 | 0.535532 | B | . 7 0.559621 | 0.786656 | B | . 8 0.874239 | 0.443472 | B | . 9 0.980164 | 0.649904 | B | . bp = df.boxplot(by=&quot;X&quot;) . 선택할 수 있는 옵션이 많습니다. 판다스 문서의 시각화 페이지에서 마음에 드는 그래프를 찾아 예제 코드를 살펴 보세요. . DataFrame &#50672;&#49328; . DataFrame이 넘파이 배열을 흉내내려는 것은 아니지만 몇 가지 비슷한 점이 있습니다. 예제 DataFrame을 만들어 보죠: . grades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]]) grades = pd.DataFrame(grades_array, columns=[&quot;sep&quot;, &quot;oct&quot;, &quot;nov&quot;], index=[&quot;alice&quot;,&quot;bob&quot;,&quot;charles&quot;,&quot;darwin&quot;]) grades . sep oct nov . alice 8 | 8 | 9 | . bob 10 | 9 | 9 | . charles 4 | 8 | 2 | . darwin 9 | 10 | 10 | . DataFrame에 넘파이 수학 함수를 적용하면 모든 값에 이 함수가 적용됩니다: . np.sqrt(grades) . sep oct nov . alice 2.828427 | 2.828427 | 3.000000 | . bob 3.162278 | 3.000000 | 3.000000 | . charles 2.000000 | 2.828427 | 1.414214 | . darwin 3.000000 | 3.162278 | 3.162278 | . 비슷하게 DataFrame에 하나의 값을 더하면 DataFrame의 모든 원소에 이 값이 더해집니다. 이를 브로드캐스팅이라고 합니다: . grades + 1 . sep oct nov . alice 9 | 9 | 10 | . bob 11 | 10 | 10 | . charles 5 | 9 | 3 | . darwin 10 | 11 | 11 | . 물론 산술 연산(*,/,**...)과 조건 연산(&gt;, ==...)을 포함해 모든 이항 연산에도 마찬가지 입니다: . grades &gt;= 5 . sep oct nov . alice True | True | True | . bob True | True | True | . charles False | True | False | . darwin True | True | True | . DataFrame의 max, sum, mean 같은 집계 연산은 각 열에 적용되어 Series 객체가 반환됩니다: . grades.mean() #default = axis=0 . sep 7.75 oct 8.75 nov 7.50 dtype: float64 . grades.values.mean() . 8.0 . all 메서드도 집계 연산입니다: 모든 값이 True인지 아닌지 확인합니다. 모든 학생의 점수가 5 이상인 월을 찾아 보죠: . (grades &gt; 5).all() #전부다 true인지 아닌지 . NameError Traceback (most recent call last) &lt;ipython-input-1-17fdf982d3e7&gt; in &lt;module&gt; -&gt; 1 (grades &gt; 5).all() #전부다 true인지 아닌지 NameError: name &#39;grades&#39; is not defined . Most of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let&#39;s find out which students had all grades greater than 5: . (grades &gt; 5).all(axis = 1) #가로축 단위로 . alice True bob True charles False darwin True dtype: bool . any 메서드는 하나라도 참이면 True를 반환합니다. 한 번이라도 10점을 받은 사람을 찾아 보죠: . (grades == 10).any(axis = 1) . alice False bob True charles False darwin True dtype: bool . DataFrame에 Series 객체를 더하면 (또는 다른 이항 연산을 수행하면) 판다스는 DataFrame에 있는 모든 행에 이 연산을 브로드캐스팅합니다. 이는 Series 객체가 DataFrame의 행의 개수와 크기가 같을 때만 동작합니다. 예를 들어 DataFrame의 mean(Series 객체)을 빼보죠: . grades - grades.mean() # grades - [7.75, 8.75, 7.50] 와 동일 . sep oct nov . alice 0.25 | -0.75 | 1.5 | . bob 2.25 | 0.25 | 1.5 | . charles -3.75 | -0.75 | -5.5 | . darwin 1.25 | 1.25 | 2.5 | . 모든 9월 성적에서 7.75를 빼고, 10월 성적에서 8.75를 빼고, 11월 성적에서 7.50을 뺍니다. 이는 다음 DataFrame을 빼는 것과 같습니다: . pd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns) . sep oct nov . alice 7.75 | 8.75 | 7.5 | . bob 7.75 | 8.75 | 7.5 | . charles 7.75 | 8.75 | 7.5 | . darwin 7.75 | 8.75 | 7.5 | . 모든 성적의 전체 평균을 빼고 싶다면 다음과 같은 방법을 사용합니다: . grades - grades.values.mean() # 모든 점수에서 전체 평균(8.00)을 뺍니다 . sep oct nov . alice 0.0 | 0.0 | 1.0 | . bob 2.0 | 1.0 | 1.0 | . charles -4.0 | 0.0 | -6.0 | . darwin 1.0 | 2.0 | 2.0 | . &#51088;&#46041; &#51221;&#47148; . Series와 비슷하게 여러 개의 DataFrame에 대한 연산을 수행하면 판다스는 자동으로 행 인덱스 레이블로 정렬하지만 열 이름으로도 정렬할 수 있습니다. 10월부터 12월까지 보너스 포인트를 담은 DataFrame을 만들어 보겠습니다: . grades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]]) grades = pd.DataFrame(grades_array, columns=[&quot;sep&quot;, &quot;oct&quot;, &quot;nov&quot;], index=[&quot;alice&quot;,&quot;bob&quot;,&quot;charles&quot;,&quot;darwin&quot;]) grades . sep oct nov . alice 8 | 8 | 9 | . bob 10 | 9 | 9 | . charles 4 | 8 | 2 | . darwin 9 | 10 | 10 | . bonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]]) bonus_points = pd.DataFrame(bonus_array, columns=[&quot;oct&quot;, &quot;nov&quot;, &quot;dec&quot;], index=[&quot;bob&quot;,&quot;colin&quot;, &quot;darwin&quot;, &quot;charles&quot;]) bonus_points . oct nov dec . bob 0.0 | NaN | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . grades + bonus_points . dec nov oct sep . alice NaN | NaN | NaN | NaN | . bob NaN | NaN | 9.0 | NaN | . charles NaN | 5.0 | 11.0 | NaN | . colin NaN | NaN | NaN | NaN | . darwin NaN | 11.0 | 10.0 | NaN | . 덧셈 연산이 수행되었지만 너무 많은 원소가 NaN이 되었습니다. DataFrame을 정렬할 때 일부 열과 행이 한 쪽에만 있기 때문입니다. 다른 쪽에는 누란되었다고 간주합니다(NaN). NaN에 어떤 수를 더하면 NaN이 됩니다. . &#45572;&#46973;&#46108; &#45936;&#51060;&#53552; &#45796;&#47336;&#44592; . 실제 데이터에서 누락된 데이터를 다루는 경우는 자주 발생합니다. 판다스는 누락된 데이터를 다룰 수 있는 몇 가지 방법을 제공합니다. . 위 데이터에 있는 문제를 해결해 보죠. 예를 들어, 누락된 데이터는 NaN이 아니라 0이 되어야 한다고 결정할 수 있습니다. fillna() 메서드를 사용해 모든 NaN 값을 어떤 값으로 바꿀 수 있습니다: . (grades + bonus_points).fillna(0) . dec nov oct sep . alice 0.0 | 0.0 | 0.0 | 0.0 | . bob 0.0 | 0.0 | 9.0 | 0.0 | . charles 0.0 | 5.0 | 11.0 | 0.0 | . colin 0.0 | 0.0 | 0.0 | 0.0 | . darwin 0.0 | 11.0 | 10.0 | 0.0 | . 9월의 점수를 0으로 만드는 것은 공정하지 않습니다. 누락된 점수는 그대로 두고, 누락된 보너스 포인트는 0으로 바꿀 수 있습니다: . fixed_bonus_points = bonus_points.fillna(0) #NA 값 0으로 바꾸기 fixed_bonus_points.insert(loc = 0, column = &quot;sep&quot;,value = 0) # 누락된 컬럼 만들기 fixed_bonus_points.loc[&quot;alice&quot;] = 0 # 누락된 행 만들기 fixed_bonus_points . sep oct nov dec . bob 0 | 0.0 | 0.0 | 2.0 | . colin 0 | 0.0 | 1.0 | 0.0 | . darwin 0 | 0.0 | 1.0 | 0.0 | . charles 0 | 3.0 | 3.0 | 0.0 | . alice 0 | 0.0 | 0.0 | 0.0 | . grades + fixed_bonus_points . dec nov oct sep . alice NaN | 9.0 | 8.0 | 8.0 | . bob NaN | 9.0 | 9.0 | 10.0 | . charles NaN | 5.0 | 11.0 | 4.0 | . colin NaN | NaN | NaN | NaN | . darwin NaN | 11.0 | 10.0 | 9.0 | . 훨씬 낫네요: 일부 데이터를 꾸며냈지만 덜 불공정합니다. . 누락된 값을 다루는 또 다른 방법은 보간입니다. bonus_points DataFrame을 다시 보죠: . bonus_points . oct nov dec . bob 0.0 | NaN | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . interpolate 메서드를 사용해 보죠. 기본적으로 수직 방향(axis=0)으로 보간합니다. 따라서 수평으로(axis=1)으로 보간하도록 지정합니다. . bonus_points.interpolate(axis=1) . oct nov dec . bob 0.0 | 1.0 | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . bob의 보너스 포인트는 10월에 0이고 12월에 2입니다. 11월을 보간하면 평균 보너스 포인트 1을 얻습니다. colin의 보너스 포인트는 11월에 1이지만 9월에 포인트는 얼마인지 모릅니다. 따라서 보간할 수 없고 10월의 포인트는 그대로 누락된 값으로 남아 있습니다. 이를 해결하려면 보간하기 전에 9월의 보너스 포인트를 0으로 설정해야 합니다. . better_bonus_points = bonus_points.copy() better_bonus_points.insert(0, &quot;sep&quot;, 0) better_bonus_points.loc[&quot;alice&quot;] = 0 better_bonus_points = better_bonus_points.interpolate(axis=1) better_bonus_points . sep oct nov dec . bob 0.0 | 0.0 | 1.0 | 2.0 | . colin 0.0 | 0.5 | 1.0 | 0.0 | . darwin 0.0 | 0.0 | 1.0 | 0.0 | . charles 0.0 | 3.0 | 3.0 | 0.0 | . alice 0.0 | 0.0 | 0.0 | 0.0 | . 좋습니다. 이제 모든 보너스 포인트가 합리적으로 보간되었습니다. 최종 점수를 확인해 보죠: . grades + better_bonus_points . dec nov oct sep . alice NaN | 9.0 | 8.0 | 8.0 | . bob NaN | 10.0 | 9.0 | 10.0 | . charles NaN | 5.0 | 11.0 | 4.0 | . colin NaN | NaN | NaN | NaN | . darwin NaN | 11.0 | 10.0 | 9.0 | . 9월 열이 오른쪽에 추가되었는데 좀 이상합니다. 이는 더하려는 DataFrame이 정확히 같은 열을 가지고 있지 않기 때문입니다(grade DataFrame에는 &quot;dec&quot; 열이 없습니다). 따라서 판다스는 알파벳 순서로 최종 열을 정렬합니다. 이를 해결하려면 덧셈을 하기 전에 누락된 열을 추가하면 됩니다: . grades[&quot;dec&quot;] = np.nan final_grades = grades + better_bonus_points final_grades . sep oct nov dec . alice 8.0 | 8.0 | 9.0 | NaN | . bob 10.0 | 9.0 | 10.0 | NaN | . charles 4.0 | 11.0 | 5.0 | NaN | . colin NaN | NaN | NaN | NaN | . darwin 9.0 | 10.0 | 11.0 | NaN | . 12월과 colin에 대해 할 수 있는 것이 많지 않습니다. 보너스 포인트를 만드는 것이 나쁘지만 점수를 합리적으로 올릴 수는 없습니다(어떤 선생님들은 그럴 수 있지만). dropna() 메서드를 사용해 모두 NaN인 행을 삭제합니다: . final_grades_clean = final_grades.dropna(how=&quot;all&quot;) #axis=0 final_grades_clean . sep oct nov dec . alice 8.0 | 8.0 | 9.0 | NaN | . bob 10.0 | 9.0 | 10.0 | NaN | . charles 4.0 | 11.0 | 5.0 | NaN | . darwin 9.0 | 10.0 | 11.0 | NaN | . 그다음 axis 매개변수를 1로 지정하여 모두 NaN인 열을 삭제합니다: . final_grades_clean = final_grades_clean.dropna(axis=1, how=&quot;all&quot;) final_grades_clean . sep oct nov . alice 8.0 | 8.0 | 9.0 | . bob 10.0 | 9.0 | 10.0 | . charles 4.0 | 11.0 | 5.0 | . darwin 9.0 | 10.0 | 11.0 | . groupby&#47196; &#51665;&#44228;&#54616;&#44592; . SQL과 비슷하게 판다스는 데이터를 그룹핑하고 각 그룹에 대해 연산을 수행할 수 있습니다. . 먼저 그루핑을 위해 각 사람의 데이터를 추가로 만들겠습니다. NaN 값을 어떻게 다루는지 보기 위해 final_grades DataFrame을 다시 사용하겠습니다: . final_grades[&quot;hobby&quot;] = [&quot;Biking&quot;, &quot;Dancing&quot;, np.nan, &quot;Dancing&quot;, &quot;Biking&quot;] final_grades . sep oct nov dec hobby . alice 8.0 | 8.0 | 9.0 | NaN | Biking | . bob 10.0 | 9.0 | 10.0 | NaN | Dancing | . charles 4.0 | 11.0 | 5.0 | NaN | NaN | . colin NaN | NaN | NaN | NaN | Dancing | . darwin 9.0 | 10.0 | 11.0 | NaN | Biking | . hobby로 이 DataFrame을 그룹핑해 보죠: . grouped_grades = final_grades.groupby(&quot;hobby&quot;) grouped_grades . &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000026B73D710A0&gt; . # grouped_grades . 이제 hobby마다 평균 점수를 계산할 수 있습니다: . grouped_grades.mean() . sep oct nov dec . hobby . Biking 8.5 | 9.0 | 10.0 | NaN | . Dancing 10.0 | 9.0 | 10.0 | NaN | . final_grades.groupby(&quot;hobby&quot;).mean() . sep oct nov dec . hobby . Biking 8.5 | 9.0 | 10.0 | NaN | . Dancing 10.0 | 9.0 | 10.0 | NaN | . 아주 쉽네요! 평균을 계산할 때 NaN 값은 그냥 무시됩니다. . &#54588;&#48391; &#53580;&#51060;&#48660; . 판다스는 스프레드시트와 비슷하 피봇 테이블을 지원하여 데이터를 빠르게 요약할 수 있습니다. 어떻게 동작하는 알아 보기 위해 간단한 DataFrame을 만들어 보죠: . bonus_points.stack().reset_index() #index가 사라짐 . level_0 level_1 0 . 0 bob | oct | 0.0 | . 1 bob | dec | 2.0 | . 2 colin | nov | 1.0 | . 3 colin | dec | 0.0 | . 4 darwin | oct | 0.0 | . 5 darwin | nov | 1.0 | . 6 darwin | dec | 0.0 | . 7 charles | oct | 3.0 | . 8 charles | nov | 3.0 | . 9 charles | dec | 0.0 | . bonus_points . oct nov dec . bob 0.0 | NaN | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . more_grades = final_grades_clean.stack().reset_index() more_grades.columns = [&quot;name&quot;, &quot;month&quot;, &quot;grade&quot;] more_grades[&quot;bonus&quot;] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0] more_grades . name month grade bonus . 0 alice | sep | 8.0 | NaN | . 1 alice | oct | 8.0 | NaN | . 2 alice | nov | 9.0 | NaN | . 3 bob | sep | 10.0 | 0.0 | . 4 bob | oct | 9.0 | NaN | . 5 bob | nov | 10.0 | 2.0 | . 6 charles | sep | 4.0 | 3.0 | . 7 charles | oct | 11.0 | 3.0 | . 8 charles | nov | 5.0 | 0.0 | . 9 darwin | sep | 9.0 | 0.0 | . 10 darwin | oct | 10.0 | 1.0 | . 11 darwin | nov | 11.0 | 0.0 | . 이제 이 DataFrame에 대해 pd.pivot_table() 함수를 호출하고 name 열로 그룹핑합니다. 기본적으로 pivot_table()은 수치 열의 평균을 계산합니다: . pd.pivot_table(more_grades, index=&quot;name&quot;) . bonus grade . name . alice NaN | 8.333333 | . bob 1.000000 | 9.666667 | . charles 2.000000 | 6.666667 | . darwin 0.333333 | 10.000000 | . 집계 함수를 aggfunc 매개변수로 바꿀 수 있습니다. 또한 집계 대상의 열을 리스트로 지정할 수 있습니다: . pd.pivot_table(more_grades, index=&quot;name&quot;, values=[&quot;grade&quot;,&quot;bonus&quot;], aggfunc=np.max) . bonus grade . name . alice NaN | 9.0 | . bob 2.0 | 10.0 | . charles 3.0 | 11.0 | . darwin 1.0 | 11.0 | . columns 매개변수를 지정하여 수평으로 집계할 수 있고 margins=True로 설정해 각 행과 열에 대해 전체 합을 계산할 수 있습니다: . pd.pivot_table(more_grades, index=&quot;name&quot;, values=&quot;grade&quot;, columns=&quot;month&quot;, margins=True) . month nov oct sep All . name . alice 9.00 | 8.0 | 8.00 | 8.333333 | . bob 10.00 | 9.0 | 10.00 | 9.666667 | . charles 5.00 | 11.0 | 4.00 | 6.666667 | . darwin 11.00 | 10.0 | 9.00 | 10.000000 | . All 8.75 | 9.5 | 7.75 | 8.666667 | . 마지막으로 여러 개의 인덱스나 열 이름을 지정하면 판다스가 다중 레벨 인덱스를 만듭니다: . pd.pivot_table(more_grades, index=(&quot;name&quot;, &quot;month&quot;), margins=True) . bonus grade . name month . alice nov NaN | 9.00 | . oct NaN | 8.00 | . sep NaN | 8.00 | . bob nov 2.000 | 10.00 | . oct NaN | 9.00 | . sep 0.000 | 10.00 | . charles nov 0.000 | 5.00 | . oct 3.000 | 11.00 | . sep 3.000 | 4.00 | . darwin nov 0.000 | 11.00 | . oct 1.000 | 10.00 | . sep 0.000 | 9.00 | . All 1.125 | 8.75 | . &#54632;&#49688; . 큰 DataFrame을 다룰 때 내용을 간단히 요약하는 것이 도움이 됩니다. 판다스는 이를 위한 몇 가지 함수를 제공합니다. 먼저 수치 값, 누락된 값, 텍스트 값이 섞인 큰 DataFrame을 만들어 보죠. 주피터 노트북은 이 DataFrame의 일부만 보여줍니다: . much_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26)) large_df = pd.DataFrame(much_data, columns=list(&quot;ABCDEFGHIJKLMNOPQRSTUVWXYZ&quot;)) large_df[large_df % 16 == 0] = np.nan large_df.insert(3,&quot;some_text&quot;, &quot;Blabla&quot;) large_df . A B C some_text D E F G H I ... Q R S T U V W X Y Z . 0 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 1 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 2 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 3 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 4 44.0 | 55.0 | 88.0 | Blabla | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | ... | 55.0 | 44.0 | 55.0 | 88.0 | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 9995 NaN | NaN | 33.0 | Blabla | 88.0 | 165.0 | 77.0 | 11.0 | 154.0 | 132.0 | ... | NaN | NaN | NaN | 33.0 | 88.0 | 165.0 | 77.0 | 11.0 | 154.0 | 132.0 | . 9996 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 9997 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 9998 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 9999 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 10000 rows × 27 columns . head() 메서드는 처음 5개 행을 반환합니다: . large_df.head() . A B C some_text D E F G H I ... Q R S T U V W X Y Z . 0 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 1 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 2 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 3 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 4 44.0 | 55.0 | 88.0 | Blabla | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | ... | 55.0 | 44.0 | 55.0 | 88.0 | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | . 5 rows × 27 columns . 마지막 5개 행을 반환하는 tail() 함수도 있습니다. 원하는 행 개수를 전달할 수도 있습니다: . large_df.tail(n=2) . A B C some_text D E F G H I ... Q R S T U V W X Y Z . 9998 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 9999 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 2 rows × 27 columns . info() 메서드는 각 열의 내용을 요약하여 출력합니다: . large_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10000 entries, 0 to 9999 Data columns (total 27 columns): # Column Non-Null Count Dtype -- -- 0 A 8823 non-null float64 1 B 8824 non-null float64 2 C 8824 non-null float64 3 some_text 10000 non-null object 4 D 8824 non-null float64 5 E 8822 non-null float64 6 F 8824 non-null float64 7 G 8824 non-null float64 8 H 8822 non-null float64 9 I 8823 non-null float64 10 J 8823 non-null float64 11 K 8822 non-null float64 12 L 8824 non-null float64 13 M 8824 non-null float64 14 N 8822 non-null float64 15 O 8824 non-null float64 16 P 8824 non-null float64 17 Q 8824 non-null float64 18 R 8823 non-null float64 19 S 8824 non-null float64 20 T 8824 non-null float64 21 U 8824 non-null float64 22 V 8822 non-null float64 23 W 8824 non-null float64 24 X 8824 non-null float64 25 Y 8822 non-null float64 26 Z 8823 non-null float64 dtypes: float64(26), object(1) memory usage: 2.1+ MB . 마지막으로 describe() 메서드는 각 열에 대한 주요 집계 연산을 수행한 결과를 보여줍니다: . Finally, the describe() method gives a nice overview of the main aggregated values over each column: . count: null(NaN)이 아닌 값의 개수 | mean: null이 아닌 값의 평균 | std: null이 아닌 값의 표준 편차 | min: null이 아닌 값의 최솟값 | 25%, 50%, 75%: null이 아닌 값의 25번째, 50번째, 75번째 백분위수 | max: null이 아닌 값의 최댓값 | . large_df.describe() . A B C D E F G H I J ... Q R S T U V W X Y Z . count 8823.000000 | 8824.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8823.000000 | 8823.000000 | ... | 8824.000000 | 8823.000000 | 8824.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8823.000000 | . mean 87.977559 | 87.972575 | 87.987534 | 88.012466 | 87.983791 | 88.007480 | 87.977561 | 88.000000 | 88.022441 | 88.022441 | ... | 87.972575 | 87.977559 | 87.972575 | 87.987534 | 88.012466 | 87.983791 | 88.007480 | 87.977561 | 88.000000 | 88.022441 | . std 47.535911 | 47.535523 | 47.521679 | 47.521679 | 47.535001 | 47.519371 | 47.529755 | 47.536879 | 47.535911 | 47.535911 | ... | 47.535523 | 47.535911 | 47.535523 | 47.521679 | 47.521679 | 47.535001 | 47.519371 | 47.529755 | 47.536879 | 47.535911 | . min 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | ... | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | . 25% 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | ... | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | . 50% 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | ... | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | . 75% 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | ... | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | . max 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | ... | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | . 8 rows × 26 columns . &#51200;&#51109; &amp; &#47196;&#46377; . 판다스는 DataFrame를 여러 가지 포맷으로 저장할 수 있습니다. CSV, Excel, JSON, HTML, HDF5, SQL 데이터베이스 같은 포맷이 가능합니다. 예제를 위해 DataFrame을 하나 만들어 보겠습니다: . my_df = pd.DataFrame( [[&quot;Biking&quot;, 68.5, 1985, np.nan], [&quot;Dancing&quot;, 83.1, 1984, 3]], columns=[&quot;hobby&quot;,&quot;weight&quot;,&quot;birthyear&quot;,&quot;children&quot;], index=[&quot;alice&quot;, &quot;bob&quot;] ) my_df . hobby weight birthyear children . alice Biking | 68.5 | 1985 | NaN | . bob Dancing | 83.1 | 1984 | 3.0 | . &#51200;&#51109; . CSV, HTML, JSON로 저장해 보죠: . my_df.to_csv(&quot;my_df.csv&quot;) my_df.to_html(&quot;my_df.html&quot;) my_df.to_json(&quot;my_df.json&quot;) . 저장된 내용을 확인해 보죠: . for filename in (&quot;my_df.csv&quot;, &quot;my_df.html&quot;, &quot;my_df.json&quot;): print(&quot;#&quot;, filename) with open(filename, &quot;rt&quot;) as f: print(f.read()) print() . # my_df.csv ,hobby,weight,birthyear,children alice,Biking,68.5,1985, bob,Dancing,83.1,1984,3.0 # my_df.html &lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt; &lt;thead&gt; &lt;tr style=&#34;text-align: right;&#34;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;hobby&lt;/th&gt; &lt;th&gt;weight&lt;/th&gt; &lt;th&gt;birthyear&lt;/th&gt; &lt;th&gt;children&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;alice&lt;/th&gt; &lt;td&gt;Biking&lt;/td&gt; &lt;td&gt;68.5&lt;/td&gt; &lt;td&gt;1985&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;bob&lt;/th&gt; &lt;td&gt;Dancing&lt;/td&gt; &lt;td&gt;83.1&lt;/td&gt; &lt;td&gt;1984&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; # my_df.json {&#34;hobby&#34;:{&#34;alice&#34;:&#34;Biking&#34;,&#34;bob&#34;:&#34;Dancing&#34;},&#34;weight&#34;:{&#34;alice&#34;:68.5,&#34;bob&#34;:83.1},&#34;birthyear&#34;:{&#34;alice&#34;:1985,&#34;bob&#34;:1984},&#34;children&#34;:{&#34;alice&#34;:null,&#34;bob&#34;:3.0}} . 인덱스는 (이름 없이) CSV 파일의 첫 번째 열에 저장되었습니다. HTML에서는 &lt;th&gt; 태그와 JSON에서는 키로 저장되었습니다. . 다른 포맷으로 저장하는 것도 비슷합니다. 하지만 일부 포맷은 추가적인 라이브러리 설치가 필요합니다. 예를 들어, 엑셀로 저장하려면 openpyxl 라이브러리가 필요합니다: . try: my_df.to_excel(&quot;my_df.xlsx&quot;, sheet_name=&#39;People&#39;) except ImportError as e: print(e) . &#47196;&#46377; . CSV 파일을 DataFrame으로 로드해 보죠: . my_df_loaded = pd.read_csv(&quot;my_df.csv&quot;, index_col=0) my_df_loaded . hobby weight birthyear children . alice Biking | 68.5 | 1985 | NaN | . bob Dancing | 83.1 | 1984 | 3.0 | . 예상할 수 있듯이 read_json, read_html, read_excel 함수도 있습니다. 인터넷에서 데이터를 바로 읽을 수도 있습니다. 예를 들어 깃허브에서 1,000개의 U.S. 도시를 로드해 보죠: . us_cities = None try: csv_url = &quot;https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv&quot; us_cities = pd.read_csv(csv_url, index_col=0) us_cities = us_cities.head() except IOError as e: print(e) us_cities . State Population lat lon . City . Marysville Washington | 63269 | 48.051764 | -122.177082 | . Perris California | 72326 | 33.782519 | -117.228648 | . Cleveland Ohio | 390113 | 41.499320 | -81.694361 | . Worcester Massachusetts | 182544 | 42.262593 | -71.802293 | . Columbia South Carolina | 133358 | 34.000710 | -81.034814 | . 이외에도 많은 옵션이 있습니다. 특히 datetime 포맷에 관련된 옵션이 많습니다. 더 자세한 내용은 온라인 문서를 참고하세요. . DataFrame &#54633;&#52824;&#44592; . SQL &#51312;&#51064; . 판다스의 강력한 기능 중 하나는 DataFrame에 대해 SQL 같은 조인(join)을 수행할 수 있는 것입니다. 여러 종류의 조인이 지원됩니다. 이너 조인(inner join), 레프트/라이트 아우터 조인(left/right outer join), 풀 조인(full join)입니다. 이에 대해 알아 보기 위해 간단한 DataFrame을 만들어 보죠: . city_loc = pd.DataFrame( [ [&quot;CA&quot;, &quot;San Francisco&quot;, 37.781334, -122.416728], [&quot;NY&quot;, &quot;New York&quot;, 40.705649, -74.008344], [&quot;FL&quot;, &quot;Miami&quot;, 25.791100, -80.320733], [&quot;OH&quot;, &quot;Cleveland&quot;, 41.473508, -81.739791], [&quot;UT&quot;, &quot;Salt Lake City&quot;, 40.755851, -111.896657] ], columns=[&quot;state&quot;, &quot;city&quot;, &quot;lat&quot;, &quot;lng&quot;]) city_loc . state city lat lng . 0 CA | San Francisco | 37.781334 | -122.416728 | . 1 NY | New York | 40.705649 | -74.008344 | . 2 FL | Miami | 25.791100 | -80.320733 | . 3 OH | Cleveland | 41.473508 | -81.739791 | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | . city_pop = pd.DataFrame( [ [808976, &quot;San Francisco&quot;, &quot;California&quot;], [8363710, &quot;New York&quot;, &quot;New-York&quot;], [413201, &quot;Miami&quot;, &quot;Florida&quot;], [2242193, &quot;Houston&quot;, &quot;Texas&quot;] ], index=[3,4,5,6], columns=[&quot;population&quot;, &quot;city&quot;, &quot;state&quot;]) city_pop . population city state . 3 808976 | San Francisco | California | . 4 8363710 | New York | New-York | . 5 413201 | Miami | Florida | . 6 2242193 | Houston | Texas | . 이제 merge() 함수를 사용해 이 DataFrame을 조인해 보죠: . pd.merge(left=city_loc, right=city_pop, on=&quot;city&quot;) . state_x city lat lng population state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976 | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710 | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201 | Florida | . 두 DataFrame은 state란 이름의 열을 가지고 있으므로 state_x와 state_y로 이름이 바뀌었습니다. . 또한 Cleveland, Salt Lake City, Houston은 두 DataFrame에 모두 존재하지 않기 때문에 삭제되었습니다. SQL의 INNER JOIN과 동일합니다. 도시를 삭제하지 않고 NaN으로 채우는 FULL OUTER JOIN을 원하면 how=&quot;outer&quot;로 지정합니다: . all_cities = pd.merge(left=city_loc, right=city_pop, on=&quot;city&quot;, how=&quot;outer&quot;) all_cities . state_x city lat lng population state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976.0 | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710.0 | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201.0 | Florida | . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | NaN | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | NaN | NaN | . 5 NaN | Houston | NaN | NaN | 2242193.0 | Texas | . 물론 LEFT OUTER JOIN은 how=&quot;left&quot;로 지정할 수 있습니다. 왼쪽의 DataFrame에 있는 도시만 남습니다. 비슷하게 how=&quot;right&quot;는 오른쪽 DataFrame에 있는 도시만 결과에 남습니다. 예를 들면: . pd.merge(left=city_loc, right=city_pop, on=&quot;city&quot;, how=&quot;right&quot;) . state_x city lat lng population state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976 | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710 | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201 | Florida | . 3 NaN | Houston | NaN | NaN | 2242193 | Texas | . 조인할 키가 DataFrame 인덱스라면 left_index=True나 right_index=True로 지정해야 합니다. 키 열의 이름이 다르면 left_on과 right_on을 사용합니다. 예를 들어: . city_pop2 = city_pop.copy() city_pop2.columns = [&quot;population&quot;, &quot;name&quot;, &quot;state&quot;] pd.merge(left=city_loc, right=city_pop2, left_on=&quot;city&quot;, right_on=&quot;name&quot;) . state_x city lat lng population name state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976 | San Francisco | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710 | New York | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201 | Miami | Florida | . &#50672;&#44208; . DataFrame을 조인하는 대신 그냥 연결할 수도 있습니다. concat() 함수가 하는 일입니다: . result_concat = pd.concat([city_loc, city_pop]) result_concat . state city lat lng population . 0 CA | San Francisco | 37.781334 | -122.416728 | NaN | . 1 NY | New York | 40.705649 | -74.008344 | NaN | . 2 FL | Miami | 25.791100 | -80.320733 | NaN | . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | NaN | . 3 California | San Francisco | NaN | NaN | 808976.0 | . 4 New-York | New York | NaN | NaN | 8363710.0 | . 5 Florida | Miami | NaN | NaN | 413201.0 | . 6 Texas | Houston | NaN | NaN | 2242193.0 | . 이 연산은 (행을 따라) 수직적으로 데이터를 연결하고 (열을 따라) 수평으로 연결하지 않습니다. 이 예에서 동일한 인덱스를 가진 행이 있습니다(예를 들면 3). 판다스는 이를 우아하게 처리합니다: . result_concat.loc[3] . state city lat lng population . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | . 3 California | San Francisco | NaN | NaN | 808976.0 | . 또는 인덱스를 무시하도록 설정할 수 있습니다: . pd.concat([city_loc, city_pop], ignore_index=True) . state city lat lng population . 0 CA | San Francisco | 37.781334 | -122.416728 | NaN | . 1 NY | New York | 40.705649 | -74.008344 | NaN | . 2 FL | Miami | 25.791100 | -80.320733 | NaN | . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | NaN | . 5 California | San Francisco | NaN | NaN | 808976.0 | . 6 New-York | New York | NaN | NaN | 8363710.0 | . 7 Florida | Miami | NaN | NaN | 413201.0 | . 8 Texas | Houston | NaN | NaN | 2242193.0 | . 한 DataFrame에 열이 없을 때 NaN이 채워져 있는 것처럼 동작합니다. join=&quot;inner&quot;로 설정하면 양쪽의 DataFrame에 존재하는 열만 반환됩니다: . pd.concat([city_loc, city_pop], join=&quot;inner&quot;) . state city . 0 CA | San Francisco | . 1 NY | New York | . 2 FL | Miami | . 3 OH | Cleveland | . 4 UT | Salt Lake City | . 3 California | San Francisco | . 4 New-York | New York | . 5 Florida | Miami | . 6 Texas | Houston | . axis=1로 설정하면 DataFrame을 수직이 아니라 수평으로 연결할 수 있습니다: . pd.concat([city_loc, city_pop], axis=1) . state city lat lng population city state . 0 CA | San Francisco | 37.781334 | -122.416728 | NaN | NaN | NaN | . 1 NY | New York | 40.705649 | -74.008344 | NaN | NaN | NaN | . 2 FL | Miami | 25.791100 | -80.320733 | NaN | NaN | NaN | . 3 OH | Cleveland | 41.473508 | -81.739791 | 808976.0 | San Francisco | California | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | 8363710.0 | New York | New-York | . 5 NaN | NaN | NaN | NaN | 413201.0 | Miami | Florida | . 6 NaN | NaN | NaN | NaN | 2242193.0 | Houston | Texas | . 이 경우 인덱스가 잘 정렬되지 않기 때문에 의미가 없습니다(예를 들어 Cleveland와 San Francisco의 인덱스 레이블이 3이기 때문에 동일한 행에 놓여 있습니다). 이 DataFrame을 연결하기 전에 도시로 인덱스를 재설정해 보죠: . pd.concat([city_loc.set_index(&quot;city&quot;), city_pop.set_index(&quot;city&quot;)], axis=1) . state lat lng population state . city . San Francisco CA | 37.781334 | -122.416728 | 808976.0 | California | . New York NY | 40.705649 | -74.008344 | 8363710.0 | New-York | . Miami FL | 25.791100 | -80.320733 | 413201.0 | Florida | . Cleveland OH | 41.473508 | -81.739791 | NaN | NaN | . Salt Lake City UT | 40.755851 | -111.896657 | NaN | NaN | . Houston NaN | NaN | NaN | 2242193.0 | Texas | . FULL OUTER JOIN을 수행한 것과 비슷합니다. 하지만 state 열이 state_x와 state_y로 바뀌지 않았고 city 열이 인덱스가 되었습니다. . append() 메서드는 DataFrame을 수직으로 연결하는 단축 메서드입니다: . city_loc.append(city_pop) . state city lat lng population . 0 CA | San Francisco | 37.781334 | -122.416728 | NaN | . 1 NY | New York | 40.705649 | -74.008344 | NaN | . 2 FL | Miami | 25.791100 | -80.320733 | NaN | . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | NaN | . 3 California | San Francisco | NaN | NaN | 808976.0 | . 4 New-York | New York | NaN | NaN | 8363710.0 | . 5 Florida | Miami | NaN | NaN | 413201.0 | . 6 Texas | Houston | NaN | NaN | 2242193.0 | . 판다스의 다른 메서드와 마찬가지로 append() 메서드는 실제 city_loc을 수정하지 않습니다. 복사본을 만들어 수정한 다음 반환합니다. . &#48276;&#51452; . 범주로 표현된 값을 가진 경우가 흔합니다. 예를 들어 1은 여성, 2는 남성이거나 &quot;A&quot;는 좋은 것, &quot;B&quot;는 평균, &quot;C&quot;는 나쁜 것 등입니다. 범주형 값을 읽기 힘들고 다루기 번거롭습니다. 하지만 판다스에서는 간단합니다. 예를 들기 위해 앞서 만든 city_pop DataFrame에 범주를 표현하는 열을 추가해 보겠습니다: . city_eco = city_pop.copy() city_eco[&quot;eco_code&quot;] = [17, 17, 34, 20] city_eco . population city state eco_code . 3 808976 | San Francisco | California | 17 | . 4 8363710 | New York | New-York | 17 | . 5 413201 | Miami | Florida | 34 | . 6 2242193 | Houston | Texas | 20 | . 이제 eco_code열은 의미없는 코드입니다. 이를 바꿔 보죠. 먼저 eco_code를 기반으로 새로운 범주형 열을 만듭니다: . city_eco[&quot;economy&quot;] = city_eco[&quot;eco_code&quot;].astype(&#39;category&#39;) city_eco[&quot;economy&quot;].cat.categories . Int64Index([17, 20, 34], dtype=&#39;int64&#39;) . 의미있는 이름을 가진 범주를 지정할 수 있습니다: . city_eco[&quot;economy&quot;].cat.categories = [&quot;Finance&quot;, &quot;Energy&quot;, &quot;Tourism&quot;] city_eco . population city state eco_code economy . 3 808976 | San Francisco | California | 17 | Finance | . 4 8363710 | New York | New-York | 17 | Finance | . 5 413201 | Miami | Florida | 34 | Tourism | . 6 2242193 | Houston | Texas | 20 | Energy | . 범주형 값은 알파벳 순서가 아니라 범주형 순서로 정렬합니다: . city_eco.sort_values(by=&quot;economy&quot;, ascending=False) . population city state eco_code economy . 5 413201 | Miami | Florida | 34 | Tourism | . 6 2242193 | Houston | Texas | 20 | Energy | . 3 808976 | San Francisco | California | 17 | Finance | . 4 8363710 | New York | New-York | 17 | Finance | . &#44536; &#45796;&#51020;&#50644;? . 이제 알았겠지만 판다스는 매우 커다란 라이브러리이고 기능이 많습니다. 가장 중요한 기능들을 둘러 보았지만 빙산의 일각일 뿐입니다. 더 많은 것을 익히려면 실전 데이터로 직접 실습해 보는 것이 제일 좋습니다. 판다스의 훌륭한 문서와 쿡북을 보는 것도 좋습니다. .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/jupyter/python/2022/03/16/pandas.html",
            "relUrl": "/jupyter/python/2022/03/16/pandas.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Markdown Cheat Sheet",
            "content": "Markdown Cheat Sheet . Thanks for visiting The Markdown Guide! . This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for basic syntax and extended syntax. . Basic Syntax . These are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements. . Heading . H1 . H2 . H3 . Bold . bold text . Italic . italicized text . Blockquote . blockquote . Ordered List . First item | Second item | Third item | Unordered List . First item | Second item | Third item | . Code . code . Horizontal Rule . . Link . Markdown Guide . Image . . Extended Syntax . These elements extend the basic syntax by adding additional features. Not all Markdown applications support these elements. . Table . Syntax Description . Header | Title | . Paragraph | Text | . Fenced Code Block . { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;age&quot;: 25 } . Footnote . Here’s a sentence with a footnote. 1 . Heading ID . My Great Heading . Definition List . term definition Strikethrough . The world is flat. . Task List . Write the press release | Update the website | Contact the media | . Emoji . That is so funny! :joy: . (See also Copying and Pasting Emoji) . Highlight . I need to highlight these ==very important words==. . Subscript . H~2~O . Superscript . X^2^ . This is the footnote. &#8617; . |",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/markdown/2022/03/11/Markdown-basics.md.html",
            "relUrl": "/markdown/2022/03/11/Markdown-basics.md.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About me . 빅데이터 응용학과 20202797 양정현입니다. . 관심분야 . 빅데이터분석 .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}