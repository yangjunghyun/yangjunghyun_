{
  
    
        "post0": {
            "title": "데이터 마이닝 프로젝트",
            "content": "&#47785;&#52264; . 인구데이터기반소득예측경진대회 | 데이터 전처리 데이터 가져오기 | 결측치 | . | EDA 상관관계 분석 범주형 상관관계 분석 | 수치형 상관관계 분석 | 필요없는 변수 제거 | . | 수치형 변수 age | education.num | capital.gain | capital.loss | hours.per.week | sex | . | 범주형 변수 변수 고유값 확인 | workclass | marital.status | occupation | relationship | . | . | 모델링 원핫인코딩 | pycaret | submission | . | 결론 summary | 활용가능 | 한계 | 향후계획 | . | . &#51064;&#44396; &#45936;&#51060;&#53552; &#44592;&#48152; &#49548;&#46301; &#50696;&#52769;&#44221;&#51652;&#45824;&#54924; . 다양한 인구 데이터 바탕으로 소득이 5만달러 이하인지 초과인지 분류하는 대회. | 분류 해야할 클래스는 5만달러 이하인지 초과인지 수입(target)을 총 두가지로 분류하는 대회. | . 대회 링크: https://dacon.io/competitions/official/235892/overview/description . 다양한 공모전 중 머신러닝 관련 공모전에 참여하고 싶었고, Basic 공모전이라 모델링까지 내가 해볼 수 있을 것 같아 이 공모전을 선택하였다. 또, 소득을 예측하기 위해 필요한 변수들 또한 한글 데이터로 어렵지 않게 구성되어있었으며, 어떠한 변수들이 소득에 영향을 주는 지 궁금하였다. . &#44592;&#48376;&#49444;&#51221; . id : 샘플 아이디 age : 나이 workclass : 일 유형 fnlwgt : CPS(Current Population Survey) 가중치 education : 교육수준 education.num : 교육수준 번호 marital.status : 결혼 상태 occupation : 직업 relationship : 가족관계 race : 인종 sex : 성별 capital.gain : 자본 이익 capital.loss : 자본 손실 hours.per.week : 주당 근무시간 native.country : 본 국적 target : 소득 ` . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . 먼저 pycaret과 markupsafe을 설치 후 런타임 재시작을 해야 pycaret이 돌아감 . !pip install pycaret . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting pycaret Downloading pycaret-2.3.10-py3-none-any.whl (320 kB) |████████████████████████████████| 320 kB 5.1 MB/s Collecting pandas-profiling&gt;=2.8.0 Downloading pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB) |████████████████████████████████| 262 kB 63.0 MB/s Requirement already satisfied: yellowbrick&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.4) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.2) Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.5.0) Collecting lightgbm&gt;=2.3.1 Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB) |████████████████████████████████| 2.0 MB 62.4 MB/s Requirement already satisfied: cufflinks&gt;=0.17.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.17.3) Requirement already satisfied: plotly&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from pycaret) (5.5.0) Requirement already satisfied: pyyaml&lt;6.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.13) Requirement already satisfied: numba&lt;0.55 in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.51.2) Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.5.0) Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.3.5) Collecting kmodes&gt;=0.10.1 Downloading kmodes-0.12.1-py2.py3-none-any.whl (20 kB) Collecting Boruta Downloading Boruta-0.3-py3-none-any.whl (56 kB) |████████████████████████████████| 56 kB 4.7 MB/s Collecting scikit-plot Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB) Requirement already satisfied: scipy&lt;=1.5.4 in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.4.1) Requirement already satisfied: spacy&lt;2.4.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (2.2.4) Collecting mlxtend&gt;=0.17.0 Downloading mlxtend-0.20.0-py2.py3-none-any.whl (1.3 MB) |████████████████████████████████| 1.3 MB 49.4 MB/s Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from pycaret) (7.7.0) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pycaret) (1.1.0) Collecting scikit-learn==0.23.2 Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB) |████████████████████████████████| 6.8 MB 50.1 MB/s Collecting mlflow Downloading mlflow-1.26.1-py3-none-any.whl (17.8 MB) |████████████████████████████████| 17.8 MB 57.6 MB/s Collecting umap-learn Downloading umap-learn-0.5.3.tar.gz (88 kB) |████████████████████████████████| 88 kB 8.0 MB/s Collecting pyod Downloading pyod-1.0.1.tar.gz (120 kB) |████████████████████████████████| 120 kB 71.0 MB/s Collecting pyLDAvis Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB) |████████████████████████████████| 1.7 MB 43.1 MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Installing backend dependencies ... done Preparing wheel metadata ... done Collecting imbalanced-learn==0.7.0 Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB) |████████████████████████████████| 167 kB 63.8 MB/s Requirement already satisfied: gensim&lt;4.0.0 in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.6.0) Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pycaret) (3.2.5) Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.11.2) Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from pycaret) (0.15.3) Requirement already satisfied: numpy&gt;=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn==0.7.0-&gt;pycaret) (1.21.6) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2-&gt;pycaret) (3.1.0) Requirement already satisfied: setuptools&gt;=34.4.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (57.4.0) Requirement already satisfied: colorlover&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (0.3.0) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from cufflinks&gt;=0.17.0-&gt;pycaret) (1.15.0) Requirement already satisfied: smart-open&gt;=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim&lt;4.0.0-&gt;pycaret) (6.0.0) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (2.6.1) Requirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (5.1.1) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (1.0.18) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (0.7.5) Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (4.8.0) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (4.4.2) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from IPython-&gt;pycaret) (0.8.1) Requirement already satisfied: ipykernel&gt;=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (4.10.1) Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (3.6.0) Requirement already satisfied: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (5.4.0) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (1.1.0) Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;pycaret) (0.2.0) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (5.3.5) Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (5.1.1) Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm&gt;=2.3.1-&gt;pycaret) (0.37.1) Collecting mlxtend&gt;=0.17.0 Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB) |████████████████████████████████| 1.3 MB 66.4 MB/s Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (0.11.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (1.4.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (3.0.9) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;pycaret) (2.8.2) Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib-&gt;pycaret) (4.2.0) Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (2.15.3) Requirement already satisfied: jsonschema&gt;=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.3.3) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.10.0) Requirement already satisfied: importlib-resources&gt;=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (5.7.1) Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,&gt;=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (0.18.1) Requirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (21.4.0) Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema&gt;=2.6-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (4.11.4) Requirement already satisfied: zipp&gt;=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources&gt;=1.4.0-&gt;jsonschema&gt;=2.6-&gt;nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;pycaret) (3.8.0) Requirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba&lt;0.55-&gt;pycaret) (0.34.0) Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;pycaret) (2022.1) Collecting phik&gt;=0.11.1 Downloading phik-0.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (690 kB) |████████████████████████████████| 690 kB 78.3 MB/s Collecting pydantic&gt;=1.8.1 Downloading pydantic-1.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB) |████████████████████████████████| 11.1 MB 51.9 MB/s Collecting htmlmin&gt;=0.1.12 Downloading htmlmin-0.1.12.tar.gz (19 kB) Requirement already satisfied: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.11.3) Collecting markupsafe~=2.1.1 Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB) Collecting tangled-up-in-unicode==0.2.0 Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB) |████████████████████████████████| 4.7 MB 47.9 MB/s Requirement already satisfied: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (0.5.1) Collecting pyyaml&lt;6.0.0 Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB) |████████████████████████████████| 636 kB 69.9 MB/s Requirement already satisfied: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling&gt;=2.8.0-&gt;pycaret) (4.64.0) Collecting requests&gt;=2.24.0 Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB) |████████████████████████████████| 63 kB 1.9 MB/s Collecting multimethod&gt;=1.4 Downloading multimethod-1.8-py3-none-any.whl (9.8 kB) Collecting visions[type_image_path]==0.7.4 Downloading visions-0.7.4-py3-none-any.whl (102 kB) |████████████████████████████████| 102 kB 12.8 MB/s Requirement already satisfied: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.6.3) Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (7.1.2) Collecting imagehash Downloading ImageHash-4.2.1.tar.gz (812 kB) |████████████████████████████████| 812 kB 59.8 MB/s Collecting scipy&lt;=1.5.4 Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB) |████████████████████████████████| 25.9 MB 75.3 MB/s Requirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly&gt;=4.4.1-&gt;pycaret) (8.0.1) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;IPython-&gt;pycaret) (0.2.5) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.0.12) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.24.3) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (2022.5.18.1) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (2.0.6) Requirement already satisfied: catalogue&lt;1.1.0,&gt;=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.0) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (0.9.1) Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (7.4.0) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (3.0.6) Requirement already satisfied: srsly&lt;1.1.0,&gt;=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.5) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.0.7) Requirement already satisfied: blis&lt;0.5.0,&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (0.4.1) Requirement already satisfied: plac&lt;1.2.0,&gt;=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy&lt;2.4.0-&gt;pycaret) (1.1.3) Requirement already satisfied: notebook&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (5.3.1) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (5.6.1) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (1.8.0) Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.13.3) Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel&gt;=4.5.1-&gt;ipywidgets-&gt;pycaret) (23.0.0) Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado&gt;=0.8.1-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.7.0) Collecting yellowbrick&gt;=1.0.1 Downloading yellowbrick-1.3.post1-py3-none-any.whl (271 kB) |████████████████████████████████| 271 kB 57.1 MB/s Collecting numpy&gt;=1.13.3 Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB) |████████████████████████████████| 14.8 MB 46.3 MB/s Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash-&gt;visions[type_image_path]==0.7.4-&gt;pandas-profiling&gt;=2.8.0-&gt;pycaret) (1.3.0) Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.3.0) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (21.3) Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.4.36) Requirement already satisfied: protobuf&gt;=3.12.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (3.17.3) Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (0.4) Collecting prometheus-flask-exporter Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB) Collecting querystring-parser Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB) Collecting alembic Downloading alembic-1.8.0-py3-none-any.whl (209 kB) |████████████████████████████████| 209 kB 66.5 MB/s Collecting databricks-cli&gt;=0.8.7 Downloading databricks-cli-0.16.6.tar.gz (62 kB) |████████████████████████████████| 62 kB 871 kB/s Requirement already satisfied: sqlparse&gt;=0.3.1 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (0.4.2) Collecting docker&gt;=4.0.0 Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB) |████████████████████████████████| 146 kB 67.3 MB/s Requirement already satisfied: click&gt;=7.0 in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (7.1.2) Collecting gunicorn Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB) |████████████████████████████████| 79 kB 7.7 MB/s Requirement already satisfied: Flask in /usr/local/lib/python3.7/dist-packages (from mlflow-&gt;pycaret) (1.1.4) Collecting gitpython&gt;=2.1.0 Downloading GitPython-3.1.27-py3-none-any.whl (181 kB) |████████████████████████████████| 181 kB 74.4 MB/s Collecting pyjwt&gt;=1.7.0 Downloading PyJWT-2.4.0-py3-none-any.whl (18 kB) Requirement already satisfied: oauthlib&gt;=3.1.0 in /usr/local/lib/python3.7/dist-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow-&gt;pycaret) (3.2.0) Requirement already satisfied: tabulate&gt;=0.7.7 in /usr/local/lib/python3.7/dist-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow-&gt;pycaret) (0.8.9) Collecting websocket-client&gt;=0.32.0 Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB) |████████████████████████████████| 54 kB 3.1 MB/s Collecting gitdb&lt;5,&gt;=4.0.1 Downloading gitdb-4.0.9-py3-none-any.whl (63 kB) |████████████████████████████████| 63 kB 1.9 MB/s Collecting smmap&lt;6,&gt;=3.0.1 Downloading smmap-5.0.0-py3-none-any.whl (24 kB) Collecting Mako Downloading Mako-1.2.0-py3-none-any.whl (78 kB) |████████████████████████████████| 78 kB 7.3 MB/s Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy-&gt;mlflow-&gt;pycaret) (1.1.2) Requirement already satisfied: itsdangerous&lt;2.0,&gt;=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask-&gt;mlflow-&gt;pycaret) (1.1.0) Requirement already satisfied: Werkzeug&lt;2.0,&gt;=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask-&gt;mlflow-&gt;pycaret) (1.0.1) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.6.0) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (5.0.0) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.7.1) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.8.4) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (1.5.0) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=4.4.1-&gt;widgetsnbextension~=3.6.0-&gt;ipywidgets-&gt;pycaret) (0.5.1) Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from prometheus-flask-exporter-&gt;mlflow-&gt;pycaret) (0.14.1) Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (0.16.0) Collecting funcy Downloading funcy-1.17-py2.py3-none-any.whl (33 kB) Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (0.0) Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis-&gt;pycaret) (2.8.1) Collecting pyLDAvis Downloading pyLDAvis-3.3.0.tar.gz (1.7 MB) |████████████████████████████████| 1.7 MB 58.7 MB/s Installing build dependencies ... done Getting requirements to build wheel ... done Installing backend dependencies ... done Preparing wheel metadata ... done Downloading pyLDAvis-3.2.2.tar.gz (1.7 MB) |████████████████████████████████| 1.7 MB 58.7 MB/s Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod-&gt;pycaret) (0.10.2) Requirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels-&gt;pyod-&gt;pycaret) (0.5.2) Collecting pynndescent&gt;=0.5 Downloading pynndescent-0.5.7.tar.gz (1.1 MB) |████████████████████████████████| 1.1 MB 67.1 MB/s Building wheels for collected packages: htmlmin, imagehash, databricks-cli, pyLDAvis, pyod, umap-learn, pynndescent Building wheel for htmlmin (setup.py) ... done Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=69ef1154b4bacbfcb58227dbe4363f3f8497216094a09a004f7518e922c1d492 Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655 Building wheel for imagehash (setup.py) ... done Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295206 sha256=f09d8a8b915bdd8854382614082bca742c58f95c60a145631c39e9e22ef96d56 Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e Building wheel for databricks-cli (setup.py) ... done Created wheel for databricks-cli: filename=databricks_cli-0.16.6-py3-none-any.whl size=112631 sha256=90efa6020a8989aaab730760f891ed61e2b3291aadc934c57bf9f588fa9371fd Stored in directory: /root/.cache/pip/wheels/96/c1/f8/d75a22e789ab6a4dff11f18338c3af4360189aa371295cc934 Building wheel for pyLDAvis (setup.py) ... done Created wheel for pyLDAvis: filename=pyLDAvis-3.2.2-py2.py3-none-any.whl size=135617 sha256=ed8de7f00e9ab8a32da34087c60dc361406ae470ed2e3b19b6a400f746e64c52 Stored in directory: /root/.cache/pip/wheels/f8/b1/9b/560ac1931796b7303f7b517b949d2d31a4fbc512aad3b9f284 Building wheel for pyod (setup.py) ... done Created wheel for pyod: filename=pyod-1.0.1-py3-none-any.whl size=147473 sha256=9625f0979644a7a662d921e3d914a373fb4262f841e30e9e22fdb575e923ab9a Stored in directory: /root/.cache/pip/wheels/ea/c4/29/67ad87835b209f72e4706369c683741b09490f2829d64ea768 Building wheel for umap-learn (setup.py) ... done Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=e9ade9f4e289696ab1d7701ba4d4f7bb018bf5df5a0b011250e41ca428fa1d88 Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821 Building wheel for pynndescent (setup.py) ... done Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=be396b5d049f2044f81597f2b274beb2f71a14ee94f3e4b6cf1550c26f2be5ba Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c Successfully built htmlmin imagehash databricks-cli pyLDAvis pyod umap-learn pynndescent Installing collected packages: markupsafe, numpy, tangled-up-in-unicode, smmap, scipy, multimethod, websocket-client, visions, scikit-learn, requests, pyjwt, Mako, imagehash, gitdb, querystring-parser, pyyaml, pynndescent, pydantic, prometheus-flask-exporter, phik, htmlmin, gunicorn, gitpython, funcy, docker, databricks-cli, alembic, yellowbrick, umap-learn, scikit-plot, pyod, pyLDAvis, pandas-profiling, mlxtend, mlflow, lightgbm, kmodes, imbalanced-learn, Boruta, pycaret Attempting uninstall: markupsafe Found existing installation: MarkupSafe 2.0.1 Uninstalling MarkupSafe-2.0.1: Successfully uninstalled MarkupSafe-2.0.1 Attempting uninstall: numpy Found existing installation: numpy 1.21.6 Uninstalling numpy-1.21.6: Successfully uninstalled numpy-1.21.6 Attempting uninstall: scipy Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Attempting uninstall: scikit-learn Found existing installation: scikit-learn 1.0.2 Uninstalling scikit-learn-1.0.2: Successfully uninstalled scikit-learn-1.0.2 Attempting uninstall: requests Found existing installation: requests 2.23.0 Uninstalling requests-2.23.0: Successfully uninstalled requests-2.23.0 Attempting uninstall: pyyaml Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: yellowbrick Found existing installation: yellowbrick 1.4 Uninstalling yellowbrick-1.4: Successfully uninstalled yellowbrick-1.4 Attempting uninstall: pandas-profiling Found existing installation: pandas-profiling 1.4.1 Uninstalling pandas-profiling-1.4.1: Successfully uninstalled pandas-profiling-1.4.1 Attempting uninstall: mlxtend Found existing installation: mlxtend 0.14.0 Uninstalling mlxtend-0.14.0: Successfully uninstalled mlxtend-0.14.0 Attempting uninstall: lightgbm Found existing installation: lightgbm 2.2.3 Uninstalling lightgbm-2.2.3: Successfully uninstalled lightgbm-2.2.3 Attempting uninstall: imbalanced-learn Found existing installation: imbalanced-learn 0.8.1 Uninstalling imbalanced-learn-0.8.1: Successfully uninstalled imbalanced-learn-0.8.1 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. xarray-einstats 0.2.2 requires numpy&gt;=1.21, but you have numpy 1.19.5 which is incompatible. tensorflow 2.8.2+zzzcolab20220527125636 requires numpy&gt;=1.20, but you have numpy 1.19.5 which is incompatible. google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible. Successfully installed Boruta-0.3 Mako-1.2.0 alembic-1.8.0 databricks-cli-0.16.6 docker-5.0.3 funcy-1.17 gitdb-4.0.9 gitpython-3.1.27 gunicorn-20.1.0 htmlmin-0.1.12 imagehash-4.2.1 imbalanced-learn-0.7.0 kmodes-0.12.1 lightgbm-3.3.2 markupsafe-2.1.1 mlflow-1.26.1 mlxtend-0.19.0 multimethod-1.8 numpy-1.19.5 pandas-profiling-3.2.0 phik-0.12.2 prometheus-flask-exporter-0.20.2 pyLDAvis-3.2.2 pycaret-2.3.10 pydantic-1.9.1 pyjwt-2.4.0 pynndescent-0.5.7 pyod-1.0.1 pyyaml-5.4.1 querystring-parser-1.2.4 requests-2.27.1 scikit-learn-0.23.2 scikit-plot-0.3.7 scipy-1.5.4 smmap-5.0.0 tangled-up-in-unicode-0.2.0 umap-learn-0.5.3 visions-0.7.4 websocket-client-1.3.2 yellowbrick-1.3.post1 . !pip install markupsafe==2.0.1 . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Collecting markupsafe==2.0.1 Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (31 kB) Installing collected packages: markupsafe Attempting uninstall: markupsafe Found existing installation: MarkupSafe 2.1.1 Uninstalling MarkupSafe-2.1.1: Successfully uninstalled MarkupSafe-2.1.1 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. pandas-profiling 3.2.0 requires markupsafe~=2.1.1, but you have markupsafe 2.0.1 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. Successfully installed markupsafe-2.0.1 . import matplotlib.pyplot as plt from pandas.plotting import scatter_matrix import seaborn as sns import platform # 그래프 폰트 설정 # Window if platform.system() == &#39;Windows&#39;: plt.rc(&#39;font&#39;, family=&#39;Malgun Gothic&#39;) elif platform.system() == &#39;Darwin&#39;: # Mac plt.rc(&#39;font&#39;, family=&#39;AppleGothic&#39;) else: #linux plt.rc(&#39;font&#39;, family=&#39;NanumGothic&#39;) # 그래프에 마이너스 표시가 되도록 변경 plt.rcParams[&#39;axes.unicode_minus&#39;] = False # 파이썬 ≥3.5 필수 import sys assert sys.version_info &gt;= (3, 5) # 사이킷런 ≥0.20 필수 # 여러가지 머신러닝 모델을 가지고 있는 라이브러리 *가장 많이 사용* import sklearn # 공통 모듈 임포트 import numpy as np import os import pandas as pd from sklearn.model_selection import train_test_split # 깔끔한 그래프 출력을 위해 plt.rcParams[&#39;font.family&#39;] = &#39;NanumGothic&#39; %matplotlib inline import matplotlib as mpl import matplotlib.pyplot as plt mpl.rc(&#39;axes&#39;, labelsize=14) mpl.rc(&#39;xtick&#39;, labelsize=12) mpl.rc(&#39;ytick&#39;, labelsize=12) . &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . &#45936;&#51060;&#53552; &#44032;&#51256;&#50724;&#44592; . cd &#39;/content/drive/MyDrive/소득예측&#39; . /content/drive/MyDrive/소득예측 . train = pd.read_csv(&#39;train.csv&#39;) test = pd.read_csv(&#39;test.csv&#39;) submission = pd.read_csv(&#39;sample_submission.csv&#39;) . print(train.shape, test.shape) . (17480, 16) (15081, 15) . del train[&#39;education&#39;] del test[&#39;education&#39;] . &#44208;&#52769;&#52824; . train.isnull().sum() # workclass, occupation, native.country 에 결측값이 존재한다. . id 0 age 0 workclass 1836 fnlwgt 0 education.num 0 marital.status 0 occupation 1843 relationship 0 race 0 sex 0 capital.gain 0 capital.loss 0 hours.per.week 0 native.country 583 target 0 dtype: int64 . . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 17480 entries, 0 to 17479 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 id 17480 non-null int64 1 age 17480 non-null int64 2 workclass 15644 non-null object 3 fnlwgt 17480 non-null int64 4 education.num 17480 non-null int64 5 marital.status 17480 non-null object 6 occupation 15637 non-null object 7 relationship 17480 non-null object 8 race 17480 non-null object 9 sex 17480 non-null object 10 capital.gain 17480 non-null int64 11 capital.loss 17480 non-null int64 12 hours.per.week 17480 non-null int64 13 native.country 16897 non-null object 14 target 17480 non-null int64 dtypes: int64(8), object(7) memory usage: 2.0+ MB . train = train.dropna(axis=0) . EDA . &#49345;&#44288;&#44288;&#44228; &#48516;&#49437; . 수치형 자료 . &#39;id&#39;,&#39;age&#39;, &#39;fnlwgt&#39;, &#39;education.num&#39;,&#39;capital.gain&#39;, &#39;capital.loss&#39;, &#39;hours.per.week&#39;, &#39;target&#39; . 범주형 자료 &#39;workclass&#39;,&#39;marital.status&#39;,&#39;occupation&#39;,&#39;relationship&#39;,&#39;race&#39;,&#39;sex&#39;,&#39;native.country&#39; . &#48276;&#51452;&#54805; &#49345;&#44288;&#44288;&#44228; &#48516;&#49437; . train_label_encoder = train.copy() train_label_encoder . id age workclass fnlwgt education.num marital.status occupation relationship race sex capital.gain capital.loss hours.per.week native.country target . 0 0 | 32 | Private | 309513 | 12 | Married-civ-spouse | Craft-repair | Husband | White | Male | 0 | 0 | 40 | United-States | 0 | . 1 1 | 33 | Private | 205469 | 10 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 40 | United-States | 1 | . 2 2 | 46 | Private | 149949 | 10 | Married-civ-spouse | Craft-repair | Husband | White | Male | 0 | 0 | 40 | United-States | 0 | . 3 3 | 23 | Private | 193090 | 13 | Never-married | Adm-clerical | Own-child | White | Female | 0 | 0 | 30 | United-States | 0 | . 4 4 | 55 | Private | 60193 | 9 | Divorced | Adm-clerical | Not-in-family | White | Female | 0 | 0 | 40 | United-States | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 15076 15076 | 35 | Private | 337286 | 14 | Never-married | Exec-managerial | Not-in-family | Asian-Pac-Islander | Male | 0 | 0 | 40 | United-States | 0 | . 15077 15077 | 36 | Private | 182074 | 10 | Divorced | Adm-clerical | Not-in-family | White | Male | 0 | 0 | 45 | United-States | 0 | . 15078 15078 | 50 | Self-emp-inc | 175070 | 15 | Married-civ-spouse | Prof-specialty | Husband | White | Male | 0 | 0 | 45 | United-States | 1 | . 15079 15079 | 39 | Private | 202937 | 10 | Divorced | Tech-support | Not-in-family | White | Female | 0 | 0 | 40 | Poland | 0 | . 15080 15080 | 33 | Private | 96245 | 12 | Married-civ-spouse | Prof-specialty | Husband | White | Male | 0 | 0 | 50 | United-States | 0 | . 15081 rows × 15 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; label encoder . # 또, label encoding을 사용하면 분석 결과가 안좋아질 수 있으므로 복제하여 train_label_encoder 만든 뒤 상관관계분석을한다. from sklearn.preprocessing import LabelEncoder le = LabelEncoder() train_label_encoder[&#39;workclass&#39;] = le.fit_transform(train[&#39;workclass&#39;]) train_label_encoder[&#39;marital.status&#39;] = le.fit_transform(train[&#39;marital.status&#39;]) train_label_encoder[&#39;occupation&#39;] = le.fit_transform(train[&#39;occupation&#39;]) train_label_encoder[&#39;relationship&#39;] = le.fit_transform(train[&#39;relationship&#39;]) train_label_encoder[&#39;race&#39;] = le.fit_transform(train[&#39;race&#39;]) train_label_encoder[&#39;sex&#39;] = le.fit_transform(train[&#39;sex&#39;]) train_label_encoder[&#39;native.country&#39;] = le.fit_transform(train[&#39;native.country&#39;]) . train_label_encoder.head() . id age workclass fnlwgt education.num marital.status occupation relationship race sex capital.gain capital.loss hours.per.week native.country target . 0 0 | 32 | 2 | 309513 | 12 | 2 | 2 | 0 | 4 | 1 | 0 | 0 | 40 | 38 | 0 | . 1 1 | 33 | 2 | 205469 | 10 | 2 | 3 | 0 | 4 | 1 | 0 | 0 | 40 | 38 | 1 | . 2 2 | 46 | 2 | 149949 | 10 | 2 | 2 | 0 | 4 | 1 | 0 | 0 | 40 | 38 | 0 | . 3 3 | 23 | 2 | 193090 | 13 | 4 | 0 | 3 | 4 | 0 | 0 | 0 | 30 | 38 | 0 | . 4 4 | 55 | 2 | 60193 | 9 | 0 | 0 | 1 | 4 | 0 | 0 | 0 | 40 | 38 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; corr_matrix = train_label_encoder.corr() corr_matrix[&quot;target&quot;].sort_values(ascending=False) . target 1.000000 education.num 0.341073 age 0.242283 hours.per.week 0.237383 capital.gain 0.221003 sex 0.216109 capital.loss 0.153660 race 0.079966 occupation 0.047896 native.country 0.030942 workclass 0.024025 id -0.003228 fnlwgt -0.005028 marital.status -0.194685 relationship -0.249029 Name: target, dtype: float64 . sns.heatmap(corr_matrix, cmap=&#39;Blues&#39;,annot=True) sns.set(rc = {&#39;figure.figsize&#39;:(20,20)}) . &#49688;&#52824;&#54805; &#49345;&#44288;&#44288;&#44228; &#48516;&#49437; . 변수가 너무 많으면 시각화를 해도 결과를 한눈에 보기에 어렵기 때문에 범주형 변수들은 따로 copy데이터를 만들어 label_encoder를 진행한 뒤 시각화하였고, 수치형 변수도 따로 상관관계 분석 및 시각화를 해주었다. . corr_matrix = train.corr() corr_matrix[&quot;target&quot;].sort_values(ascending=False) . target 1.000000 education.num 0.341073 age 0.242283 hours.per.week 0.237383 capital.gain 0.221003 capital.loss 0.153660 id -0.003228 fnlwgt -0.005028 Name: target, dtype: float64 . sns.heatmap(corr_matrix, cmap=&#39;Blues&#39;,annot=True) sns.set(rc = {&#39;figure.figsize&#39;:(10,10)}) . 상관관계 분석 결과 target과 관계성이 높은 변수는 education.num, hours.per.week, age 관계성이 낮은 변수는 fnlwgt, race, native.country임을 알 수 있다. . &#54596;&#50836;&#50630;&#45716; &#48320;&#49688; &#51228;&#44144; . plt.figure(figsize=(30,16),dpi=50) sns.set_style(&quot;whitegrid&quot;) # income 별 fnlwgt 분포 plt.subplot(311) fnlwgt_0 = train.loc[train[&#39;target&#39;] == 0, &#39;fnlwgt&#39;] fnlwgt_1 = train.loc[train[&#39;target&#39;] == 1, &#39;fnlwgt&#39;] sns.set(font_scale = 2) sb = sns.distplot(fnlwgt_0, hist = False, kde_kws={&quot;color&quot;: &quot;skyblue&quot;, &quot;lw&quot;: 4}) pn = sns.distplot(fnlwgt_1, hist = False, kde_kws={&quot;color&quot;: &quot;pink&quot;, &quot;lw&quot;: 4}) sb.set_xlabel(&quot;fnlwgt&quot;, fontsize = 20) pn.set_ylabel(&quot;distribution&quot;, fontsize = 20) plt.legend(labels=[&quot;&lt;= 50k&quot;,&quot;&gt;50k&quot;]) sns.set_style(&quot;whitegrid&quot;) # income 별 race 분포 plt.subplot(312) race_0 = train_label_encoder.loc[train[&#39;target&#39;] == 0, &#39;race&#39;] race_1 = train_label_encoder.loc[train[&#39;target&#39;] == 1, &#39;race&#39;] sns.set(font_scale = 2) sb = sns.distplot(race_0, hist = False, kde_kws={&quot;color&quot;: &quot;skyblue&quot;, &quot;lw&quot;: 4}) pn = sns.distplot(race_1, hist = False, kde_kws={&quot;color&quot;: &quot;pink&quot;, &quot;lw&quot;: 4}) sb.set_xlabel(&quot;race&quot;, fontsize = 20) pn.set_ylabel(&quot;distribution&quot;, fontsize = 20) sns.set_style(&quot;whitegrid&quot;) # income 별 native.country 분포 plt.subplot(313) native_country_0 = train_label_encoder.loc[train[&#39;target&#39;] == 0, &#39;native.country&#39;] native_country_1 = train_label_encoder.loc[train[&#39;target&#39;] == 1, &#39;native.country&#39;] sns.set(font_scale = 2) sb = sns.distplot(native_country_0, hist = False, kde_kws={&quot;color&quot;: &quot;skyblue&quot;, &quot;lw&quot;: 4}) pn = sns.distplot(native_country_1, hist = False, kde_kws={&quot;color&quot;: &quot;pink&quot;, &quot;lw&quot;: 4}) sb.set_xlabel(&quot;native.country&quot;, fontsize = 20) pn.set_ylabel(&quot;distribution&quot;, fontsize = 20) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) . Text(0, 0.5, &#39;distribution&#39;) . del train[&#39;fnlwgt&#39;] del train[&#39;race&#39;] del train[&#39;native.country&#39;] del test[&#39;fnlwgt&#39;] del test[&#39;race&#39;] del test[&#39;native.country&#39;] . train . id age workclass education.num marital.status occupation relationship sex capital.gain capital.loss hours.per.week target . 0 0 | 32 | Private | 12 | Married-civ-spouse | Craft-repair | Husband | Male | 0 | 0 | 40 | 0 | . 1 1 | 33 | Private | 10 | Married-civ-spouse | Exec-managerial | Husband | Male | 0 | 0 | 40 | 1 | . 2 2 | 46 | Private | 10 | Married-civ-spouse | Craft-repair | Husband | Male | 0 | 0 | 40 | 0 | . 3 3 | 23 | Private | 13 | Never-married | Adm-clerical | Own-child | Female | 0 | 0 | 30 | 0 | . 4 4 | 55 | Private | 9 | Divorced | Adm-clerical | Not-in-family | Female | 0 | 0 | 40 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 15076 15076 | 35 | Private | 14 | Never-married | Exec-managerial | Not-in-family | Male | 0 | 0 | 40 | 0 | . 15077 15077 | 36 | Private | 10 | Divorced | Adm-clerical | Not-in-family | Male | 0 | 0 | 45 | 0 | . 15078 15078 | 50 | Self-emp-inc | 15 | Married-civ-spouse | Prof-specialty | Husband | Male | 0 | 0 | 45 | 1 | . 15079 15079 | 39 | Private | 10 | Divorced | Tech-support | Not-in-family | Female | 0 | 0 | 40 | 0 | . 15080 15080 | 33 | Private | 12 | Married-civ-spouse | Prof-specialty | Husband | Male | 0 | 0 | 50 | 0 | . 15081 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; &#49688;&#52824;&#54805; &#48320;&#49688; age, education.num, capital.gain, capital.loss, hours.per.week, sex . target_0 = train.query(&#39;target==0&#39;).copy() target_0 . id age workclass education.num marital.status occupation relationship sex capital.gain capital.loss hours.per.week target . 0 0 | 32 | Private | 12 | Married-civ-spouse | Craft-repair | Husband | Male | 0 | 0 | 40 | 0 | . 2 2 | 46 | Private | 10 | Married-civ-spouse | Craft-repair | Husband | Male | 0 | 0 | 40 | 0 | . 3 3 | 23 | Private | 13 | Never-married | Adm-clerical | Own-child | Female | 0 | 0 | 30 | 0 | . 4 4 | 55 | Private | 9 | Divorced | Adm-clerical | Not-in-family | Female | 0 | 0 | 40 | 0 | . 5 5 | 33 | Private | 13 | Married-civ-spouse | Prof-specialty | Husband | Male | 0 | 0 | 40 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 15075 15075 | 18 | Private | 9 | Never-married | Machine-op-inspct | Not-in-family | Male | 0 | 0 | 37 | 0 | . 15076 15076 | 35 | Private | 14 | Never-married | Exec-managerial | Not-in-family | Male | 0 | 0 | 40 | 0 | . 15077 15077 | 36 | Private | 10 | Divorced | Adm-clerical | Not-in-family | Male | 0 | 0 | 45 | 0 | . 15079 15079 | 39 | Private | 10 | Divorced | Tech-support | Not-in-family | Female | 0 | 0 | 40 | 0 | . 15080 15080 | 33 | Private | 12 | Married-civ-spouse | Prof-specialty | Husband | Male | 0 | 0 | 50 | 0 | . 11308 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; target_1 = train.query(&#39;target==1&#39;).copy() target_1 . id age workclass education.num marital.status occupation relationship sex capital.gain capital.loss hours.per.week target . 1 1 | 33 | Private | 10 | Married-civ-spouse | Exec-managerial | Husband | Male | 0 | 0 | 40 | 1 | . 6 6 | 40 | State-gov | 15 | Married-civ-spouse | Prof-specialty | Husband | Male | 0 | 0 | 38 | 1 | . 13 13 | 51 | Private | 7 | Married-civ-spouse | Craft-repair | Husband | Male | 0 | 0 | 40 | 1 | . 18 18 | 27 | Private | 10 | Married-civ-spouse | Prof-specialty | Husband | Male | 0 | 0 | 40 | 1 | . 21 21 | 58 | Private | 10 | Married-civ-spouse | Sales | Husband | Male | 0 | 0 | 45 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 15069 15069 | 42 | Federal-gov | 11 | Divorced | Adm-clerical | Unmarried | Female | 0 | 0 | 40 | 1 | . 15070 15070 | 41 | Private | 10 | Married-civ-spouse | Craft-repair | Husband | Male | 7298 | 0 | 50 | 1 | . 15072 15072 | 61 | Private | 13 | Divorced | Adm-clerical | Not-in-family | Male | 0 | 0 | 40 | 1 | . 15073 15073 | 57 | Self-emp-not-inc | 10 | Married-civ-spouse | Transport-moving | Husband | Male | 0 | 0 | 45 | 1 | . 15078 15078 | 50 | Self-emp-inc | 15 | Married-civ-spouse | Prof-specialty | Husband | Male | 0 | 0 | 45 | 1 | . 3773 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; age . age min = 17 age max = 90 . age_0 = target_0[&#39;age&#39;] age_1 = target_1[&#39;age&#39;] sns.set(font_scale = 2) plt.figure(figsize=(12,8)) sns.set_style(&#39;whitegrid&#39;) ax_0 = sns.distplot(age_0, hist = False, kde_kws={&quot;color&quot;: &quot;skyblue&quot;, &quot;lw&quot;: 3}) ax_1 = sns.distplot(age_1, hist = False, kde_kws={&quot;color&quot;: &quot;pink&quot;, &quot;lw&quot;: 3}) ax_0.set_xlabel(&quot;AGE&quot;, fontsize = 20) ax_0.set_ylabel(&quot;DIST&quot;, fontsize = 20) plt.legend(labels=[&quot;&lt;= 50k&quot;,&quot;&gt; 50k&quot;]) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) . &lt;matplotlib.legend.Legend at 0x7f2b5a1e9610&gt; . # target_0데이터 age변수의 count를 시각화해준다. sns.set_style(&quot;whitegrid&quot;) # 그래프의 배경을 whitegrid로 지정해준다. ax = sns.countplot(x=target_0[&#39;age&#39;], data=target_0) ax.set(xlabel=&quot;AGE&quot;, ylabel = &quot;COUNT&quot;) # xlabel = AGE, ylabel = COUNT로 지정해준다. plt.xticks(rotation = - 90 ) sns.set(rc = {&#39;figure.figsize&#39;:(15,15)}) plt.show() . sns.set_style(&quot;whitegrid&quot;) # 그래프의 배경을 whitegrid로 지정해준다. ax = sns.countplot(x=target_1[&#39;age&#39;], data=target_1) ax.set(xlabel=&quot;AGE&quot;, ylabel = &quot;COUNT&quot;) # xlabel = AGE, ylabel = COUNT로 지정해준다. plt.xticks(rotation = - 90 ) sns.set(rc = {&#39;figure.figsize&#39;:(15,15)}) plt.show() . 분포가 너무 많아 시각화로 보기 힘들 것 같아 적당한 구간을 나누어 ppt 차트에 값을 입력해주었다. . education.num . education_0 = target_0[&#39;education.num&#39;] education_1 = target_1[&#39;education.num&#39;] sns.set(font_scale = 2) plt.figure(figsize=(12,8)) sns.set_style(&#39;whitegrid&#39;) ax_0 = sns.distplot(education_0, hist = False, kde_kws={&quot;color&quot;: &quot;indigo&quot;, &quot;lw&quot;: 3}) ax_1 = sns.distplot(education_1, hist = False, kde_kws={&quot;color&quot;: &quot;cornflowerblue&quot;, &quot;lw&quot;: 3}) ax_0.set_xlabel(&quot;EDUCATION&quot;, fontsize = 20) ax_0.set_ylabel(&quot;DIST&quot;, fontsize = 20) plt.legend(labels=[&quot;&lt;= 50k&quot;,&quot;&gt; 50k&quot;]) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) . &lt;matplotlib.legend.Legend at 0x7f2b59ca1b90&gt; . sns.set_style(&quot;whitegrid&quot;) # 그래프의 배경을 whitegrid로 지정해준다. ax = sns.countplot(x=target_0[&#39;education.num&#39;], data=target_0) ax.set(xlabel=&quot;EDUCATION&quot;, ylabel = &quot;COUNT&quot;) # xlabe = EDUCATION, ylabel = COUNT로 지정해준다. plt.xticks(rotation = - 90 ) sns.set(rc = {&#39;figure.figsize&#39;:(10,10)}) plt.show() . sns.set_style(&quot;whitegrid&quot;) ax = sns.countplot(x=target_1[&#39;education.num&#39;], data=target_1) ax.set(xlabel=&quot;EDUCATION&quot;, ylabel = &quot;COUNT&quot;) plt.xticks(rotation = - 90 ) sns.set(rc = {&#39;figure.figsize&#39;:(10,10)}) plt.show() . capital.gain . train[&#39;capital.gain&#39;].value_counts() . 0 13812 15024 176 7688 136 7298 116 99999 76 ... 4687 1 34095 1 1409 1 401 1 1797 1 Name: capital.gain, Length: 107, dtype: int64 . train[&#39;capital.gain&#39;] . 0 0 1 0 2 0 3 0 4 0 .. 15076 0 15077 0 15078 0 15079 0 15080 0 Name: capital.gain, Length: 15081, dtype: int64 . fig, axes = plt.subplots(1, 2) fig.set_size_inches(20, 8) gain1 = train.loc[(train[&#39;target&#39;] == 0) &amp; (train[&#39;capital.gain&#39;] &gt; 0), &#39;capital.gain&#39;].value_counts().sort_index() gain1.plot(kind=&#39;bar&#39;, ax=axes[0]) plt.xticks(fontsize =10) gain1 = train.loc[(train[&#39;target&#39;] == 1) &amp; (train[&#39;capital.gain&#39;] &gt; 0), &#39;capital.gain&#39;].value_counts().sort_index() gain1.plot(kind=&#39;bar&#39;, ax=axes[1]) plt.tight_layout() plt.show() . fig, axes = plt.subplots(1, 2) fig.set_size_inches(20, 8) loss1 = train.loc[(train[&#39;target&#39;] == 0) &amp; (train[&#39;capital.loss&#39;] &gt; 0), &#39;capital.loss&#39;].value_counts().sort_index() loss1.plot(kind=&#39;bar&#39;, ax=axes[0]) plt.xticks(fontsize =10) loss1 = train.loc[(train[&#39;target&#39;] == 1) &amp; (train[&#39;capital.loss&#39;] &gt; 0), &#39;capital.loss&#39;].value_counts().sort_index() loss1.plot(kind=&#39;bar&#39;, ax=axes[1]) plt.tight_layout() plt.show() . plt.figure(figsize=(12,8)) sns.histplot(x=&#39;capital.gain&#39;,hue=&#39;target&#39;,data=train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2b5a349ed0&gt; . capital.loss . plt.figure(figsize=(12,8)) sns.histplot(x=&#39;capital.loss&#39;,hue=&#39;target&#39;,data=train) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2b5a835050&gt; . capital.gain 변수와 capital.loss 변수를 다양하게 시각화 해보았으나 값들이 무엇을 의미하는지 알 수 없어서 아쉬웠다. . hours.per.week . hours_per_week_0 = target_0[&#39;hours.per.week&#39;] hours_per_week_1 = target_1[&#39;hours.per.week&#39;] sns.set(font_scale = 2) plt.figure(figsize=(12,8)) sns.set_style(&#39;ticks&#39;) ax_0 = sns.distplot(hours_per_week_0, hist = False, kde_kws={&quot;color&quot;: &quot;red&quot;, &quot;lw&quot;: 3}) ax_1 = sns.distplot(hours_per_week_1, hist = False, kde_kws={&quot;color&quot;: &quot;blue&quot;, &quot;lw&quot;: 3}) ax_0.set_xlabel(&quot;HOURS.PER.WEEK&quot;, fontsize = 20) ax_0.set_ylabel(&quot;DIST&quot;, fontsize = 20) plt.legend(labels=[&quot;&lt;= 50k&quot;,&quot;&gt; 50k&quot;]) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) . &lt;matplotlib.legend.Legend at 0x7f2b5abef690&gt; . target_0[&#39;hours.per.week&#39;].value_counts() . 40 5634 50 721 35 540 45 527 20 495 ... 82 1 92 1 94 1 88 1 74 1 Name: hours.per.week, Length: 86, dtype: int64 . def hours_per_week(a): if a &lt;= 10: score = &#39;10시간 이하&#39; elif a &lt;= 20: score = &#39;20시간 이하&#39; elif a &lt;= 30: score = &#39;30시간 이하&#39; elif a &lt;= 40: score = &#39;40시간 이하&#39; elif a &lt;= 50: score = &#39;50시간 이하&#39; elif a &lt;= 60: score = &#39;60시간 이하&#39; elif a &lt;= 70: score = &#39;70시간 이하&#39; elif a &lt;= 80: score = &#39;80시간 이하&#39; else: score = &#39;81시간 이상&#39; return score . target_0[&quot;주당근무시간&quot;] = target_0[&quot;hours.per.week&quot;].apply(lambda a: hours_per_week(a)) target_1[&quot;주당근무시간&quot;] = target_1[&quot;hours.per.week&quot;].apply(lambda a: hours_per_week(a)) . target_0 . id age workclass education.num marital.status occupation relationship sex capital.gain capital.loss hours.per.week target 주당근무시간 . 0 0 | 32 | Private | 12 | Married-civ-spouse | Craft-repair | Husband | Male | 0 | 0 | 40 | 0 | 40시간 이하 | . 2 2 | 46 | Private | 10 | Married-civ-spouse | Craft-repair | Husband | Male | 0 | 0 | 40 | 0 | 40시간 이하 | . 3 3 | 23 | Private | 13 | Never-married | Adm-clerical | Own-child | Female | 0 | 0 | 30 | 0 | 30시간 이하 | . 4 4 | 55 | Private | 9 | Divorced | Adm-clerical | Not-in-family | Female | 0 | 0 | 40 | 0 | 40시간 이하 | . 5 5 | 33 | Private | 13 | Married-civ-spouse | Prof-specialty | Husband | Male | 0 | 0 | 40 | 0 | 40시간 이하 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 15075 15075 | 18 | Private | 9 | Never-married | Machine-op-inspct | Not-in-family | Male | 0 | 0 | 37 | 0 | 40시간 이하 | . 15076 15076 | 35 | Private | 14 | Never-married | Exec-managerial | Not-in-family | Male | 0 | 0 | 40 | 0 | 40시간 이하 | . 15077 15077 | 36 | Private | 10 | Divorced | Adm-clerical | Not-in-family | Male | 0 | 0 | 45 | 0 | 50시간 이하 | . 15079 15079 | 39 | Private | 10 | Divorced | Tech-support | Not-in-family | Female | 0 | 0 | 40 | 0 | 40시간 이하 | . 15080 15080 | 33 | Private | 12 | Married-civ-spouse | Prof-specialty | Husband | Male | 0 | 0 | 50 | 0 | 50시간 이하 | . 11308 rows × 13 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; sns.set_style(&quot;whitegrid&quot;) ax = sns.countplot(x=target_0[&#39;주당근무시간&#39;], data=target_0) ax.set(xlabel=&quot;HOURS_PER_WEEK&quot;, ylabel = &quot;COUNT&quot;) plt.xticks(rotation = - 90 ) sns.set(rc = {&#39;figure.figsize&#39;:(10,10)}) # plt.rcParams[&#39;font.family&#39;] = &#39;Malgun Gothic&#39; plt.rc(&#39;font&#39;, family=&#39;Malgun Gothic&#39;) plt.show() . /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49345 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font. font.set_text(s, 0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font. font.set_text(s, 0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font. font.set_text(s, 0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font. font.set_text(s, 0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49345 missing from current font. font.set_text(s, 0, flags=flags) . sns.set_style(&quot;whitegrid&quot;) ax = sns.countplot(x=target_1[&#39;주당근무시간&#39;], data=target_1) ax.set(xlabel=&quot;HOURS_PER_WEEK&quot;, ylabel = &quot;COUNT&quot;) plt.xticks(rotation = - 90 ) sns.set(rc = {&#39;figure.figsize&#39;:(10,10)}) plt.rcParams[&#39;font.family&#39;] = &#39;Malgun Gothic&#39; plt.show() . /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51060 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54616 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49345 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font. font.set_text(s, 0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font. font.set_text(s, 0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51060 missing from current font. font.set_text(s, 0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54616 missing from current font. font.set_text(s, 0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49345 missing from current font. font.set_text(s, 0, flags=flags) . ppt 시각화를 위해 분포하는 정확한 값들을 뽑아 ppt 차트에 적용하여주었다. . target_0[&#39;주당근무시간&#39;].value_counts() . 40시간 이하 6627 50시간 이하 1659 30시간 이하 917 20시간 이하 876 60시간 이하 654 10시간 이하 242 70시간 이하 189 80시간 이하 75 81시간 이상 69 Name: 주당근무시간, dtype: int64 . target_1[&#39;주당근무시간&#39;].value_counts() . 40시간 이하 1716 50시간 이하 1149 60시간 이하 551 70시간 이하 120 30시간 이하 73 20시간 이하 61 80시간 이하 53 81시간 이상 34 10시간 이하 16 Name: 주당근무시간, dtype: int64 . sex . train[&#39;sex&#39;] = le.fit_transform(train[&#39;sex&#39;]) test[&#39;sex&#39;] = le.fit_transform(test[&#39;sex&#39;]) target_0[&#39;sex&#39;] = le.fit_transform(target_0[&#39;sex&#39;]) target_1[&#39;sex&#39;] = le.fit_transform(target_1[&#39;sex&#39;]) . sex_0 = target_0[&#39;sex&#39;] sex_1 = target_1[&#39;sex&#39;] sns.set(font_scale = 2) plt.figure(figsize=(12,8)) sns.set_style(&#39;ticks&#39;) ax_0 = sns.distplot(sex_0, hist = False, kde_kws={&quot;color&quot;: &quot;red&quot;, &quot;lw&quot;: 3}) ax_1 = sns.distplot(sex_1, hist = False, kde_kws={&quot;color&quot;: &quot;blue&quot;, &quot;lw&quot;: 3}) ax_0.set_xlabel(&quot;SEX&quot;, fontsize = 20) ax_0.set_ylabel(&quot;DIST&quot;, fontsize = 20) plt.legend(labels=[&quot;&lt;= 50k&quot;,&quot;&gt; 50k&quot;]) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots). warnings.warn(msg, FutureWarning) . &lt;matplotlib.legend.Legend at 0x7f2b59f415d0&gt; . sns.catplot(x=&quot;sex&quot;, col=&quot;target&quot;, data=train, kind=&quot;count&quot;,palette=&quot;husl&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f2b59f4b990&gt; . target_0[&#39;sex&#39;].value_counts() . 1 6972 0 4336 Name: sex, dtype: int64 . target_1[&#39;sex&#39;].value_counts() . 1 3208 0 565 Name: sex, dtype: int64 . &#48276;&#51452;&#54805; &#48320;&#49688; workclass, marital.status, occupation, relationship, sex . &#48320;&#49688; &#44256;&#50976;&#44050; &#54869;&#51064; . sex 변수처럼 수치화하여 EDA 해보아도 괜찮을 것 같은 변수를 찾아보기 위해 확인해보았다. . train[&#39;workclass&#39;].value_counts() . Private 11158 Self-emp-not-inc 1230 Local-gov 1027 State-gov 640 Self-emp-inc 552 Federal-gov 468 Without-pay 6 Name: workclass, dtype: int64 . train[&#39;marital.status&#39;].value_counts() . Married-civ-spouse 7092 Never-married 4814 Divorced 2086 Separated 454 Widowed 424 Married-spouse-absent 199 Married-AF-spouse 12 Name: marital.status, dtype: int64 . train[&#39;occupation&#39;].value_counts() . Exec-managerial 2039 Craft-repair 2032 Prof-specialty 1983 Adm-clerical 1844 Sales 1763 Other-service 1594 Machine-op-inspct 1004 Transport-moving 760 Handlers-cleaners 675 Farming-fishing 503 Tech-support 459 Protective-serv 345 Priv-house-serv 77 Armed-Forces 3 Name: occupation, dtype: int64 . train[&#39;relationship&#39;].value_counts() . Husband 6242 Not-in-family 3836 Own-child 2232 Unmarried 1588 Wife 742 Other-relative 441 Name: relationship, dtype: int64 . train[&#39;sex&#39;].value_counts() . 1 10180 0 4901 Name: sex, dtype: int64 . 변수의 분포를 살펴보았지만 sex변수를 제외하고는 범주형으로 시각화를 하는 것이 더 좋겠다고 판단하였다. . workclass . ppt 시각화를 위한 코드이다. 더 좋은 시각화를 위해 ppt내의 파이차트를 사용하였다. . train.groupby(&#39;workclass&#39;)[&#39;target&#39;].value_counts() . workclass target Federal-gov 0 291 1 177 Local-gov 0 718 1 309 Private 0 8713 1 2445 Self-emp-inc 1 301 0 251 Self-emp-not-inc 0 867 1 363 State-gov 0 462 1 178 Without-pay 0 6 Name: target, dtype: int64 . sns.set_style(&quot;whitegrid&quot;) sns.countplot(x=train[&#39;workclass&#39;], hue = train[&#39;target&#39;],palette = &quot;muted&quot;) plt.xticks(rotation = - 90 ) sns.set(rc = {&#39;figure.figsize&#39;:(7,7)}) . marital.status . train.groupby(&#39;marital.status&#39;)[&#39;target&#39;].value_counts() . marital.status target Divorced 0 1850 1 236 Married-AF-spouse 0 7 1 5 Married-civ-spouse 0 3885 1 3207 Married-spouse-absent 0 183 1 16 Never-married 0 4582 1 232 Separated 0 420 1 34 Widowed 0 381 1 43 Name: target, dtype: int64 . sns.set_style(&quot;whitegrid&quot;) sns.countplot(x=train[&#39;marital.status&#39;], hue = train[&#39;target&#39;],palette = &quot;muted&quot;) plt.xticks(rotation = - 90 ) sns.set(rc = {&#39;figure.figsize&#39;:(7,7)}) . occupation . train.groupby(&#39;occupation&#39;)[&#39;target&#39;].value_counts() . occupation target Adm-clerical 0 1591 1 253 Armed-Forces 0 3 Craft-repair 0 1576 1 456 Exec-managerial 0 1047 1 992 Farming-fishing 0 438 1 65 Handlers-cleaners 0 639 1 36 Machine-op-inspct 0 876 1 128 Other-service 0 1527 1 67 Priv-house-serv 0 77 Prof-specialty 0 1088 1 895 Protective-serv 0 233 1 112 Sales 0 1280 1 483 Tech-support 0 322 1 137 Transport-moving 0 611 1 149 Name: target, dtype: int64 . sns.set_style(&quot;whitegrid&quot;) sns.countplot(x=train[&#39;occupation&#39;], hue = train[&#39;target&#39;],palette = &quot;Paired&quot;) plt.xticks(rotation = - 90 ) sns.set(rc = {&#39;figure.figsize&#39;:(10,10)}) . relationship . train.groupby(&#39;relationship&#39;)[&#39;target&#39;].value_counts() . relationship target Husband 0 3409 1 2833 Not-in-family 0 3407 1 429 Other-relative 0 422 1 19 Own-child 0 2197 1 35 Unmarried 0 1486 1 102 Wife 0 387 1 355 Name: target, dtype: int64 . sns.set_style(&quot;whitegrid&quot;) sns.countplot(x=train[&#39;relationship&#39;], hue = train[&#39;target&#39;],palette = &quot;muted&quot;) plt.xticks(rotation = - 90 ) sns.set(rc = {&#39;figure.figsize&#39;:(7,7)}) . &#47784;&#45944;&#47553; . &#50896;&#54635;&#51064;&#53076;&#46377; . 범주형 데이터를 one-hot encoding을 사용하여 범주의 개수만큼 dummy variable을 만들어 0 또는 1을 각 범주마다 할당하여 새로운 특성으로 바꾸는 방법이다. . train_one = pd.get_dummies(train) train_one.head() . id age education.num sex capital.gain capital.loss hours.per.week target workclass_Federal-gov workclass_Local-gov ... occupation_Protective-serv occupation_Sales occupation_Tech-support occupation_Transport-moving relationship_Husband relationship_Not-in-family relationship_Other-relative relationship_Own-child relationship_Unmarried relationship_Wife . 0 0 | 32 | 12 | 1 | 0 | 0 | 40 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 1 1 | 33 | 10 | 1 | 0 | 0 | 40 | 1 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 2 2 | 46 | 10 | 1 | 0 | 0 | 40 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 3 3 | 23 | 13 | 0 | 0 | 0 | 30 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 4 4 | 55 | 9 | 0 | 0 | 0 | 40 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 5 rows × 42 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train . id age workclass education.num marital.status occupation relationship sex capital.gain capital.loss hours.per.week target . 0 0 | 32 | Private | 12 | Married-civ-spouse | Craft-repair | Husband | 1 | 0 | 0 | 40 | 0 | . 1 1 | 33 | Private | 10 | Married-civ-spouse | Exec-managerial | Husband | 1 | 0 | 0 | 40 | 1 | . 2 2 | 46 | Private | 10 | Married-civ-spouse | Craft-repair | Husband | 1 | 0 | 0 | 40 | 0 | . 3 3 | 23 | Private | 13 | Never-married | Adm-clerical | Own-child | 0 | 0 | 0 | 30 | 0 | . 4 4 | 55 | Private | 9 | Divorced | Adm-clerical | Not-in-family | 0 | 0 | 0 | 40 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 15076 15076 | 35 | Private | 14 | Never-married | Exec-managerial | Not-in-family | 1 | 0 | 0 | 40 | 0 | . 15077 15077 | 36 | Private | 10 | Divorced | Adm-clerical | Not-in-family | 1 | 0 | 0 | 45 | 0 | . 15078 15078 | 50 | Self-emp-inc | 15 | Married-civ-spouse | Prof-specialty | Husband | 1 | 0 | 0 | 45 | 1 | . 15079 15079 | 39 | Private | 10 | Divorced | Tech-support | Not-in-family | 0 | 0 | 0 | 40 | 0 | . 15080 15080 | 33 | Private | 12 | Married-civ-spouse | Prof-specialty | Husband | 1 | 0 | 0 | 50 | 0 | . 15081 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; test_one = pd.get_dummies(test) test_one.head() . id age education.num sex capital.gain capital.loss hours.per.week workclass_Federal-gov workclass_Local-gov workclass_Private ... occupation_Protective-serv occupation_Sales occupation_Tech-support occupation_Transport-moving relationship_Husband relationship_Not-in-family relationship_Other-relative relationship_Own-child relationship_Unmarried relationship_Wife . 0 0 | 47 | 10 | 1 | 0 | 0 | 45 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 1 1 | 34 | 10 | 1 | 0 | 0 | 75 | 0 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 2 2 | 31 | 13 | 1 | 8614 | 0 | 40 | 0 | 1 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 3 3 | 28 | 13 | 1 | 0 | 0 | 55 | 0 | 0 | 1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | . 4 4 | 54 | 10 | 0 | 0 | 0 | 40 | 0 | 1 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 5 rows × 41 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; test . id age workclass education.num marital.status occupation relationship sex capital.gain capital.loss hours.per.week . 0 0 | 47 | Private | 10 | Married-civ-spouse | Exec-managerial | Husband | 1 | 0 | 0 | 45 | . 1 1 | 34 | Self-emp-inc | 10 | Never-married | Sales | Not-in-family | 1 | 0 | 0 | 75 | . 2 2 | 31 | Local-gov | 13 | Never-married | Craft-repair | Not-in-family | 1 | 8614 | 0 | 40 | . 3 3 | 28 | Private | 13 | Married-civ-spouse | Prof-specialty | Husband | 1 | 0 | 0 | 55 | . 4 4 | 54 | Local-gov | 10 | Widowed | Adm-clerical | Unmarried | 0 | 0 | 0 | 40 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 15076 15076 | 30 | Local-gov | 10 | Married-civ-spouse | Protective-serv | Husband | 1 | 15024 | 0 | 40 | . 15077 15077 | 39 | Private | 13 | Married-civ-spouse | Craft-repair | Husband | 1 | 0 | 0 | 40 | . 15078 15078 | 48 | Private | 7 | Never-married | Machine-op-inspct | Unmarried | 1 | 0 | 0 | 40 | . 15079 15079 | 44 | Private | 9 | Married-civ-spouse | Machine-op-inspct | Husband | 1 | 0 | 0 | 40 | . 15080 15080 | 30 | Private | 9 | Never-married | Craft-repair | Not-in-family | 1 | 0 | 0 | 46 | . 15081 rows × 11 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; pycaret . from pycaret.classification import * reg = setup(train_one, target=&#39;target&#39;) . Description Value . 0 session_id | 4081 | . 1 Target | target | . 2 Target Type | Binary | . 3 Label Encoded | None | . 4 Original Data | (15081, 42) | . 5 Missing Values | False | . 6 Numeric Features | 39 | . 7 Categorical Features | 2 | . 8 Ordinal Features | False | . 9 High Cardinality Features | False | . 10 High Cardinality Method | None | . 11 Transformed Train Set | (10556, 55) | . 12 Transformed Test Set | (4525, 55) | . 13 Shuffle Train-Test | True | . 14 Stratify Train-Test | False | . 15 Fold Generator | StratifiedKFold | . 16 Fold Number | 10 | . 17 CPU Jobs | -1 | . 18 Use GPU | False | . 19 Log Experiment | False | . 20 Experiment Name | clf-default-name | . 21 USI | a9a8 | . 22 Imputation Type | simple | . 23 Iterative Imputation Iteration | None | . 24 Numeric Imputer | mean | . 25 Iterative Imputation Numeric Model | None | . 26 Categorical Imputer | constant | . 27 Iterative Imputation Categorical Model | None | . 28 Unknown Categoricals Handling | least_frequent | . 29 Normalize | False | . 30 Normalize Method | None | . 31 Transformation | False | . 32 Transformation Method | None | . 33 PCA | False | . 34 PCA Method | None | . 35 PCA Components | None | . 36 Ignore Low Variance | False | . 37 Combine Rare Levels | False | . 38 Rare Level Threshold | None | . 39 Numeric Binning | False | . 40 Remove Outliers | False | . 41 Outliers Threshold | None | . 42 Remove Multicollinearity | False | . 43 Multicollinearity Threshold | None | . 44 Remove Perfect Collinearity | True | . 45 Clustering | False | . 46 Clustering Iteration | None | . 47 Polynomial Features | False | . 48 Polynomial Degree | None | . 49 Trignometry Features | False | . 50 Polynomial Threshold | None | . 51 Group Features | False | . 52 Feature Selection | False | . 53 Feature Selection Method | classic | . 54 Features Selection Threshold | None | . 55 Feature Interaction | False | . 56 Feature Ratio | False | . 57 Interaction Threshold | None | . 58 Fix Imbalance | False | . 59 Fix Imbalance Method | SMOTE | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; top3 = compare_models(sort = &#39;Accuracy&#39;,n_select = 3) # top3 = lightgbm, gbc, ada . Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) . lightgbm Light Gradient Boosting Machine | 0.8644 | 0.9218 | 0.6503 | 0.7659 | 0.7026 | 0.6157 | 0.6196 | 0.166 | . gbc Gradient Boosting Classifier | 0.8584 | 0.9149 | 0.5842 | 0.7873 | 0.6699 | 0.5824 | 0.5934 | 1.095 | . ada Ada Boost Classifier | 0.8534 | 0.9082 | 0.6118 | 0.7483 | 0.6727 | 0.5795 | 0.5847 | 0.325 | . lr Logistic Regression | 0.8435 | 0.9009 | 0.5903 | 0.7247 | 0.6502 | 0.5508 | 0.5559 | 1.426 | . rf Random Forest Classifier | 0.8396 | 0.8861 | 0.6153 | 0.6989 | 0.6541 | 0.5503 | 0.5524 | 1.002 | . lda Linear Discriminant Analysis | 0.8354 | 0.8910 | 0.5576 | 0.7122 | 0.6251 | 0.5217 | 0.5283 | 0.100 | . ridge Ridge Classifier | 0.8326 | 0.0000 | 0.5042 | 0.7339 | 0.5970 | 0.4961 | 0.5104 | 0.022 | . knn K Neighbors Classifier | 0.8309 | 0.8588 | 0.6184 | 0.6707 | 0.6431 | 0.5326 | 0.5336 | 0.513 | . et Extra Trees Classifier | 0.8186 | 0.8432 | 0.5841 | 0.6465 | 0.6134 | 0.4954 | 0.4967 | 1.088 | . dt Decision Tree Classifier | 0.8109 | 0.7537 | 0.5999 | 0.6209 | 0.6100 | 0.4853 | 0.4856 | 0.054 | . nb Naive Bayes | 0.8058 | 0.8889 | 0.7894 | 0.5780 | 0.6671 | 0.5347 | 0.5478 | 0.024 | . svm SVM - Linear Kernel | 0.7873 | 0.0000 | 0.5359 | 0.5944 | 0.5346 | 0.4051 | 0.4228 | 0.051 | . dummy Dummy Classifier | 0.7535 | 0.5000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.017 | . qda Quadratic Discriminant Analysis | 0.2540 | 0.5050 | 1.0000 | 0.2484 | 0.3979 | 0.0049 | 0.0487 | 0.051 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; tuned_top3 = [tune_model(i) for i in top3] . Accuracy AUC Recall Prec. F1 Kappa MCC . Fold . 0 0.8466 | 0.9005 | 0.6231 | 0.7168 | 0.6667 | 0.5677 | 0.5700 | . 1 0.8580 | 0.9193 | 0.6385 | 0.7477 | 0.6888 | 0.5975 | 0.6006 | . 2 0.8494 | 0.9004 | 0.5885 | 0.7463 | 0.6581 | 0.5633 | 0.5698 | . 3 0.8665 | 0.9235 | 0.6192 | 0.7931 | 0.6955 | 0.6116 | 0.6193 | . 4 0.8608 | 0.9204 | 0.6015 | 0.7850 | 0.6811 | 0.5941 | 0.6027 | . 5 0.8608 | 0.9281 | 0.6130 | 0.7767 | 0.6852 | 0.5974 | 0.6043 | . 6 0.8597 | 0.9185 | 0.6154 | 0.7692 | 0.6838 | 0.5951 | 0.6012 | . 7 0.8654 | 0.9226 | 0.6308 | 0.7810 | 0.6979 | 0.6125 | 0.6183 | . 8 0.8692 | 0.9151 | 0.5962 | 0.8245 | 0.6920 | 0.6116 | 0.6246 | . 9 0.8635 | 0.9119 | 0.6269 | 0.7762 | 0.6936 | 0.6071 | 0.6128 | . Mean 0.8600 | 0.9160 | 0.6153 | 0.7717 | 0.6843 | 0.5958 | 0.6024 | . Std 0.0068 | 0.0088 | 0.0151 | 0.0279 | 0.0122 | 0.0166 | 0.0181 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; blender_top3 = blend_models(estimator_list=tuned_top3) . Accuracy AUC Recall Prec. F1 Kappa MCC . Fold . 0 0.8513 | 0.9015 | 0.6385 | 0.7249 | 0.6789 | 0.5827 | 0.5847 | . 1 0.8703 | 0.9218 | 0.6577 | 0.7808 | 0.7140 | 0.6309 | 0.6348 | . 2 0.8513 | 0.9025 | 0.5962 | 0.7488 | 0.6638 | 0.5699 | 0.5760 | . 3 0.8665 | 0.9260 | 0.6269 | 0.7874 | 0.6981 | 0.6138 | 0.6203 | . 4 0.8655 | 0.9244 | 0.6130 | 0.7960 | 0.6926 | 0.6084 | 0.6169 | . 5 0.8598 | 0.9304 | 0.6169 | 0.7703 | 0.6851 | 0.5964 | 0.6025 | . 6 0.8578 | 0.9201 | 0.6115 | 0.7644 | 0.6795 | 0.5896 | 0.5956 | . 7 0.8635 | 0.9242 | 0.6192 | 0.7816 | 0.6910 | 0.6049 | 0.6117 | . 8 0.8635 | 0.9182 | 0.5692 | 0.8222 | 0.6727 | 0.5901 | 0.6060 | . 9 0.8626 | 0.9197 | 0.6231 | 0.7751 | 0.6908 | 0.6038 | 0.6098 | . Mean 0.8612 | 0.9189 | 0.6172 | 0.7752 | 0.6867 | 0.5990 | 0.6058 | . Std 0.0059 | 0.0091 | 0.0224 | 0.0250 | 0.0133 | 0.0163 | 0.0163 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; final_model = finalize_model(blender_top3) prediction = predict_model(final_model, data=test_one) . prediction.rename(columns = {&quot;Label&quot;: &quot;target&quot;}, inplace = True) . submission . submission[&#39;target&#39;] = prediction[&#39;target&#39;] . submission.to_csv(&#39;소득예측.csv&#39;, index = False) . &#44208;&#47200; . 결론 요약(summary), 시사점, 활용가능, 한계, 향휴계획등등 . summary . 데이터 전처리 과정에서 : 총 4,262개의 결측치를 발견 및 제거해주었고, 교육수준 변수를 하나 제거해주었다. . EDA 과정에서 세 개의 변수들을 제거해주었고, TARGET값이 0인 그룹과 1인 그룹으로 비교하여 시각화를 진행하였다. 다양한 변수들이 TARGET값에 따라 많은 차이가 있다는 것을 시각화로 알 수 있었지만 capital.gain과 capital.loss 변수에 대해서는 잘 분석하지 못한 점이 아쉬웠다. . 모델링 과정에서 모델 선택을 위해 PYCARET이라는 파이썬 라이브러리를 사용하였고, 상위 3개의 모델을 뽑아 각각 모델링도 진행해보았으나, 상위 3개 모델로 튜닝, 블렌딩 한 모델이 가장 ACCURACY값이 높았다. . &#54876;&#50857;&#44032;&#45733; . 대회의 목적처럼 앞으로도 다양한 인구데이터를 기반으로 꽤 높은 정확도로 소득을 예측하는 것이 가능할 것 같다. . &#54620;&#44228; . EDA 과정에서 변수들에 대해 모든 분석 및 해석을 하지 못한 점이 아쉽다. . 모델링 과정에서는 basic 공모전인만큼 직접 모델 선정 및 모델링까지 진행해보고 싶었으나 파이썬 라이브러리인 pycaret을 이용하여 모델 선정과 모델링을 진행한 점이 아쉽다. . &#54693;&#54980;&#44228;&#54925; . 머신러닝에 대한 공부를 더 한 뒤에 다음 번 머신러닝 관련 공모전에서는 직접 모델 선정을 해보는 것이 목표이다. 또, 작년 머신러닝 수업 때 조별로 진행했던 프로젝트 퀄리티만큼 스스로 해 보는 것이 목표이다. .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/2022/07/23/%EC%9D%B8%EA%B5%AC%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B8%B0%EB%B0%98%EC%86%8C%EB%93%9D%EC%98%88%EC%B8%A1%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C_.html",
            "relUrl": "/2022/07/23/%EC%9D%B8%EA%B5%AC%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B8%B0%EB%B0%98%EC%86%8C%EB%93%9D%EC%98%88%EC%B8%A1%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C_.html",
            "date": " • Jul 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Geospatial_05_exercise",
            "content": "import math import geopandas as gpd import pandas as pd from shapely.geometry import MultiPolygon import folium from folium import Choropleth, Marker from folium.plugins import HeatMap, MarkerCluster from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex5 import * . embed_map() 함수를 사용하여 지도를 시각화할 수 있습니다. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . 1) &#52649;&#46028; &#45936;&#51060;&#53552;&#47484; &#49884;&#44033;&#54868;&#54633;&#45768;&#45796;. . 아래 코드 셀을 실행하여 2013-2018년 주요 자동차 충돌을 추적하는 GeoDataFrame &quot;충돌&quot;을 로드하십시오. . collisions = gpd.read_file(&quot;data/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp&quot;) collisions.head() . DATE TIME BOROUGH ZIP CODE LATITUDE LONGITUDE LOCATION ON STREET CROSS STRE OFF STREET ... CONTRIBU_2 CONTRIBU_3 CONTRIBU_4 UNIQUE KEY VEHICLE TY VEHICLE _1 VEHICLE _2 VEHICLE _3 VEHICLE _4 geometry . 0 07/30/2019 | 0:00 | BRONX | 10464 | 40.841100 | -73.784960 | (40.8411, -73.78496) | None | None | 121 PILOT STREET | ... | Unspecified | None | None | 4180045 | Sedan | Station Wagon/Sport Utility Vehicle | Station Wagon/Sport Utility Vehicle | None | None | POINT (1043750.211 245785.815) | . 1 07/30/2019 | 0:10 | QUEENS | 11423 | 40.710827 | -73.770660 | (40.710827, -73.77066) | JAMAICA AVENUE | 188 STREET | None | ... | None | None | None | 4180007 | Sedan | Sedan | None | None | None | POINT (1047831.185 198333.171) | . 2 07/30/2019 | 0:25 | None | None | 40.880318 | -73.841286 | (40.880318, -73.841286) | BOSTON ROAD | None | None | ... | None | None | None | 4179575 | Sedan | Station Wagon/Sport Utility Vehicle | None | None | None | POINT (1028139.293 260041.178) | . 3 07/30/2019 | 0:35 | MANHATTAN | 10036 | 40.756744 | -73.984590 | (40.756744, -73.98459) | None | None | 155 WEST 44 STREET | ... | None | None | None | 4179544 | Box Truck | Station Wagon/Sport Utility Vehicle | None | None | None | POINT (988519.261 214979.320) | . 4 07/30/2019 | 10:00 | BROOKLYN | 11223 | 40.600090 | -73.965910 | (40.60009, -73.96591) | AVENUE T | OCEAN PARKWAY | None | ... | None | None | None | 4180660 | Station Wagon/Sport Utility Vehicle | Bike | None | None | None | POINT (993716.669 157907.212) | . 5 rows × 30 columns . &quot;LATITUDE&quot; 및 &quot;LONGITUDE&quot; 열을 사용하여 충돌 데이터를 시각화하는 대화형 맵을 만듭니다. 어떤 종류의 지도가 가장 효과적이라고 생각하세요? . m_1 = folium.Map(location=[40.7, -74], zoom_start=11) # Your code here: Visualize the collision data HeatMap(data=collisions[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m_1) # Get credit for your work after you have created a map q_1.check() # Show the map m_1 . Thank you for creating a map! . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 2) &#48337;&#50896;&#51032; &#51201;&#50857; &#48276;&#50948;&#47484; &#51060;&#54644;&#54633;&#45768;&#45796;. . 다음 코드 셀을 실행하여 병원 데이터를 로드합니다. . hospitals = gpd.read_file(&quot;data/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp&quot;) hospitals.head() . id name address zip factype facname capacity capname bcode xcoord ycoord latitude longitude geometry . 0 317000001H1178 | BRONX-LEBANON HOSPITAL CENTER - CONCOURSE DIVI... | 1650 Grand Concourse | 10457 | 3102 | Hospital | 415 | Beds | 36005 | 1008872.0 | 246596.0 | 40.843490 | -73.911010 | POINT (1008872.000 246596.000) | . 1 317000001H1164 | BRONX-LEBANON HOSPITAL CENTER - FULTON DIVISION | 1276 Fulton Ave | 10456 | 3102 | Hospital | 164 | Beds | 36005 | 1011044.0 | 242204.0 | 40.831429 | -73.903178 | POINT (1011044.000 242204.000) | . 2 317000011H1175 | CALVARY HOSPITAL INC | 1740-70 Eastchester Rd | 10461 | 3102 | Hospital | 225 | Beds | 36005 | 1027505.0 | 248287.0 | 40.848060 | -73.843656 | POINT (1027505.000 248287.000) | . 3 317000002H1165 | JACOBI MEDICAL CENTER | 1400 Pelham Pkwy | 10461 | 3102 | Hospital | 457 | Beds | 36005 | 1027042.0 | 251065.0 | 40.855687 | -73.845311 | POINT (1027042.000 251065.000) | . 4 317000008H1172 | LINCOLN MEDICAL &amp; MENTAL HEALTH CENTER | 234 E 149 St | 10451 | 3102 | Hospital | 362 | Beds | 36005 | 1005154.0 | 236853.0 | 40.816758 | -73.924478 | POINT (1005154.000 236853.000) | . &quot;위도&quot; 및 &quot;경도&quot; 열을 사용하여 병원 위치를 시각화합니다. . m_2 = folium.Map(location=[40.7, -74], zoom_start=11) # Your code here: Visualize the hospital locations for idx, row in hospitals.iterrows(): Marker([row[&#39;latitude&#39;], row[&#39;longitude&#39;]], popup=row[&#39;name&#39;]).add_to(m_2) # Get credit for your work after you have created a map q_2.check() # Show the map m_2 . Thank you for creating a map! . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 3) &#44032;&#51109; &#44032;&#44620;&#50868; &#48337;&#50896;&#51008; &#50616;&#51228; 10&#53420;&#47196;&#48120;&#53552; &#51060;&#49345; &#46504;&#50612;&#51256; &#51080;&#50632;&#45208;&#50836;? . 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌과 &#39;충돌&#39;의 모든 행을 포함하는 DataFrame &#39;outside_range&#39;를 만듭니다. . 병원과 충돌은 모두 EPSG 2263을 좌표계로 하고 EPSG 2263은 미터 단위를 가진다. . coverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000) my_union = coverage.geometry.unary_union outside_range = collisions.loc[~collisions[&quot;geometry&quot;].apply(lambda x: my_union.contains(x))] # Check your answer q_3.check() . Correct . 다음 코드 셀은 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌 비율을 계산합니다. . percentage = round(100*len(outside_range)/len(collisions), 2) print(&quot;Percentage of collisions more than 10 km away from the closest hospital: {}%&quot;.format(percentage)) . Percentage of collisions more than 10 km away from the closest hospital: 15.12% . 4) &#52628;&#52380;&#51088;&#47484; &#47564;&#46308;&#50612;&#46972;. . 먼 곳에서 충돌이 일어날 때, 부상자들이 가장 가까운 병원으로 이송되는 것이 훨씬 더 중요해진다. . 이 점을 고려하여 다음과 같은 권장 사항을 작성하기로 결정했습니다. . 충돌 위치(EPSG 2263)를 입력으로 사용합니다. | (EPSG 2263에서 거리 계산이 수행되는) 가장 가까운 병원을 찾는다. | 가장 가까운 병원의 이름을 반환합니다. | . def best_hospital(collision_location): idx_min = hospitals.geometry.distance(collision_location).idxmin() my_hospital = hospitals.iloc[idx_min] name = my_hospital[&quot;name&quot;] return name # Test your function: this should suggest CALVARY HOSPITAL INC print(best_hospital(outside_range.geometry.iloc[0])) # Check your answer q_4.check() . CALVARY HOSPITAL INC . Correct . 5) &#49688;&#50836;&#44032; &#44032;&#51109; &#47566;&#51008; &#48337;&#50896;&#51008; &#50612;&#46356;&#51077;&#45768;&#44620;? . outside_range 데이터 프레임의 충돌만 고려한다면 어느 병원을 가장 추천합니까? . 답변은 4)에서 만든 함수에 의해 반환된 병원 이름과 정확히 일치하는 Python 문자열이어야 합니다. . highest_demand = outside_range.geometry.apply(best_hospital).value_counts().idxmax() # Check your answer q_5.check() . Correct . 6) &#49884;&#45716; &#50612;&#46356;&#50640; &#49352; &#48337;&#50896;&#51012; &#51648;&#50612;&#50556; &#54616;&#45716;&#44032;? . 가장 가까운 병원에서 10km 이상 떨어진 곳에서 발생한 충돌 외에도 다음 코드 셀(변경 사항 없이)을 실행하여 병원 위치를 시각화합니다. . m_6 = folium.Map(location=[40.7, -74], zoom_start=11) coverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000) folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6) HeatMap(data=outside_range[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m_6) folium.LatLngPopup().add_to(m_6) m_6 . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 지도의 아무 곳이나 클릭하면 해당 위치가 위도 및 경도로 표시된 팝업이 나타납니다. . 뉴욕시는 두 개의 새로운 병원을 위한 장소를 결정하는 데 도움을 요청했습니다. 이들은 특히 *3) 단계에서 계산된 백분율을 10% 미만으로 낮추기 위해 위치를 식별하는 데 도움을 필요로 합니다. 지도를 사용하여 (병원을 짓기 위해 구역법이나 어떤 잠재적인 건물을 제거해야 하는지에 대한 걱정 없이) 도시가 이 목표를 달성하는 데 도움이 되는 두 개의 위치를 식별할 수 있습니까? . 병원 1에 대해 제안된 위도와 경도를 각각 lat_1과 long_1에 넣는다. (병원 2에 대해서도 마찬가지로) . 그런 다음, 새 병원의 효과를 보기 위해 나머지 세포를 그대로 가동합니다. 두 개의 새 병원이 10% 미만으로 비율을 낮추면 정답으로 표시됩니다. . lat_1 = 40.6714 long_1 = -73.8492 # Your answer here: proposed location of hospital 2 lat_2 = 40.6702 long_2 = -73.7612 # Do not modify the code below this line try: new_df = pd.DataFrame( {&#39;Latitude&#39;: [lat_1, lat_2], &#39;Longitude&#39;: [long_1, long_2]}) new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude)) new_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} new_gdf = new_gdf.to_crs(epsg=2263) # get new percentage new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000) new_my_union = new_coverage.geometry.unary_union new_outside_range = outside_range.loc[~outside_range[&quot;geometry&quot;].apply(lambda x: new_my_union.contains(x))] new_percentage = round(100*len(new_outside_range)/len(collisions), 2) print(&quot;(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%&quot;.format(new_percentage)) # Did you help the city to meet its goal? q_6.check() # make the map m = folium.Map(location=[40.7, -74], zoom_start=11) folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m) folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m) for idx, row in new_gdf.iterrows(): Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]]).add_to(m) HeatMap(data=new_outside_range[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m) folium.LatLngPopup().add_to(m) display(m) except: q_6.hint() . c: Users User anaconda3 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . (NEW) Percentage of collisions more than 10 km away from the closest hospital: 9.12% . Correct . Make this Notebook Trusted to load map: File -&gt; Trust Notebook Congratulations! . You have just completed the Geospatial Analysis micro-course! Great job! .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/2022/07/23/_Geospatial05.html",
            "relUrl": "/2022/07/23/_Geospatial05.html",
            "date": " • Jul 23, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Geospatial_04_exercise",
            "content": "import math import pandas as pd import geopandas as gpd #from geopy.geocoders import Nominatim # What you&#39;d normally run from learntools.geospatial.tools import Nominatim # Just for this exercise import folium from folium import Marker from folium.plugins import MarkerCluster from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex4 import * . 이전 연습의 embed_map() 함수를 사용하여 지도를 시각화합니다. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . 1) &#45572;&#46973;&#46108; &#50948;&#52824;&#47484; &#51648;&#50724;&#53076;&#46377;&#54633;&#45768;&#45796;. . 다음 코드 셀을 실행하여 캘리포니아 주에 있는 스타벅스 매장을 포함하는 데이터 프레임 &quot;스타벅스&quot;를 만드십시오. . starbucks = pd.read_csv(&quot;data/starbucks_locations.csv&quot;) starbucks.head() . Store Number Store Name Address City Longitude Latitude . 0 10429-100710 | Palmdale &amp; Hwy 395 | 14136 US Hwy 395 Adelanto CA | Adelanto | -117.40 | 34.51 | . 1 635-352 | Kanan &amp; Thousand Oaks | 5827 Kanan Road Agoura CA | Agoura | -118.76 | 34.16 | . 2 74510-27669 | Vons-Agoura Hills #2001 | 5671 Kanan Rd. Agoura Hills CA | Agoura Hills | -118.76 | 34.15 | . 3 29839-255026 | Target Anaheim T-0677 | 8148 E SANTA ANA CANYON ROAD AHAHEIM CA | AHAHEIM | -117.75 | 33.87 | . 4 23463-230284 | Safeway - Alameda 3281 | 2600 5th Street Alameda CA | Alameda | -122.28 | 37.79 | . 대부분의 상점들은 위도, 경도 등의 위치를 알고 있다. 하지만, 버클리 시의 모든 장소가 사라졌습니다. . print(starbucks.isnull().sum()) # View rows with missing locations rows_with_missing = starbucks[starbucks[&quot;City&quot;]==&quot;Berkeley&quot;] rows_with_missing . Store Number 0 Store Name 0 Address 0 City 0 Longitude 5 Latitude 5 dtype: int64 . Store Number Store Name Address City Longitude Latitude . 153 5406-945 | 2224 Shattuck - Berkeley | 2224 Shattuck Avenue Berkeley CA | Berkeley | NaN | NaN | . 154 570-512 | Solano Ave | 1799 Solano Avenue Berkeley CA | Berkeley | NaN | NaN | . 155 17877-164526 | Safeway - Berkeley #691 | 1444 Shattuck Place Berkeley CA | Berkeley | NaN | NaN | . 156 19864-202264 | Telegraph &amp; Ashby | 3001 Telegraph Avenue Berkeley CA | Berkeley | NaN | NaN | . 157 9217-9253 | 2128 Oxford St. | 2128 Oxford Street Berkeley CA | Berkeley | NaN | NaN | . 아래 코드 셀을 사용하여 이 값을 Nominatim 지오코더로 채우십시오. . 튜토리얼에서 우리는 지오코드 값에 &quot;Nominatim()&quot;(&quot;geopy.geocoders&quot;의)을 사용했으며, 이것이 이 과정 이외의 프로젝트에서 사용할 수 있는 것이다. . 이 연습에서는 learntools.geospacial과는 약간 다른 함수인 &quot;Nominatim()&quot;을 사용합니다.툴&#39;). 이 기능은 노트북 상단에 가져온 것으로 GeoPandas의 기능과 동일하게 작동합니다. . 다시 말해서, 다음과 같은 경우: . 노트북 맨 위에 있는 가져오기 문을 변경하지 않습니다. | 당신은 아래의 코드 셀에서 지오코딩 함수를 &quot;geocode`&quot;라고 부른다. | . 코드가 의도한 대로 작동합니다! . geolocator = Nominatim(user_agent=&quot;kaggle_learn&quot;) # Your code here def my_geocoder(row): point = geolocator.geocode(row).point return pd.Series({&#39;Longitude&#39;: point.longitude, &#39;Latitude&#39;: point.latitude}) berkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x[&#39;Address&#39;]), axis=1) starbucks.update(berkeley_locations) # Check your answer q_1.check() . Correct . 2) &#48260;&#53364;&#47532;&#51032; &#50948;&#52824;&#47484; &#54869;&#51064;&#54633;&#45768;&#45796;. . 당신이 방금 찾은 장소들을 살펴봅시다. 버클리의 (위도, 경도) 위치를 OpenStreetMap 스타일로 시각화합니다. . m_2 = folium.Map(location=[37.88,-122.26], zoom_start=13) # Your code here: Add a marker for each Berkeley location for idx, row in starbucks[starbucks[&quot;City&quot;]==&#39;Berkeley&#39;].iterrows(): Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]]).add_to(m_2) # Get credit for your work after you have created a map q_2.a.check() # Show the map m_2 . Thank you for creating a map! . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 버클리의 5개 위치만 고려했을 때, (위도, 경도) 위치 중 몇 개가 (올바른 도시에 위치) 잠재적으로 맞는 것 같습니까? . q_2.b.solution() . Solution: All five locations appear to be correct! . 3) &#45936;&#51060;&#53552; &#53685;&#54633; . 아래 코드를 실행하여 캘리포니아 주의 각 카운티에 대한 이름, 면적(제곱킬로미터 단위) 및 고유 ID(&quot;GEOID&quot; 열에 있음)가 포함된 GeoDataFrame &quot;CA_counties&quot;를 로드합니다. 지오메트리 열에는 군 경계를 가진 다각형이 포함됩니다. . CA_counties = gpd.read_file(&quot;data/CA_county_boundaries/CA_county_boundaries/CA_county_boundaries.shp&quot;) CA_counties.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} CA_counties.head() . c: Users User anaconda3 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . GEOID name area_sqkm geometry . 0 6091 | Sierra County | 2491.995494 | POLYGON ((-120.65560 39.69357, -120.65554 39.6... | . 1 6067 | Sacramento County | 2575.258262 | POLYGON ((-121.18858 38.71431, -121.18732 38.7... | . 2 6083 | Santa Barbara County | 9813.817958 | MULTIPOLYGON (((-120.58191 34.09856, -120.5822... | . 3 6009 | Calaveras County | 2685.626726 | POLYGON ((-120.63095 38.34111, -120.63058 38.3... | . 4 6111 | Ventura County | 5719.321379 | MULTIPOLYGON (((-119.63631 33.27304, -119.6360... | . 그런 다음 세 가지 데이터 프레임을 생성합니다. . CA_pop은 각 군의 인구 추계를 담고 있다. | CA_high_earners에는 연간 최소 15만 달러의 소득이 있는 가구 수가 포함되어 있다. | CA_median_age에는 각 카운티의 중위연령이 포함됩니다. | . CA_pop = pd.read_csv(&quot;data/CA_county_population.csv&quot;, index_col=&quot;GEOID&quot;) CA_high_earners = pd.read_csv(&quot;data/CA_county_high_earners.csv&quot;, index_col=&quot;GEOID&quot;) CA_median_age = pd.read_csv(&quot;data/CA_county_median_age.csv&quot;, index_col=&quot;GEOID&quot;) . 다음 코드 셀을 사용하여 &#39;CA_pop&#39;, &#39;CA_high_earners&#39;, &#39;CA_median_age&#39;와 &#39;CA_counties&#39; GeoDataFrame을 결합합니다. . 생성된 GeoDataFrame의 이름을 &#39;CA_stats&#39;로 지정하고 8개의 열(&#39;GEOID&#39;, &#39;이름&#39;, &#39;area_sqkm&#39;, &#39;geometry&#39;, &#39;인구&#39;, &#39;high_earners&#39;, &#39;median_age&#39;)이 있는지 확인합니다. . CA_stats = CA_counties.set_index(&quot;GEOID&quot;, inplace=False).join([CA_high_earners, CA_median_age, CA_pop]).reset_index() # Check your answer q_3.check() . Correct . 이제 모든 데이터를 한 곳에 모아 놓았으므로 열의 조합을 사용하는 통계량을 계산하는 것이 훨씬 쉬워졌습니다. 다음 코드 셀을 실행하여 모집단 밀도가 있는 &quot;밀도&quot; 열을 만듭니다. . CA_stats[&quot;density&quot;] = CA_stats[&quot;population&quot;] / CA_stats[&quot;area_sqkm&quot;] . 4) &#50612;&#45712; &#45208;&#46972;&#44032; &#50976;&#47581;&#54644; &#48372;&#51060;&#45208;&#50836;? . 모든 정보를 단일 GeoDataFrame으로 축소하면 특정 기준에 맞는 카운티를 훨씬 쉽게 선택할 수 있습니다. . 다음 코드 셀을 사용하여 &quot;CA_stats&quot; GeoDataFrame의 행 부분 집합(및 모든 열)을 포함하는 GeoDataFrame &#39;sel_counties&#39;를 만듭니다. 특히 다음과 같은 카운티를 선택해야 합니다. . 적어도 100,000가구가 매년 150,000원을 번다. | 중위연령은 38.5세 미만이다. | 인구 밀도는 최소 285명(평방킬로미터당)이다. | . 또한, 선택된 카운티는 다음 기준 중 적어도 하나를 충족해야 합니다. . 적어도 50만 가구가 매년 150,000원을 번다. | 중위연령이 35.5세 미만 또는 | 거주자의 밀도는 최소한 1400 (평방 킬로미터 당)이다. | . sel_counties = CA_stats[((CA_stats.high_earners &gt; 100000) &amp; (CA_stats.median_age &lt; 38.5) &amp; (CA_stats.density &gt; 285) &amp; ((CA_stats.median_age &lt; 35.5) | (CA_stats.density &gt; 1400) | (CA_stats.high_earners &gt; 500000)))] # Check your answer q_4.check() . Correct . 5) &#47751; &#44060;&#51032; &#47588;&#51109;&#51012; &#54869;&#51064;&#54616;&#49512;&#45208;&#50836;? . 다음 Starbucks Reserve Roastery 위치를 찾을 때 선택한 카운티 내의 모든 매장을 고려하려고 합니다. 그렇다면, 선택된 카운티 내에 몇 개의 상점이 있을까요? . 이 질문에 대답할 준비를 하려면 다음 코드 셀을 실행하여 모든 스타벅스 위치가 포함된 GeoDataFrame &#39;starbucks_gdf&#39;를 생성하십시오. . starbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude)) starbucks_gdf.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} . c: Users User anaconda3 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . 그래서, 당신이 선택한 카운티에는 몇 개의 상점이 있나요? . locations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties) num_stores = len(locations_of_interest) # Check your answer q_5.check() . Correct . 6) &#49345;&#51216; &#50948;&#52824;&#47484; &#49884;&#44033;&#54868;&#54633;&#45768;&#45796;. . 이전 질문에서 식별한 스토어의 위치를 표시하는 맵을 만듭니다. . m_6 = folium.Map(location=[37,-120], zoom_start=6) # Your code here: show selected store locations mc = MarkerCluster() locations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties) for idx, row in locations_of_interest.iterrows(): if not math.isnan(row[&#39;Longitude&#39;]) and not math.isnan(row[&#39;Latitude&#39;]): mc.add_child(folium.Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]])) m_6.add_child(mc) # Get credit for your work after you have created a map q_6.check() # Show the map m_6 . Thank you for creating a map! . Make this Notebook Trusted to load map: File -&gt; Trust Notebook Keep going . Learn about how proximity analysis can help you to understand the relationships between points on a map. .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/2022/07/23/_Geospatial04.html",
            "relUrl": "/2022/07/23/_Geospatial04.html",
            "date": " • Jul 23, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Geospatial_03_exercise",
            "content": "import pandas as pd import geopandas as gpd import folium from folium import Choropleth from folium.plugins import HeatMap from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex3 import * . 우리는 대화형 맵을 표시하기 위한 함수 &#39;embed_map()&#39;을 정의한다. 맵을 포함하는 변수와 맵을 저장할 HTML 파일의 이름이라는 두 가지 인수를 사용할 수 있습니다. . 이 기능은 [모든 웹 브라우저에서] 지도를 볼 수 있도록 합니다(https://github.com/python-visualization/folium/issues/812). . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . 1) &#51648;&#51652;&#51008; &#54032; &#44221;&#44228;&#50752; &#51068;&#52824;&#54633;&#45768;&#44620;? . 아래의 코드 셀을 실행하여 글로벌 플레이트 경계를 표시하는 DataFrame &#39;plate_boundaries&#39;를 생성하십시오. 좌표 열은 경계를 따라 (위도, 경도) 위치 목록입니다. . plate_boundaries = gpd.read_file(&quot;data/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp&quot;) plate_boundaries[&#39;coordinates&#39;] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis=&#39;columns&#39;) plate_boundaries.drop(&#39;geometry&#39;, axis=1, inplace=True) plate_boundaries.head() . HAZ_PLATES HAZ_PLAT_1 HAZ_PLAT_2 Shape_Leng coordinates . 0 TRENCH | SERAM TROUGH (ACTIVE) | 6722 | 5.843467 | [(-5.444200361999947, 133.6808931800001), (-5.... | . 1 TRENCH | WETAR THRUST | 6722 | 1.829013 | [(-7.760600482999962, 125.47879802900002), (-7... | . 2 TRENCH | TRENCH WEST OF LUZON (MANILA TRENCH) NORTHERN ... | 6621 | 6.743604 | [(19.817899819000047, 120.09999798800004), (19... | . 3 TRENCH | BONIN TRENCH | 9821 | 8.329381 | [(26.175899215000072, 143.20620700100005), (26... | . 4 TRENCH | NEW GUINEA TRENCH | 8001 | 11.998145 | [(0.41880004000006466, 132.8273013480001), (0.... | . 그런 다음 변경 없이 아래의 코드 셀을 실행하여 과거 지진 데이터를 데이터 프레임 &quot;지진&quot;에 로드합니다. . earthquakes = pd.read_csv(&quot;data/earthquakes1970-2014.csv&quot;, parse_dates=[&quot;DateTime&quot;]) earthquakes.head() . DateTime Latitude Longitude Depth Magnitude MagType NbStations Gap Distance RMS Source EventID . 0 1970-01-04 17:00:40.200 | 24.139 | 102.503 | 31.0 | 7.5 | Ms | 90.0 | NaN | NaN | 0.0 | NEI | 1.970010e+09 | . 1 1970-01-06 05:35:51.800 | -9.628 | 151.458 | 8.0 | 6.2 | Ms | 85.0 | NaN | NaN | 0.0 | NEI | 1.970011e+09 | . 2 1970-01-08 17:12:39.100 | -34.741 | 178.568 | 179.0 | 6.1 | Mb | 59.0 | NaN | NaN | 0.0 | NEI | 1.970011e+09 | . 3 1970-01-10 12:07:08.600 | 6.825 | 126.737 | 73.0 | 6.1 | Mb | 91.0 | NaN | NaN | 0.0 | NEI | 1.970011e+09 | . 4 1970-01-16 08:05:39.000 | 60.280 | -152.660 | 85.0 | 6.0 | ML | 0.0 | NaN | NaN | NaN | AK | NaN | . 아래의 코드 셀은 지도에서 플레이트 경계를 시각화합니다. 지진 데이터를 모두 사용하여 동일한 지도에 열 지도를 추가하고 지진이 플레이트 경계와 일치하는지 여부를 확인합니다. . m_1 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) for i in range(len(plate_boundaries)): folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color=&#39;black&#39;).add_to(m_1) # Your code here: Add a heatmap to the map HeatMap(data=earthquakes[[&#39;Latitude&#39;, &#39;Longitude&#39;]], radius=15).add_to(m_1) # Get credit for your work after you have created a map q_1.a.check() # Show the map m_1 . Thank you for creating a map! . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 그렇다면, 위의 지도를 볼 때, 지진은 판 경계와 일치할까요? . q_1.b.solution() . Solution: Yes, earthquakes coincide with plate boundaries. . 2) &#51068;&#48376;&#50640;&#49436;&#45716; &#51648;&#51652;&#51032; &#44618;&#51060;&#50752; &#54032; &#44221;&#44228;&#50640; &#45824;&#54620; &#44540;&#51217;&#49457; &#49324;&#51060;&#50640; &#44288;&#44228;&#44032; &#51080;&#49845;&#45768;&#44620;? . 당신은 최근에 지진의 깊이가 우리에게 지구의 구조에 대한 중요한 정보를 알려준다는 것을 읽었습니다. 여러분은 흥미로운 세계적 패턴이 있는지 알고 싶으실 겁니다. 그리고 일본에서는 깊이가 어떻게 다른지 알고 싶으실 겁니다. . m_2 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) for i in range(len(plate_boundaries)): folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color=&#39;black&#39;).add_to(m_2) # Your code here: Add a map to visualize earthquake depth def color_producer(val): if val &lt; 50: return &#39;forestgreen&#39; elif val &lt; 100: return &#39;darkorange&#39; else: return &#39;darkred&#39; for i in range(0,len(earthquakes)): folium.Circle( location=[earthquakes.iloc[i][&#39;Latitude&#39;], earthquakes.iloc[i][&#39;Longitude&#39;]], radius=2000, color=color_producer(earthquakes.iloc[i][&#39;Depth&#39;])).add_to(m_2) # Get credit for your work after you have created a map q_2.a.check() # View the map m_2 . Thank you for creating a map! . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 플레이트 경계에 대한 근접성과 지진 깊이 사이의 관계를 탐지할 수 있습니까? 이 패턴이 전체적으로 유지됩니까? 일본에서요? . q_2.b.solution() . Solution: In the northern half of Japan, it does appear that earthquakes closer to plate boundaries tend to be shallower (and earthquakes farther from plate boundaries are deeper). This pattern is repeated in other locations, such as the western coast of South America. But, it does not hold everywhere (for instance, in China, Mongolia, and Russia). . 3) &#51064;&#44396;&#48128;&#46020;&#44032; &#45458;&#51008; &#54788;&#51008; &#50612;&#46356;&#51077;&#45768;&#44620;? . 다음 코드 셀(변경 없이)을 실행하여 일본 현의 지리적 경계를 포함하는 GeoDataFrame &#39;현&#39;을 만듭니다. . prefectures = gpd.read_file(&quot;data/japan-prefecture-boundaries/japan-prefecture-boundaries/japan-prefecture-boundaries.shp&quot;) prefectures.set_index(&#39;prefecture&#39;, inplace=True) prefectures.head() . geometry . prefecture . Aichi MULTIPOLYGON (((137.09523 34.65330, 137.09546 ... | . Akita MULTIPOLYGON (((139.55725 39.20330, 139.55765 ... | . Aomori MULTIPOLYGON (((141.39860 40.92472, 141.39806 ... | . Chiba MULTIPOLYGON (((139.82488 34.98967, 139.82434 ... | . Ehime MULTIPOLYGON (((132.55859 32.91224, 132.55904 ... | . 다음 코드셀은 일본 각 도도부현의 인구, 면적(제곱킬로미터), 인구밀도(제곱킬로미터당)를 포함하는 데이터 프레임 &#39;통계&#39;를 생성한다. 코드 셀을 변경하지 않고 실행합니다. . population = pd.read_csv(&quot;data/japan-prefecture-population.csv&quot;) population.set_index(&#39;prefecture&#39;, inplace=True) # Calculate area (in square kilometers) of each prefecture area_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name=&#39;area_sqkm&#39;) stats = population.join(area_sqkm) # Add density (per square kilometer) of each prefecture stats[&#39;density&#39;] = stats[&quot;population&quot;] / stats[&quot;area_sqkm&quot;] stats.head() . population area_sqkm density . prefecture . Tokyo 12868000 | 1800.614782 | 7146.448049 | . Kanagawa 8943000 | 2383.038975 | 3752.771186 | . Osaka 8801000 | 1923.151529 | 4576.342460 | . Aichi 7418000 | 5164.400005 | 1436.372085 | . Saitama 7130000 | 3794.036890 | 1879.264806 | . 다음 코드 셀을 사용하여 모집단 밀도를 시각화하는 맥락막 맵을 만듭니다. . m_3 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) # Your code here: create a choropleth map to visualize population density Choropleth(geo_data=prefectures[&#39;geometry&#39;].__geo_interface__, data=stats[&#39;density&#39;], key_on=&quot;feature.id&quot;, fill_color=&#39;YlGnBu&#39;, legend_name=&#39;Population density (per square kilometer)&#39; ).add_to(m_3) # Get credit for your work after you have created a map q_3.a.check() # View the map # embed_map(m_3, &#39;q_3.html&#39;) # m_3 . Thank you for creating a map! . 어느 세 개의 현이 다른 현보다 상대적으로 밀도가 높습니까? 그들은 전국에 퍼져 있는가, 아니면 거의 같은 지리적 지역에 위치해 있는가? (일본 지리에 익숙하지 않은 경우 이 지도가 질문에 답하는 데 유용할 수 있습니다. . q_3.b.solution() . Solution: Tokyo, Kanagawa, and Osaka have the highest population density. All of these prefectures are located in central Japan, and Tokyo and Kanagawa are adjacent. . 4) &#44256;&#48128;&#46020; &#54788; &#51473; &#44508;&#47784; &#51648;&#51652;&#51060; &#48156;&#49373;&#54616;&#44592; &#49772;&#50868; &#54788;&#51008; &#50612;&#46356;&#51077;&#45768;&#44620;? . 지진 보강의 혜택을 받을 수 있는 하나의 현을 제안하기 위한 지도를 만듭니다. 지도는 밀도와 지진 규모를 모두 시각화해야 한다. . m_4 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) # Your code here: create a map def color_producer(magnitude): if magnitude &gt; 6.5: return &#39;red&#39; else: return &#39;green&#39; Choropleth( geo_data=prefectures[&#39;geometry&#39;].__geo_interface__, data=stats[&#39;density&#39;], key_on=&quot;feature.id&quot;, fill_color=&#39;BuPu&#39;, legend_name=&#39;Population density (per square kilometer)&#39;).add_to(m_4) for i in range(0,len(earthquakes)): folium.Circle( location=[earthquakes.iloc[i][&#39;Latitude&#39;], earthquakes.iloc[i][&#39;Longitude&#39;]], popup=(&quot;{} ({})&quot;).format( earthquakes.iloc[i][&#39;Magnitude&#39;], earthquakes.iloc[i][&#39;DateTime&#39;].year), radius=earthquakes.iloc[i][&#39;Magnitude&#39;]**5.5, color=color_producer(earthquakes.iloc[i][&#39;Magnitude&#39;])).add_to(m_4) # Get credit for your work after you have created a map q_4.a.check() # View the map # m_4 . Thank you for creating a map! . 추가 지진 보강을 위해 어느 현을 추천하십니까? . q_4.b.solution() . Solution: While there&#39;s no clear, single answer to this question, there are a few reasonable options. Tokyo is by far the most densely populated prefecture and has also experienced a number of earthquakes. Osaka is relatively less densely populated, but experienced an earthquake that was relatively stronger than those near Tokyo. And, the long coast of Kanagawa (in addition to its high density and the historical proximity of strong earthquakes) might lead us to worry about the added potential tsunami risk. . Keep going . Learn how to convert names of places to geographic coordinates with geocoding. You&#39;ll also explore special ways to join information from multiple GeoDataFrames. .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/2022/07/23/_Geospatial03.html",
            "relUrl": "/2022/07/23/_Geospatial03.html",
            "date": " • Jul 23, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Geospatial_02_exercise",
            "content": "import pandas as pd import geopandas as gpd from shapely.geometry import LineString from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex2 import * . c: Users User anaconda3 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) c: Users User anaconda3 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . 1) &#45936;&#51060;&#53552;&#47484; &#47196;&#46300;&#54633;&#45768;&#45796;. . 다음 코드 셀(변경 없이)을 실행하여 GPS 데이터를 팬더 데이터 프레임 &#39;birds_df&#39;에 로드합니다. . birds_df = pd.read_csv(&quot;data/purple_martin.csv&quot;, parse_dates=[&#39;timestamp&#39;]) print(&quot;There are {} different birds in the dataset.&quot;.format(birds_df[&quot;tag-local-identifier&quot;].nunique())) birds_df.head() . There are 11 different birds in the dataset. . timestamp location-long location-lat tag-local-identifier . 0 2014-08-15 05:56:00 | -88.146014 | 17.513049 | 30448 | . 1 2014-09-01 05:59:00 | -85.243501 | 13.095782 | 30448 | . 2 2014-10-30 23:58:00 | -62.906089 | -7.852436 | 30448 | . 3 2014-11-15 04:59:00 | -61.776826 | -11.723898 | 30448 | . 4 2014-11-30 09:59:00 | -61.241538 | -11.612237 | 30448 | . 데이터 세트에는 11개의 새가 있으며, 각 새는 &quot;태그-로컬-식별자&quot; 열의 고유한 값으로 식별된다. 각 새들은 일 년 중 다른 시간에 수집되는 몇 가지 치수를 가지고 있다. . 다음 코드 셀을 사용하여 GeoDataFrame &quot;birds&quot;를 만듭니다. . birds는 birds_df의 모든 열과 (경도, 위도) 위치의 점 객체를 포함하는 &quot;기하학&quot; 열을 가져야 한다. | &#39;birds&#39;의 CRS를 &#39;{&#39;init&#39;&#39;로 설정합니다. &#39;epsg:4326&#39;}? | . birds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[&quot;location-long&quot;], birds_df[&quot;location-lat&quot;])) # Your code here: Set the CRS to {&#39;init&#39;: &#39;epsg:4326&#39;} birds.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # Check your answer q_1.check() . c: Users User anaconda3 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . Correct . 2) &#45936;&#51060;&#53552;&#47484; &#54364;&#49884;&#54633;&#45768;&#45796;. . 그런 다음 GeoPandas에서 &#39;natural earth_lowres&#39; 데이터 세트를 로드하고, &#39;아메리카&#39;를 미주(북미 및 남미)의 모든 국가의 경계를 포함하는 GeoDataFrame으로 설정한다. 변경 사항 없이 다음 코드 셀을 실행합니다. . world = gpd.read_file(gpd.datasets.get_path(&#39;naturalearth_lowres&#39;)) americas = world.loc[world[&#39;continent&#39;].isin([&#39;North America&#39;, &#39;South America&#39;])] americas.head() . pop_est continent name iso_a3 gdp_md_est geometry . 3 35623680 | North America | Canada | CAN | 1674000.0 | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 326625791 | North America | United States of America | USA | 18560000.0 | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . 9 44293293 | South America | Argentina | ARG | 879400.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.25000... | . 10 17789267 | South America | Chile | CHL | 436100.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.63335... | . 16 10646714 | North America | Haiti | HTI | 19340.0 | POLYGON ((-71.71236 19.71446, -71.62487 19.169... | . 다음 코드 셀을 사용하여 (1) &quot;아메리카&quot; GeoDataFrame의 국가 경계와 (2) &quot;birds_gdf&quot; GeoDataFrame의 모든 점을 보여주는 단일 그래프를 만듭니다. . 여기서는 특별한 스타일링에 대해 걱정할 필요 없이 모든 데이터가 올바르게 로드되었는지 신속하게 확인하기 위해 예비 플롯을 생성하기만 하면 됩니다. 특히 새를 구분하기 위해 포인트를 컬러 코딩할 염려도 없고, 시작 포인트와 끝 포인트를 구분할 필요도 없다. 우리는 연습의 다음 부분에서 그것을 할 것이다. . ax = americas.plot(figsize=(10,10), color=&#39;white&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;gray&#39;) birds.plot(ax=ax, markersize=10) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1e890a0d640&gt; . q_2.check() . Thank you for creating a map! . 3) &#44033;&#44033;&#51032; &#49352;&#45716; &#50612;&#46356;&#50640;&#49436; &#52636;&#48156;&#54616;&#44256; &#44536; &#50668;&#51221;&#51012; &#45149;&#45253;&#45768;&#44620;? (1&#48512;) . 이제, 우리는 각각의 새들의 길을 더 자세히 볼 준비가 되었습니다. 다음 코드 셀을 실행하여 두 개의 GeoDataFrame을 생성합니다. . &#39;path_gdf&#39;에는 각 버드의 경로를 보여주는 LineString 개체가 포함되어 있습니다. LineString() 메서드를 사용하여 Point 객체 목록에서 LineString 객체를 생성합니다. | &#39;start_gdf&#39;에는 각 새의 시작점이 포함되어 있습니다. | . path_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: LineString(x)).reset_index() path_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry) path_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # GeoDataFrame showing starting point for each bird start_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: x[0]).reset_index() start_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry) start_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # Show first five rows of GeoDataFrame start_gdf.head() . c: Users User anaconda3 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) c: Users User anaconda3 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . tag-local-identifier geometry . 0 30048 | POINT (-90.12992 20.73242) | . 1 30054 | POINT (-93.60861 46.50563) | . 2 30198 | POINT (-80.31036 25.92545) | . 3 30263 | POINT (-76.78146 42.99209) | . 4 30275 | POINT (-76.78213 42.99207) | . 다음 코드 셀을 사용하여 각 버드의 최종 위치를 포함하는 GeoDataFrame &#39;end_gdf&#39;를 만듭니다. . 형식은 &quot;tag-local-identifier&quot; 및 &quot;geometry&quot; 열에서 점 객체를 포함하는 두 개의 열(&quot;tag-local-identifier&quot;)과 &quot;start_gdf&quot;의 형식과 동일해야 한다. | &#39;end_gdf&#39;의 CRS를 &#39;{&#39;init&#39;로 설정합니다. &#39;epsg:4326&#39;}: | . end_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: x[-1]).reset_index() end_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry) end_gdf.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} # Check your answer q_3.check() . c: Users User anaconda3 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . Correct . 4) &#44033;&#44033;&#51032; &#49352;&#45716; &#50612;&#46356;&#50640;&#49436; &#52636;&#48156;&#54616;&#44256; &#44536; &#50668;&#51221;&#51012; &#45149;&#45253;&#45768;&#44620;? (2&#48512;) . 위 질문(&#39;path_gdf&#39;, &#39;start_gdf&#39;, &#39;end_gdf&#39;)의 GeoDataFrames를 사용하여 모든 새의 경로를 단일 지도에서 시각화한다. 또한 &quot;아메리카&quot; GeoDataFrame을 사용할 수도 있습니다. . ax = americas.plot(figsize=(10, 10), color=&#39;white&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;gray&#39;) start_gdf.plot(ax=ax, color=&#39;red&#39;, markersize=30) path_gdf.plot(ax=ax, cmap=&#39;tab20b&#39;, linestyle=&#39;-&#39;, linewidth=1, zorder=1) end_gdf.plot(ax=ax, color=&#39;black&#39;, markersize=30) # Uncomment to see a hint #_COMMENT_IF(PROD)_ # Get credit for your work after you have created a map q_4.check() . Thank you for creating a map! . 5) &#45224;&#50500;&#47700;&#47532;&#52852;&#51032; &#48372;&#54840; &#44396;&#50669;&#51008; &#50612;&#46356;&#51064;&#44032;? (1&#48512;) . 모든 새들은 결국 남아메리카의 어딘가에 있는 것처럼 보입니다. 하지만 그들은 보호구역으로 갈까요? . 다음 코드 셀에서는 남아메리카에 있는 모든 보호 지역의 위치를 포함하는 GeoDataFrame &quot;protected_areas&quot;를 만듭니다. 해당 셰이프 파일은 파일 경로 &#39;protected_filepath&#39;에 있습니다. . protected_filepath = &quot;data/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp&quot; # Your code here protected_areas = gpd.read_file(protected_filepath) # Check your answer q_5.check() . Correct . 6) &#45224;&#50500;&#47700;&#47532;&#52852;&#51032; &#48372;&#54840; &#44396;&#50669;&#51008; &#50612;&#46356;&#51064;&#44032;? (2&#48512;) . &#39;protected_areas&#39; GeoDataFrame을 사용하여 남미에서 보호 지역의 위치를 표시하는 그래프를 만듭니다. (여러분은 어떤 보호 구역은 육지에 있고 다른 보호 구역은 해양에 있다는 것을 알게 될 것입니다.) . south_america = americas.loc[americas[&#39;continent&#39;]==&#39;South America&#39;] # Your code here: plot protected areas in South America ax = south_america.plot(figsize=(10,10), color=&#39;white&#39;, edgecolor=&#39;gray&#39;) protected_areas.plot(ax=ax, alpha=0.4) # Uncomment to see a hint # Get credit for your work after you have created a map q_6.check() . Thank you for creating a map! . 7) &#45224;&#50500;&#47700;&#47532;&#52852;&#51032; &#47751; &#54140;&#49468;&#53944;&#44032; &#48372;&#54840;&#46104;&#44256; &#51080;&#45208;&#50836;? . 여러분은 남아메리카의 몇 퍼센트가 보호되고 있는지 알아내는 것에 관심이 있습니다. 그래서 여러분은 남아메리카의 얼마나 많은 부분이 새들에게 적합한지 알 수 있습니다. . 첫 번째 단계로 남아메리카에 있는 모든 보호 토지의 총 면적(해상 면적은 제외)을 계산합니다. 이렇게 하려면 &quot;REP_AREA&quot; 열과 &quot;REP_M_AREA&quot; 열을 사용합니다. 이 열에는 총 면적과 총 해양 면적이 각각 제곱 킬로미터 단위로 포함됩니다. . 아래의 코드 셀을 변경하지 않고 실행합니다. . P_Area = sum(protected_areas[&#39;REP_AREA&#39;]-protected_areas[&#39;REP_M_AREA&#39;]) print(&quot;South America has {} square kilometers of protected areas.&quot;.format(P_Area)) . South America has 5396761.9116883585 square kilometers of protected areas. . Then, to finish the calculation, you&#39;ll use the south_america GeoDataFrame. . south_america.head() . pop_est continent name iso_a3 gdp_md_est geometry . 9 44293293 | South America | Argentina | ARG | 879400.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.25000... | . 10 17789267 | South America | Chile | CHL | 436100.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.63335... | . 20 2931 | South America | Falkland Is. | FLK | 281.8 | POLYGON ((-61.20000 -51.85000, -60.00000 -51.2... | . 28 3360148 | South America | Uruguay | URY | 73250.0 | POLYGON ((-57.62513 -30.21629, -56.97603 -30.1... | . 29 207353391 | South America | Brazil | BRA | 3081000.0 | POLYGON ((-53.37366 -33.76838, -53.65054 -33.2... | . 다음 단계에 따라 남아메리카의 총 면적을 계산합니다. . 각 다각형(EPSG 3035를 CRS로 하여)의 &#39;면적&#39; 속성을 이용하여 각국의 면적을 계산하고 그 결과를 합산한다. 계산된 면적은 제곱미터 단위로 표시됩니다. | 답을 제곱 킬로미터 단위로 변환합니다. | . totalArea = sum(south_america.geometry.to_crs(epsg=3035).area) / 10**6 # Check your answer q_7.check() . Correct . 아래의 코드 셀을 실행하여 보호되는 남아메리카의 비율을 계산하십시오. . percentage_protected = P_Area/totalArea print(&#39;Approximately {}% of South America is protected.&#39;.format(round(percentage_protected*100, 2))) . Approximately 30.39% of South America is protected. . 8) &#45224;&#50500;&#47700;&#47532;&#52852;&#51032; &#49352;&#46308;&#51008; &#50612;&#46356;&#50640; &#51080;&#45208;&#50836;? . 그렇다면, 그 새들은 보호 구역에 있을까요? . 남아메리카에서 발견된 모든 새의 위치를 보여주는 그래프를 만듭니다. 또한 남아메리카에 있는 모든 보호 구역의 위치를 표시합니다. . 순수 해양 영역(육지 구성요소가 없는)인 보호 영역을 제외하려면 &quot;MARINE&quot; 열을 사용합니다(그리고 &#39;protected_areas[protected_areas]&#39;의 행만 표시).마린&#39;]!=&#39;2&#39;]는 &#39;protected_filename&#39; GeoDataFrame)의 모든 행 대신 발음한다. . ax = south_america.plot(figsize=(10,10), color=&#39;white&#39;, edgecolor=&#39;gray&#39;) protected_areas[protected_areas[&#39;MARINE&#39;]!=&#39;2&#39;].plot(ax=ax, alpha=0.4, zorder=1) birds[birds.geometry.y &lt; 0].plot(ax=ax, color=&#39;red&#39;, alpha=0.6, markersize=10, zorder=2) # Get credit for your work after you have created a map q_8.check() . Thank you for creating a map! . Keep going . Create stunning interactive maps with your geospatial data. .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/2022/07/23/_Geospatial02.html",
            "relUrl": "/2022/07/23/_Geospatial02.html",
            "date": " • Jul 23, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Geospatial_01_exercise",
            "content": "import geopandas as gpd from learntools.core import binder binder.bind(globals()) from learntools.geospatial.ex1 import * . 1) &#45936;&#51060;&#53552;&#47484; &#44032;&#51256;&#50741;&#45768;&#45796;. . 다음 셀을 사용하여 &#39;loans_filepath&#39;에 있는 쉐이프 파일을 로드하여 GeoDataFrame &#39;world_loans&#39;를 만듭니다. . loans_filepath = &quot;data/kiva_loans/kiva_loans/kiva_loans.shp&quot; # Your code here: Load the data world_loans = gpd.read_file(loans_filepath) # Check your answer world_loans.head() # Uncomment to view the first five rows of the data #world_loans.head() q_1.check() . Correct . 2) &#45936;&#51060;&#53552;&#47484; &#54364;&#49884;&#54633;&#45768;&#45796;. . 변경 사항 없이 다음 코드 셀을 실행하여 국가 경계를 포함하는 GeoDataFrame &quot;world&quot;를 로드합니다. . world_filepath = gpd.datasets.get_path(&#39;naturalearth_lowres&#39;) world = gpd.read_file(world_filepath) world.head() . pop_est continent name iso_a3 gdp_md_est geometry . 0 920938 | Oceania | Fiji | FJI | 8374.0 | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | . 1 53950935 | Africa | Tanzania | TZA | 150600.0 | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | . 2 603253 | Africa | W. Sahara | ESH | 906.5 | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | . 3 35623680 | North America | Canada | CAN | 1674000.0 | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 326625791 | North America | United States of America | USA | 18560000.0 | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . &quot;world&quot; 및 &quot;world_loans&quot; GeoDataFrames를 사용하여 전 세계 Kiva 대출 위치를 시각화합니다. . world.plot() world_loans.plot() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1745ed772e0&gt; . ax = world.plot(figsize=(20,20), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;black&#39;) world_loans.plot(ax=ax, markersize=2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1745ee26fa0&gt; . 3) &#54596;&#47532;&#54592;&#51012; &#44592;&#48152;&#51004;&#47196; loans&#47484; &#49440;&#53469;&#54633;&#45768;&#45796;. . 다음으로, 당신은 필리핀에 기반을 둔 loans에 초점을 맞출 것이다. 다음 코드 셀을 사용하여 필리핀에 기반을 둔 loans이 있는 &quot;world_loans&quot;의 모든 행을 포함하는 GeoDataFrame &quot;PHL_loans&quot;를 만듭니다. . PHL_loans = world_loans[world_loans[&#39;country&#39;] == &quot;Philippines&quot;].copy() # Check your answer q_3.check() . Correct . # PHL_loans = world_loans.loc[world_loans.country==&quot;Philippines&quot;].copy() . 4) &#54596;&#47532;&#54592;&#51032; loans&#47484; &#51060;&#54644;&#54633;&#45768;&#45796;. . 변경 없이 다음 코드 셀을 실행하여 필리핀의 모든 섬에 대한 경계를 포함하는 GeoDataFrame &quot;PHL&quot;을 로드합니다. . gpd.io.file.fiona.drvsupport.supported_drivers[&#39;KML&#39;] = &#39;rw&#39; PHL = gpd.read_file(&quot;data/Philippines_AL258.kml&quot;, driver=&#39;KML&#39;) PHL.head() . Name Description geometry . 0 Autonomous Region in Muslim Mindanao | | MULTIPOLYGON (((119.46690 4.58718, 119.46653 4... | . 1 Bicol Region | | MULTIPOLYGON (((124.04577 11.57862, 124.04594 ... | . 2 Cagayan Valley | | MULTIPOLYGON (((122.51581 17.04436, 122.51568 ... | . 3 Calabarzon | | MULTIPOLYGON (((120.49202 14.05403, 120.49201 ... | . 4 Caraga | | MULTIPOLYGON (((126.45401 8.24400, 126.45407 8... | . 필리핀의 loans를 시각화하려면 &#39;PHL&#39;과 &#39;PHL_loans&#39; GeoDataFrame을 사용하십시오. . ax = PHL.plot(figsize=(12,12), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;lightgray&#39;) PHL_loans.plot(ax=ax, markersize=2) # Uncomment to see a hint #_COMMENT_IF(PROD)_ q_4.a.hint() . Hint: Use the plot() method of each GeoDataFrame. . # ax = PHL.plot(figsize=(12,12), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;lightgray&#39;) # PHL_loans.plot(ax=ax, markersize=2) . q_4.a.check() # Uncomment to see our solution (your code may look different!) #_COMMENT_IF(PROD)_ q_4.a.solution() . Thank you for creating a map! . Solution: . ax = PHL.plot(figsize=(12,12), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;lightgray&#39;) PHL_loans.plot(ax=ax, markersize=2) . 새로운 Field Partner를 모집하는 데 유용한 섬이 있습니까? 현재 키바의 손이 닿지 않는 곳에 있는 섬이 있나요? . 질문에 답하는 데 이 지도가 유용할 수 있습니다. . q_4.b.solution() . Solution: There are a number of potential islands, but Mindoro (in the central part of the Philippines) stands out as a relatively large island without any loans in the current dataset. This island is potentially a good location for recruiting new Field Partners! . Keep going . Continue to learn about coordinate reference systems. .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/2022/07/23/_Geospatial01.html",
            "relUrl": "/2022/07/23/_Geospatial01.html",
            "date": " • Jul 23, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Visualization_Seaborn_and_Matplotlib",
            "content": "Tools - matplotlib . 원문: by Aurélien Geron (Link) . | Translated by Chansung PARK (Link) . | Object Oriented API Addition by Jehyun LEE (Link) | . Tools - matplotlib . 이 노트북은 matplotlib 라이브러리를 사용하여 아름다운 그래프를 그리는 방법을 보여줍니다. . 이제현 주 : 원 코드가 pyplot 기반으로 작성되었기에 object oriented API를 추가하였습니다. pyplot은 pandas 같은 라이브러리와 함께 사용하며 그래프를 빠르게 그려보기 좋습니다. 그러나 코드의 가독성과 섬세한 제어는 object oriented API(객체지향 인터페이스)방식이 더 유리하게 느껴집니다. . pyplot과 object oriented API의 차이에 대해 상세히 알고 싶으시면 이 글을 참고하십시오 | . &#47785;&#52264; . 1&nbsp;&nbsp;처음으로 그려보는 그래프2&nbsp;&nbsp;선의 스타일과 색상3&nbsp;&nbsp;그림의 저장4&nbsp;&nbsp;부분 그래프5&nbsp;&nbsp;여러개의 그림6&nbsp;&nbsp;Pyplot의 상태 머신: 암시적 vs 명시적 7&nbsp;&nbsp;Pylab vs Pyplot vs Matplotlib8&nbsp;&nbsp;텍스트 그리기9&nbsp;&nbsp;범례 (Legends)10&nbsp;&nbsp;비선형 척도 (Non linear scales)11&nbsp;&nbsp;틱과 티커 (Ticks and tickers)12&nbsp;&nbsp;극좌표계 투영 (Polar projection)13&nbsp;&nbsp;3차원 투영14&nbsp;&nbsp;산점도15&nbsp;&nbsp;선16&nbsp;&nbsp;히스토그램17&nbsp;&nbsp;이미지18&nbsp;&nbsp;애니메이션19&nbsp;&nbsp;애니메이션을 비디오로 저장20&nbsp;&nbsp;다음은 무엇을 해야할까? . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . &#52376;&#51020;&#51004;&#47196; &#44536;&#47140;&#48372;&#45716; &#44536;&#47000;&#54532; . 우선은 matplotlib 라이브러리를 임포트 해줘야 합니다. . import matplotlib . Matplotlib은 Tk, wxPython 등과 같이 다양한 그래픽 라이브러리를 기반으로 사용하여 그래프를 출력할 수 있습니다. 명령줄을 사용하여 Python을 실행하는 경우, 일반적으로 그래프는 별도의 윈도우 창에서 나타납니다. 주피터 노트북을 사용한다면, %matplotlib inline 이라는 magic command를 사용하여 노트북 속에서 그래프를 출력할 수 있습니다. . 이제현 주:* 매직 명령어, 마술 명령어라고도 불리는 Magic command는 파이썬 코드를 실행하는 것이 아니라 주피터 노트북이나 Colab같은 IPython 커널 사용을 도와주는 명령입니다. 현재 작업 디렉토리를 확인하거나(%pwd) 작업 수행에 걸리는 시간을 확인할 수 있습니다(%timeit). 여기서는 matplotlib의 실행 결과를 노트북에 그대로 보여주도록 지정하고 있습니다(%matplotlib inline). . magic command에 대해 자세히 알고 싶으면 이 글을 참고하세요.) | . %matplotlib inline # matplotlib.use(&quot;TKAgg&quot;) # 그래픽 백엔드로 Tk를 사용하고자 한다면, 이 코드를 사용하시기 바랍니다. . 그러면 첫 번째 그래프를 그려보겠습니다! :) . import matplotlib.pyplot as plt plt.plot([1, 2, 4, 9, 5, 3]) plt.show() . 그렇습니다. 데이터 몇 개로 plot 함수를 호출한 다음, show 함수를 호출해주면 간단히 그래프를 그려볼 수 있습니다! . plot 함수에 단일 배열의 데이터가 주어진다면, 수직 축의 좌표로서 이를 사용하게 되며, 각 데이터의 배열상 색인(인덱스)을 수평 좌표로서 사용합니다. 두 개의 배열을 넣어줄 수도 있습니다: 그러면, 하나는 x 축에 대한것이며, 다른 하나는 y 축에 대한것이 됩니다: . 이제현 주: 같은 그림을 object oriented API를 이용해 그려보겠습니다. object oriented API는 그래프의 각 부분을 객체로 지정하고 그리는 것으로, 다음과 같은 패턴을 가지고 있습니다. 아래 코드와 주석의 # object oriented API 부분은 이제현이 추가한 부분입니다. . object oriented API와 구분하기 위해 원본 코드에는 #pyplot이라는 헤더를 달았습니다.) | . fig, ax = plt.subplots() # 2. ax 위에 그래프를 그립니다. ax.plot([1, 2, 4, 9, 5, 3]) # 3. 그래프를 화면에 출력합니다. plt.show() . 이제현 주: pyplot과 동일한 형태의 그래프가 그려집니다. fig, ax를 선언하느라 한 줄을 더 입력해야 한다는 불편함이 있지만 ax 객체가 있어 그래프를 제어하기 더 쉬워집니다. . 많은 경우 fig, ax = plt.subplots() 대신 ax = plt.subplot()으로 해도 됩니다. | 그러나 fig 대상 명령(예. savefig)을 사용해야 할 때도 있고, 두 가지를 따로 외우려면 혼동이 되니 한 가지로 통일하는 것이 좋습니다. | . plt.plot([-3, -2, 5, 0], [1, 6, 4, 3]) plt.show() . fig, ax = plt.subplots() ax.plot([-3, -2, 5, 0], [1, 6, 4, 3]) plt.show() . 이번에는 수학적인 함수를 그려보겠습니다. NumPy의 linespace 함수를 사용하여 -2 ~ 2 범위에 속하는 500개의 부동소수로 구성된 x 배열을 생성합니다. 그 다음 x의 각 값의 거듭제곱된 값을 포함하는 y 배열을 생성합니다 (NumPy에 대하여 좀 더 알고 싶다면, NumPy 튜토리얼을 참고하시기 바랍니다). . import numpy as np x = np.linspace(-2, 2, 500) y = x**2 plt.plot(x, y) plt.show() . fig, ax = plt.subplots() ax.plot(x, y) plt.show() . 그래프가 약간은 삭막해 보입니다. 타이틀과 x 및 y축에 대한 라벨, 그리고 모눈자를 추가적으로 그려보겠습니다. . plt.plot(x, y) plt.title(&quot;Square function&quot;) plt.xlabel(&quot;x&quot;) plt.ylabel(&quot;y = x**2&quot;) plt.grid(True) #모눈자 plt.show() . 이제현 주 : object-oriented API는 축 이름과 같은 설정 명령어가 pyplot과 다소 다릅니다. 대체로 축 이름(label), 범위(limits) 등을 지정하는 명령어는 set_대상(), 거꾸로 그래프에서 설정값을 가져오는 명령어는 get_대상()으로 통일되어 있습니다. . 개인적으로 pyplot의 명령어 체계보다 object-oriented API의 체계를 선호합니다. | . fig, ax = plt.subplots() ax.plot(x, y) ax.set_title(&quot;Square function&quot;) ax.set_xlabel(&quot;x&quot;) ax.set_ylabel(&quot;y = x**2&quot;) ax.grid(True) plt.show() . &#49440;&#51032; &#49828;&#53440;&#51068;&#44284; &#49353;&#49345; . 기본적으로 matplotlib은 바로 다음에 위치한(연이은) 데이터 사이에 선을 그립니다. . plt.plot([0, 100, 100, 0, 0, 100, 50, 0, 100], [0, 0, 100, 100, 0, 100, 130, 100, 0]) plt.axis([-10, 110, -10, 140]) plt.show() . fig, ax = plt.subplots() ax.plot([0, 100, 100, 0, 0, 100, 50, 0, 100], [0, 0, 100, 100, 0, 100, 130, 100, 0]) ax.set_xlim(-10, 110) ax.set_ylim(-10, 140) # 그래프의 범위는 pyplot과 같이 ax.axis([-10, 110, -10, 140]) 으로 지정할 수 있습니다. # 하지만 위와 같이 set_xlim, set_ylim을 사용해서 명시하는 것이 더 체계적으로 느껴집니다. plt.show() . 세 번째 파라미터를 지정하면 선의 스타일과 색상을 바꿀 수 있습니다. 예를 들어서 &quot;g--&quot;는 &quot;초록색 파선&quot;을 의미합니다. 예를 들어 아래와 같이 말이죠: . plt.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], &quot;r-&quot;, [0, 100, 50, 0, 100], [0, 100, 130, 100, 0], &quot;g--&quot;) plt.axis([-10, 110, -10, 140]) plt.show() . fig, ax = plt.subplots() ax.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], &quot;r-&quot;, [0, 100, 50, 0, 100], [0, 100, 130, 100, 0], &quot;g--&quot;) ax.set_xlim(-10, 110) ax.set_ylim(-10, 140) plt.show() . 또는 show를 호출하기 전 plot을 여러번 호출해도 가능합니다. . plt.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], &quot;r-&quot;) plt.plot([0, 100, 50, 0, 100], [0, 100, 130, 100, 0], &quot;g--&quot;) plt.axis([-10, 110, -10, 140]) plt.show() . fig, ax = plt.subplots() ax.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], &quot;r-&quot;) ax.plot([0, 100, 50, 0, 100], [0, 100, 130, 100, 0], &quot;g--&quot;) ax.set_xlim(-10, 110) ax.set_ylim(-10, 140) plt.show() . 선 대신에 간단한 점을 그려보는 것도 가능합니다. 아래는 초록색 파선, 빨강 점선, 파랑 삼각형의 예를 보여줍니다. 공식 문서에서 사용 가능한 스타일 및 색상의 모든 옵션을 확인해 볼 수 있습니다. . x = np.linspace(-1.4, 1.4, 30) plt.plot(x, x, &#39;g--&#39;, x, x**2, &#39;r:&#39;, x, x**3, &#39;b^&#39;) plt.show() . fig, ax = plt.subplots() x = np.linspace(-1.4, 1.4, 30) ax.plot(x, x, &#39;g--&#39;) ax.plot(x, x**2, &#39;r:&#39;) ax.plot(x, x**3, &#39;b^&#39;) # 여러 그래프를 ax.plot(x, x, &#39;g--&#39;, x, x**2, &#39;r:&#39;, x, x**3, &#39;b^&#39;)과 같이 한 줄에 그릴 수도 있습니다. # 그러나 이와 같이 따로 떼서 그리면 혼동을 방지할 수 있습니다. # 이는 pyplot도 마찬가지입니다. plt.show() . plot 함수는 Line2D객체로 구성된 리스트를 반환합니다 (각 객체가 각 선에 대응됩니다). 이 선들에 대한 추가적인 속성을 설정할 수도 있습니다. 가령 선의 두께, 스타일, 투명도 같은것의 설정이 가능합니다. 공식 문서에서 설정 가능한 모든 속성을 확인해볼 수 있습니다. . x = np.linspace(-1.4, 1.4, 30) line1, line2, line3 = plt.plot(x, x, &#39;g--&#39;, x, x**2, &#39;r:&#39;, x, x**3, &#39;b^&#39;) line1.set_linewidth(3.0) #선의굵기 line1.set_dash_capstyle(&quot;round&quot;) #스타일 line3.set_alpha(0.2) #투명도 plt.show() . x = np.linspace(-1.4, 1.4, 30) fig, ax = plt.subplots() # plot을 나누어 그리면 어디에 어떤 설정이 적용되었는지 알아보기 편합니다. # linewidth, alpha와 같은 line style도 plot() 안에 넣으면 혼동을 방지할 수 있습니다. line1 = ax.plot(x, x, &#39;g--&#39;, linewidth=3, dash_capstyle=&#39;round&#39;) line2 = ax.plot(x, x**2, &#39;r:&#39;) line3 = ax.plot(x, x**3, &#39;b^&#39;, alpha=0.2) plt.show() . &#44536;&#47548; &#51200;&#51109; . 그래프를 그림파일로 저장하는 방법은 간단합니다. 단순히 파일이름을 지정하여 savefig 함수를 호출해 주기만 하면 됩니다. 가능한 이미지 포맷은 사용하는 그래픽 백엔드에 따라서 지원 여부가 결정됩니다. . x = np.linspace(-1.4, 1.4, 30) plt.plot(x, x**2) plt.savefig(&quot;my_square_function.png&quot;, transparent=True) . &#48512;&#48516; &#44536;&#47000;&#54532; (subplot) . matplotlib는 하나의 그림(figure)에 여러개의 부분 그래프를 포함할 수 있습니다. 이 부분 그래프는 격자 형식으로 관리됩니다. subplot 함수를 호출하여 부분 그래프를 생성할 수 있습니다. 이 때 격자의 행/열의 수 및 그래프를 그리고자 하는 부분 그래프의 색인을 파라미터로서 지정해줄 수 있습니다 (색인은 1부터 시작하며, 좌-&gt;우, 상단-&gt;하단의 방향입니다). . pyplot은 현재 활성화된 부분 그래프를 계속해서 추적합니다 (plt.gca()를 호출하여 해당 부분 그래프의 참조를 얻을 수 있습니다). 따라서, plot 함수를 호출할 때 활성화된 부분 그래프에 그림이 그려지게 됩니다.이제현 주 : object oriented API 방식에서는 그래프를 그리기 전에 먼저 틀을 잡아둡니다. 그래프를 그릴 때 사전에 정의된 영역 중 어디에 그래프를 그릴지 지정하는 방식입니다. pyplot의 plt.gca()가 바로 object oriented API의 axes입니다. . | . x = np.linspace(-1.4, 1.4, 30) # subplot(2,2,1)은 subplot(221)로 축약할 수 있습니다. plt.subplot(2, 2, 1) # 2 행 2 열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단 plt.plot(x, x) plt.subplot(2, 2, 2) # 2 행 2 열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단 plt.plot(x, x**2) plt.subplot(2, 2, 3) # 2 행 2 열 크기의 격자 중 세 번째 부분 그래프 = 좌측 하단 plt.plot(x, x**3) plt.subplot(2, 2, 4) # 2 행 2 열 크기의 격자 중 네 번째 부분 그래프 = 우측 하단 plt.plot(x, x**4) plt.show() . x = np.linspace(-1.4, 1.4, 30) fig, ax = plt.subplots(2, 2) # 순서대로 row의 갯수, col의 갯수입니다. nrows=2, cols=2로 지정할 수도 있습니다. # plot위치는 ax[row, col] 또는 ax[row][col]로 지정합니다. ax[0, 0].plot(x, x) # 2 행 2 열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단 ax[0, 1].plot(x, x**2) # 2 행 2 열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단 ax[1, 0].plot(x, x**3) # 2 행 2 열 크기의 격자 중 세 번째 부분 그래프 = 좌측 하단 ax[1, 1].plot(x, x**4) # 2 행 2 열 크기의 격자 중 네 번째 부분 그래프 = 우측 하단 plt.show() . 격자의 여러 영역으로 확장된 부분 그래프를 생성하는 것도 쉽습니다: . plt.subplot(2, 2, 1) # 2 행 2 열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단 plt.plot(x, x) plt.subplot(2, 2, 2) # 2 행 2 열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단 plt.plot(x, x**2) plt.subplot(2, 1, 2) # 2행 *1* 열의 두 번째 부분 그래프 = 하단 # 2행 1열 크기의 그래프가 두 개 그려질 수 있지만, # 상단 부분은 이미 두 개의 부분 그래프가 차지하였다. # 따라서, 두 번째 부분 그래프로 지정함 plt.plot(x, x**3) plt.show() . grid = plt.GridSpec(2, 2) # 2행 2열 크기의 격?자를 준비합니다. ax1 = plt.subplot(grid[0, 0]) # 2행 2열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단 ax2 = plt.subplot(grid[0, 1]) # 2행 2열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단 ax3 = plt.subplot(grid[1, 0:]) # 2행 *1*열의 두 번째 부분 그래프 = 하단 # 범위를 [1, 0:]으로 설정하여 2행 전체를 지정함. ax1.plot(x, x) ax2.plot(x, x**2) ax3.plot(x, x**3) plt.show() . 보다 복잡한 부분 그래프의 위치 선정이 필요하다면, subplot2grid를 대신 사용할 수 있습니다. 격자의 행과 열의 번호 및 격자에서 해당 부분 그래프를 그릴 위치를 지정해줄 수 있습니다 (좌측상단 = (0,0). 또한 몇 개의 행/열로 확장되어야 하는지도 추가적으로 지정할 수 있습니다. 아래는 그에 대한 예를 보여줍니다: . plt.subplot2grid((3,3), (0, 0), rowspan=2, colspan=2) plt.plot(x, x**2) plt.subplot2grid((3,3), (0, 2)) plt.plot(x, x**3) plt.subplot2grid((3,3), (1, 2), rowspan=2) plt.plot(x, x**4) plt.subplot2grid((3,3), (2, 0), colspan=2) plt.plot(x, x**5) plt.show() . gridsize = (3, 3) # 2행 2열 크기의 격자를 준비합니다. ax1 = plt.subplot2grid(gridsize, (0,0), rowspan=2, colspan=2) ax2 = plt.subplot2grid(gridsize, (0,2)) ax3 = plt.subplot2grid(gridsize, (1,2), rowspan=2) ax4 = plt.subplot2grid(gridsize, (2,0), colspan=2) ax1.plot(x, x**2) ax2.plot(x, x**3) ax3.plot(x, x**4) ax4.plot(x, x**5) plt.show() . 보다 유연한 부분그래프 위치선정이 필요하다면, GridSpec 문서를 확인해 보시길 바랍니다. . &#50668;&#47084;&#44060;&#51032; &#44536;&#47548; (figure) . 여러개의 그림을 그리는것도 가능합니다. 각 그림은 하나 이상의 부분 그래프를 가질 수 있습니다. 기본적으로는 matplotlib이 자동으로 figure(1)을 생성합니다. 그림간 전환을 할 때, pyplot은 현재 활성화된 그림을 계속해서 추적합니다 (이에대한 참조는 plt.gcf()의 호출로 알 수 있습니다). 또한 활성화된 그림의 활성화된 부분 그래프가 현재 그래프가 그려질 부분 그래프가 됩니다. . 이제현 주 : object oriented API에서는 실행 순이 아니라 객체를 중심으로 명령을 실행합니다. 다른 그림을 그리다가 앞서 그림을 추가할 때 pyplot에서 plt.figure() 명령으로 위 그림을 호출하는 대신 object oriented API는 목표 Axes를 지정하여 추가합니다. . x = np.linspace(-1.4, 1.4, 30) plt.figure(1) plt.subplot(211) plt.plot(x, x**2) plt.title(&quot;Square and Cube&quot;) plt.subplot(212) plt.plot(x, x**3) plt.figure(2, figsize=(10, 5)) plt.subplot(121) plt.plot(x, x**4) plt.title(&quot;y = x**4&quot;) plt.subplot(122) plt.plot(x, x**5) plt.title(&quot;y = x**5&quot;) plt.figure(1) # 그림 1로 돌아가며, 활성화된 부분 그래프는 212 (하단)이 됩니다 plt.plot(x, -x**3, &quot;r:&quot;) plt.show() . x = np.linspace(-1.4, 1.4, 30) fig1, ax1 = plt.subplots(nrows=2, ncols=1) ax1[0].plot(x, x**2) ax1[0].set_title(&quot;Square and Cube&quot;) ax1[1].plot(x, x**3) fig2, ax2 = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) ax2[0].plot(x, x**4) ax2[0].set_title(&quot;y = x**4&quot;) ax2[1].plot(x, x**5) ax2[1].set_title(&quot;y = x**5&quot;) ax1[1].plot(x, -x**3, &quot;r:&quot;) # 그림 1로 돌아가며, 활성화된 부분 그래프는 ax1[1] (하단)이 됩니다. plt.show() . Pyplot&#51032; &#49345;&#53468; &#47672;&#49888;: &#50516;&#49884;&#51201; vs &#47749;&#49884;&#51201; . 지금까지 현재의 활성화된 부분 그래프를 추적하는 Pyplot의 상태 머신을 사용했었습니다. plot 함수를 호출할 때마다 pyplot은 단지 현재 활성화된 부분 그래프에 그림을 그립니다. 그리고 plot 함수를 호출 할 때, 그림 및 부분 그래프가 아직 존재하지 않는다면 이들을 만들어내는 마법같은(?) 작업도 일부 수행합니다. 이는 주피터와 같은 대화식의 환경에서 편리합니다. . 그러나 프로그램을 작성하는 것이라면, 명시적인 것이 암시적인것 보다 더 낫습니다. 명시적인 코드는 일반적으로 디버깅과 유지보수가 더 쉽습니다. 이 말에 동의하지 않는다면, Python 젠(Zen)의 두 번째 규칙을 읽어보시기 바랍니다. . import this . The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren&#39;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you&#39;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it&#39;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let&#39;s do more of those! . 아름다움이 추한 것보다 낫다. . 명확함이 함축된 것보다 낫다. . 단순함이 복잡한 것보다 낫다. . 복잡함이 난해한 것보다 낫다. . 단조로움이 중접된 것보다 낫다. . 여유로움이 밀집된 것보다 낫다. . 가독성은 중요하다. . 비록 실용성이 이상을 능가한다 하더라도 규칙을 깨야할 정도로 특별한 경우란 없다. . 알고도 침묵하지 않는 한 오류는 결코 조용히 지나가지 않는다. . 모호함을 마주하고 추측하려는 유혹을 거절하라. 비록 당신이 우둔해서 처음에는 명백해 보이지 않을 수도 있겠지만 문제를 해결할 하나의 - 바람직하고 유일한 - 명백한 방법이 있을 것이다. . 비록 하지않는 것이 지금 하는 것보다 나을 때도 있지만 지금 하는 것이 전혀 안하는 것보다 낫다. . 설명하기 어려운 구현이라면 좋은 아이디어가 아니다. 쉽게 설명할 수 있는 구현이라면 좋은 아이디어일 수 있다. 네임스페이스는 정말 대단한 아이디어다. -- 자주 사용하자! . from 출처 . 다행히도 Pyplot은 상태 머신을 완전히 무시할 수 있게끔 해 줍니다. 따라서 아름다운 명시적 코드를 작성하는것이 가능하죠. 간단히 subplots 함수를 호출해서 반환되는 figure 객체 및 축의 리스트를 사용하면 됩니다*. 마법은 더 이상 없습니다! . 이제현 주:* 여기서 설명하는 부분이 matplotlib의 object oriented API(객체지향 인터페이스)입니다. 아래는 이에 대한 예 입니다: . x = np.linspace(-2, 2, 200) fig1, (ax_top, ax_bottom) = plt.subplots(2, 1, sharex=True) fig1.set_size_inches(10,5) line1, line2 = ax_top.plot(x, np.sin(3*x**2), &quot;r-&quot;, x, np.cos(5*x**2), &quot;b-&quot;) line3, = ax_bottom.plot(x, np.sin(3*x), &quot;r-&quot;) ax_top.grid(True) fig2, ax = plt.subplots(1, 1) ax.plot(x, x**2) plt.show() . 일관성을 위해서 이 튜토리얼의 나머지 부분에서는 pyplot의 상태 머신을 계속해서 사용할 것입니다. 그러나 프로그램에서는 객체지향 인터페이스의 사용을 권장하고 싶습니다. . Pylab vs Pyplot vs Matplotlib . pylab, pyplot, matplotlib 간의 관계에대한 혼동이 있습니다. 그러나 이들의 관계는 매우 단순합니다: matplotlib은 완전한 라이브러리이며, pylab 및 pyplot을 포함한 모든것을 가지고 있습니다. . Pyplot은 그래프를 그리기위한 다양한 도구를 제공합니다. 여기에는 내부적인 객체지향적인 그래프 그리기 라이브러리에 대한 상태 머신 인터페이스도 포함됩니다. . Pylab은 mkatplotlib.pyplot 및 NumPy를 단일 네임스페이스로 임포트하는 편리성을 위한 모듈입니다. 인터넷에 떠도는 pylab을 사용하는 여러 예제를 보게 될 것입니다. 그러나 이는 더이상 권장되는 사용방법은 아닙니다 (왜냐하면 명시적인 임포트가 암시적인것 보다 더 낫기 때문입니다). . 이제현 주 :* Pylab, Pyplot, Object oriented API의 관계는 여기를 참고하십시오 . &#53581;&#49828;&#53944; &#44536;&#47532;&#44592; . text 함수를 호출하여 텍스트를 그래프의 원하는 위치에 추가할 수 있습니다. 출력을 원하는 텍스트와 수평 및 수직 좌표를 지정하고, 추가적으로 몇 가지 속성을 지정해 주기만 하면 됩니다. matplotlib의 모든 텍스트는 TeX 방정식 표현을 포함할 수 있습니다. 더 자세한 내용은 공식 문서를 참조하시기 바랍니다. . x = np.linspace(-1.5, 1.5, 30) px = 0.8 py = px**2 plt.plot(x, x**2, &quot;b-&quot;, px, py, &quot;ro&quot;) plt.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;blue&#39;, horizontalalignment=&quot;center&quot;) plt.text(px - 0.08, py, &quot;Beautiful point&quot;, ha=&quot;right&quot;, weight=&quot;heavy&quot;) plt.text(px, py, &quot;x = %0.2f ny = %0.2f&quot;%(px, py), rotation=50, color=&#39;gray&#39;) plt.show() . fig, ax = plt.subplots() ax.plot(x, x**2, &quot;b-&quot;) ax.plot(px, py, &quot;ro&quot;) ax.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;blue&#39;, horizontalalignment=&quot;center&quot;) ax.text(px - 0.08, py, &quot;Beautiful point&quot;, ha=&quot;right&quot;, weight=&quot;heavy&quot;) ax.text(px, py, &quot;x = %0.2f ny = %0.2f&quot;%(px, py), rotation=50, color=&#39;gray&#39;) plt.show() . 알아둘 것: ha는 horizontalalignment(수평정렬)의 이명 입니다. | . 더 많은 텍스트 속성을 알고 싶다면, 공식 문서를 참조하시기 바랍니다. . 아래 그래프의 &quot;beautiful point&quot; 같은 텍스트 처럼, 그래프의 요소에 주석을 다는것은 꽤 흔한 일입니다. annotate 함수는 이를 쉽게 할 수 있게 해 줍니다: 관심있는 부분의 위치를 지정하고, 텍스트의 위치를 지정합니다. 그리고 텍스트 및 화살표에 대한 추가적인 속성도 지정해줄 수 있습니다. . plt.plot(x, x**2, px, py, &quot;ro&quot;) plt.annotate(&quot;Beautiful point&quot;, xy=(px, py), xytext=(px-1.3,py+0.5), color=&quot;green&quot;, weight=&quot;heavy&quot;, fontsize=14, arrowprops={&quot;facecolor&quot;: &quot;lightgreen&quot;}) plt.show() . fig, ax = plt.subplots() ax.plot(x, x**2, px, py, &quot;ro&quot;) ax.annotate(&quot;Beautiful point&quot;, xy=(px, py), xytext=(px-1.3,py+0.5), color=&quot;green&quot;, weight=&quot;heavy&quot;, fontsize=14, arrowprops={&quot;facecolor&quot;: &quot;lightgreen&quot;}) plt.show() . bbox 속성을 사용하면, 텍스트를 포함하는 사각형을 그려볼 수도 있습니다: . plt.plot(x, x**2, px, py, &quot;ro&quot;) bbox_props = dict(boxstyle=&quot;rarrow,pad=0.3&quot;, ec=&quot;b&quot;, lw=2, fc=&quot;lightblue&quot;) plt.text(px-0.2, py, &quot;Beautiful point&quot;, bbox=bbox_props, ha=&quot;right&quot;) bbox_props = dict(boxstyle=&quot;round4,pad=1,rounding_size=0.2&quot;, ec=&quot;black&quot;, fc=&quot;#EEEEFF&quot;, lw=5) plt.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;black&#39;, ha=&quot;center&quot;, bbox=bbox_props) plt.show() . fig, ax = plt.subplots() ax.plot(x, x**2) ax.plot(px, py, &quot;ro&quot;) bbox_props = dict(boxstyle=&quot;rarrow,pad=0.3&quot;, ec=&quot;b&quot;, lw=2, fc=&quot;lightblue&quot;) ax.text(px-0.2, py, &quot;Beautiful point&quot;, bbox=bbox_props, ha=&quot;right&quot;) bbox_props = dict(boxstyle=&quot;round4,pad=1,rounding_size=0.2&quot;, ec=&quot;black&quot;, fc=&quot;#EEEEFF&quot;, lw=5) ax.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;black&#39;, ha=&quot;center&quot;, bbox=bbox_props) plt.show() . 재미를 위해서 xkcd 스타일의 그래프를 그려보고 싶다면, with plt.xkcd() 섹션 블록을 활용할 수도 있습니다: . with plt.xkcd(): plt.plot(x, x**2, px, py, &quot;ro&quot;) bbox_props = dict(boxstyle=&quot;rarrow,pad=0.3&quot;, ec=&quot;b&quot;, lw=2, fc=&quot;lightblue&quot;) plt.text(px-0.2, py, &quot;Beautiful point&quot;, bbox=bbox_props, ha=&quot;right&quot;) bbox_props = dict(boxstyle=&quot;round4,pad=1,rounding_size=0.2&quot;, ec=&quot;black&quot;, fc=&quot;#EEEEFF&quot;, lw=5) plt.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;black&#39;, ha=&quot;center&quot;, bbox=bbox_props) plt.show() . &#48276;&#47168; (Legends) . 범례를 추가하는 가장 간단한 방법은 모든 선에 라벨을 설정 해 주고, legend 함수를 호출하는 것입니다. . x = np.linspace(-1.4, 1.4, 50) plt.plot(x, x**2, &quot;r--&quot;, label=&quot;Square function&quot;) plt.plot(x, x**3, &quot;g-&quot;, label=&quot;Cube function&quot;) plt.legend(loc=&quot;best&quot;) plt.grid(True) plt.show() . x = np.linspace(-1.4, 1.4, 50) fig, ax = plt.subplots() ax.plot(x, x**2, &quot;r--&quot;, label=&quot;Square function&quot;) ax.plot(x, x**3, &quot;g-&quot;, label=&quot;Cube function&quot;) ax.legend(loc=&quot;best&quot;) ax.grid(True) plt.show() . &#48708;&#49440;&#54805; &#52377;&#46020; . Matplotlib은 로그, 로짓(logit)과 같은 비선형 척도를 지원합니다. . x = np.linspace(0.1, 15, 500) y = x**3/np.exp(2*x) plt.figure(1) plt.plot(x, y) plt.yscale(&#39;linear&#39;) plt.title(&#39;linear&#39;) plt.grid(True) plt.figure(2) plt.plot(x, y) plt.yscale(&#39;log&#39;) plt.title(&#39;log&#39;) plt.grid(True) plt.figure(3) plt.plot(x, y) plt.yscale(&#39;logit&#39;) plt.title(&#39;logit&#39;) plt.grid(True) plt.figure(4) plt.plot(x, y - y.mean()) plt.yscale(&#39;symlog&#39;, linthreshy=0.05) plt.title(&#39;symlog&#39;) plt.grid(True) plt.show() . x = np.linspace(0.1, 15, 500) y = x**3/np.exp(2*x) fig1, ax1 = plt.subplots() ax1.plot(x, y) ax1.set_yscale(&#39;linear&#39;) ax1.set_title(&#39;linear&#39;) ax1.grid(True) fig2, ax2 = plt.subplots() ax2.plot(x, y) ax2.set_yscale(&#39;log&#39;) ax2.set_title(&#39;log&#39;) ax2.grid(True) fig3, ax3 = plt.subplots() ax3.plot(x, y) ax3.set_yscale(&#39;logit&#39;) ax3.set_title(&#39;logit&#39;) ax3.grid(True) fig4, ax4 = plt.subplots() ax4.plot(x, y - y.mean()) ax4.set_yscale(&#39;symlog&#39;, linthreshy=0.05) ax4.set_title(&#39;symlog&#39;) ax4.grid(True) plt.show() . &#54001;&#44284; &#54001;&#52964; (Ticks and tickers) . 각 축에는 &quot;틱(ticks)&quot;이라는 작은 표시가 있습니다. 정확히 말하자면, &quot;틱&quot;은 표시(예. (-1, 0, 1))의 위치&quot;이며, 틱 선은 그 위치에 그려지는 작은 선입니다. 또한 &quot;틱 라벨&quot;은 틱 선 옆에 그려지는 라벨이며, &quot;틱커&quot;는 틱의 위치를 결정하는 객체 입니다. 기본적인 틱커는 ~5 에서 8 틱을 위치시키는데 꽤 잘 작동합니다. 즉, 틱 서로간에 적당한 거리를 표현합니다. . 하지만, 가끔은 좀 더 이를 제어할 필요가 있습니다 (예. 위의 로짓 그래프에서는 너무 많은 틱 라벨이 있습니다). 다행히도 matplotlib은 틱을 완전히 제어하는 방법을 제공합니다. 심지어 보조 눈금(minor tick)을 활성화 할 수도 있습니다. . # 이제현 주: 사실상 object oriented API 입니다. x = np.linspace(-2, 2, 100) plt.figure(1, figsize=(15,10)) plt.subplot(131) plt.plot(x, x**3) plt.grid(True) plt.title(&quot;Default ticks&quot;) ax = plt.subplot(132) plt.plot(x, x**3) ax.xaxis.set_ticks(np.arange(-2, 2, 1)) plt.grid(True) plt.title(&quot;Manual ticks on the x-axis&quot;) ax = plt.subplot(133) plt.plot(x, x**3) plt.minorticks_on() ax.tick_params(axis=&#39;x&#39;, which=&#39;minor&#39;, bottom=&#39;off&#39;) ax.xaxis.set_ticks([-2, 0, 1, 2]) ax.yaxis.set_ticks(np.arange(-5, 5, 1)) ax.yaxis.set_ticklabels([&quot;min&quot;, -4, -3, -2, -1, 0, 1, 2, 3, &quot;max&quot;]) plt.title(&quot;Manual ticks and tick labels n(plus minor ticks) on the y-axis&quot;) plt.grid(True) plt.show() . # 위 pyplot 예제는 사실상 object oriented API 입니다. # 여기에서는 같은 기능을 더 단순한 코드로 구현하였습니다 x = np.linspace(-2, 2, 100) fig, ax = plt.subplots(ncols=3, figsize=(15, 10)) ax[0].plot(x, x**3) ax[0].grid(True) ax[0].set_title(&quot;Default ticks&quot;) ax[1].plot(x, x**3) ax[1].grid(True) ax[1].set_xticks(np.arange(-2, 2, 1)) ax[1].set_title(&quot;Manual ticks on the x-axis&quot;) ax[2].plot(x, x**3) ax[2].grid(True) ax[2].minorticks_on() ax[2].set_xticks([-2, 0, 1, 2], minor=False) ax[2].set_yticks(np.arange(-5, 5, 1)) ax[2].set_yticklabels([&quot;min&quot;, -4, -3, -2, -1, 0, 1, 2, 3, &quot;max&quot;]) ax[2].set_title(&quot;Manual ticks and tick labels n(plus minor ticks) on the y-axis&quot;) plt.show() . &#44537;&#51340;&#54364;&#44228;&#51032; &#53804;&#50689; (Polar projection) . 극좌표계 그래프를 그리는 것은 매우 간단합니다. 부분 그래프를 생성할 때 projection 속성을 &quot;polar&quot;로 설정해 주기만 하면 됩니다. . 이제현 주: object oriented API는 일반적으로 plt.subplots()로 Figure와 Axes 객체를 동시에 생성합니다. plt.subplots()는 projection 속성을 가지고 있지 않습니다. . 따라서 projection을 사용할 때는 plt.figure()로 Figure 객체를 먼저 생성한 후 plt.subplot()이나 plt.add_subplot()으로 Axes 객체를 추가해 주거나, fig.subplots() 안에 subplot_kw=={&#39;polar&#39;:True}로 지정해 주어야 합니다. | . radius = 1 theta = np.linspace(0, 2*np.pi*radius, 1000) plt.subplot(111, projection=&#39;polar&#39;) plt.plot(theta, np.sin(5*theta), &quot;g-&quot;) plt.plot(theta, 0.5*np.cos(20*theta), &quot;b-&quot;) plt.show() . radius = 1 theta = np.linspace(0, 2*np.pi*radius, 1000) fig = plt.figure() ax = fig.add_subplot(projection=&#39;polar&#39;) # 또는, subplot_kw 를 이용해서 polar plot으로 설정합니다. # fig, ax = plt.subplots(subplot_kw={&#39;polar&#39;:True}) ax.plot(theta, np.sin(5*theta), &quot;g-&quot;) ax.plot(theta, 0.5*np.cos(20*theta), &quot;b-&quot;) plt.show() . 3&#52264;&#50896; &#53804;&#50689; . 3차원 그래프를 그리는것은 꽤 간단합니다. 우선 &quot;3d&quot; 투영을 등록하는 Axes3D를 임포트 해줘야 합니다. 그리곤 projection 속성을 &quot;3d&quot;로 설정된 부분 그래프 생성합니다. 그러면 Axes3DSubplot 이라는 객체가 반환되는데, 이 객체의 plot_surface 메서드를 호출하면 x, y, z 좌표를 포함한 추가적이나 속성을 지정할 수 있습니다. . # 이제현 주: 사실상 object oriented API 입니다. from mpl_toolkits.mplot3d import Axes3D x = np.linspace(-5, 5, 50) y = np.linspace(-5, 5, 50) X, Y = np.meshgrid(x, y) R = np.sqrt(X**2 + Y**2) Z = np.sin(R) figure = plt.figure(1, figsize = (12, 4)) subplot3d = plt.subplot(111, projection=&#39;3d&#39;) # 이제현 주: Axes 객체입니다. surface = subplot3d.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, linewidth=0.1) plt.show() . 동일한 데이터를 출력하는 또 다른 방법은 등고선도(contour plot)를 이용하는 것입니다. . plt.contourf(X, Y, Z, cmap=matplotlib.cm.coolwarm) plt.colorbar() plt.show() . # 이제현 주: 종종 object oriented API가 pyplot보다 불편할 때가 있습니다. # contour plot의 colorbar는 무엇을 대상으로 할 지를 인자로 전달해야 합니다. fig, ax = plt.subplots() contour = ax.contourf(X, Y, Z, cmap=matplotlib.cm.coolwarm) plt.colorbar(contour) plt.show() . &#49328;&#51216;&#46020;(Scatter plot) . 단순히 각 점에 대한 x 및 y 좌표를 제공하면 산점도를 그릴 수 있습니다. . from numpy.random import rand x, y = rand(2, 100) plt.scatter(x, y) plt.show() . from numpy.random import rand x, y = rand(2, 100) fig, ax = plt.subplots() ax.scatter(x, y) plt.show() . 부수적으로 각 점의 크기를 정할 수도 있습니다. . x, y, scale = rand(3, 100) scale = 500 * scale ** 5 plt.scatter(x, y, s=scale) plt.show() . x, y, scale = rand(3, 100) scale = 500 * scale ** 5 fig, ax = plt.subplots() ax.scatter(x, y, s=scale) plt.show() . 마찬가지로 여러 속성을 설정할 수 있습니다. 가령 테두리 및 모양의 내부 색상, 그리고 투명도와 같은것의 설정이 가능합니다. . for color in [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]: n = 100 x, y = rand(2, n) scale = 500.0 * rand(n) ** 5 plt.scatter(x, y, s=scale, c=color, alpha=0.3, edgecolors=&#39;blue&#39;) plt.grid(True) plt.show() . fig, ax = plt.subplots() for color in [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]: n = 100 x, y = rand(2, n) scale = 500.0 * rand(n) ** 5 ax.scatter(x, y, s=scale, c=color, alpha=0.3, edgecolors=&#39;blue&#39;) ax.grid(True) plt.show() . &#49440; . 지금까지 해온것 처럼 plot 함수를 사용하여 선을 그릴 수 있습니다. 하지만, 가끔은 그래프를 통과하는 무한한 선을 그리는 유틸리티 함수를 만들면 편리합니다 (기울기와 절편으로). 또한 hlines 및 vlines 함수를 사용하면, 아래와 같이 부분 수평 및 수직 선을 그릴 수도 있습니다: . from numpy.random import randn def plot_line(axis, slope, intercept, **kargs): xmin, xmax = axis.get_xlim() plt.plot([xmin, xmax], [xmin*slope+intercept, xmax*slope+intercept], **kargs) x = randn(1000) y = 0.5*x + 5 + randn(1000)*2 plt.axis([-2.5, 2.5, -5, 15]) plt.scatter(x, y, alpha=0.2) plt.plot(1, 0, &quot;ro&quot;) plt.vlines(1, -5, 0, color=&quot;red&quot;) plt.hlines(0, -2.5, 1, color=&quot;red&quot;) plot_line(axis=plt.gca(), slope=0.5, intercept=5, color=&quot;magenta&quot;) plt.grid(True) plt.show() . from numpy.random import randn # Axis를 인자로 전달하여 함수 연산과 시각화를 수행합니다. def plot_line(axis, slope, intercept, **kargs): xmin, xmax = axis.get_xlim() axis.plot([xmin, xmax], [xmin*slope+intercept, xmax*slope+intercept], **kargs) x = randn(1000) y = 0.5*x + 5 + randn(1000)*2 fig, ax = plt.subplots() ax.set_xlim(-2.5, 2.5) ax.set_ylim(-5, 15) ax.scatter(x, y, alpha=0.2) ax.plot(1, 0, &quot;ro&quot;) ax.vlines(1, -5, 0, color=&quot;red&quot;) ax.hlines(0, -2.5, 1, color=&quot;red&quot;) plot_line(axis=ax, slope=0.5, intercept=5, color=&quot;magenta&quot;) ax.grid(True) plt.show() . &#55176;&#49828;&#53664;&#44536;&#47016; . data = [1, 1.1, 1.8, 2, 2.1, 3.2, 3, 3, 3, 3] plt.subplot(211) plt.hist(data, bins = 10, rwidth=0.8) plt.subplot(212) plt.hist(data, bins = [1, 1.5, 2, 2.5, 3], rwidth=0.95) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Frequency&quot;) plt.show() . data = [1, 1.1, 1.8, 2, 2.1, 3.2, 3, 3, 3, 3] fig, ax = plt.subplots(2, 1) ax[0].hist(data, bins = 10, rwidth=0.8) ax[1].hist(data, bins = [1, 1.5, 2, 2.5, 3], rwidth=0.95) ax[1].set_xlabel(&quot;Value&quot;) ax[1].set_ylabel(&quot;Frequency&quot;) plt.show() . data1 = np.random.randn(400) data2 = np.random.randn(500) + 3 data3 = np.random.randn(450) + 6 data4a = np.random.randn(200) + 9 data4b = np.random.randn(100) + 10 plt.hist(data1, bins=5, color=&#39;g&#39;, alpha=0.75, label=&#39;bar hist&#39;) # default histtype=&#39;bar&#39; plt.hist(data2, color=&#39;b&#39;, alpha=0.65, histtype=&#39;stepfilled&#39;, label=&#39;stepfilled hist&#39;) plt.hist(data3, color=&#39;r&#39;, histtype=&#39;step&#39;, label=&#39;step hist&#39;) plt.hist((data4a, data4b), color=(&#39;r&#39;,&#39;m&#39;), alpha=0.55, histtype=&#39;barstacked&#39;, label=(&#39;barstacked a&#39;, &#39;barstacked b&#39;)) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Frequency&quot;) plt.legend() plt.grid(True) plt.show() . c: Users User anaconda3 lib site-packages numpy core fromnumeric.py:3245: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray. return asarray(a).size c: Users User anaconda3 lib site-packages matplotlib cbook __init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray. X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X)) . data1 = np.random.randn(400) data2 = np.random.randn(500) + 3 data3 = np.random.randn(450) + 6 data4a = np.random.randn(200) + 9 data4b = np.random.randn(100) + 10 fig, ax = plt.subplots() ax.hist(data1, bins=5, color=&#39;g&#39;, alpha=0.75, label=&#39;bar hist&#39;) # default histtype=&#39;bar&#39; ax.hist(data2, color=&#39;b&#39;, alpha=0.65, histtype=&#39;stepfilled&#39;, label=&#39;stepfilled hist&#39;) ax.hist(data3, color=&#39;r&#39;, histtype=&#39;step&#39;, label=&#39;step hist&#39;) ax.hist((data4a, data4b), color=(&#39;r&#39;,&#39;m&#39;), alpha=0.55, histtype=&#39;barstacked&#39;, label=(&#39;barstacked a&#39;, &#39;barstacked b&#39;)) ax.set_xlabel(&quot;Value&quot;) ax.set_ylabel(&quot;Frequency&quot;) ax.legend() ax.grid(True) plt.show() . c: Users User anaconda3 lib site-packages numpy core fromnumeric.py:3245: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray. return asarray(a).size c: Users User anaconda3 lib site-packages matplotlib cbook __init__.py:1376: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray. X = np.atleast_1d(X.T if isinstance(X, np.ndarray) else np.asarray(X)) . &#51060;&#48120;&#51648; . matplotlib에서의 이미지 불러오기, 생성하기, 화면에 그리기는 꽤 간단합니다. . 이미지를 불러오려면 matplotlib.image 모듈을 임포트하고, 파일이름을 지정한 imread 함수를 호출해 주면 됩니다. 그러면 이미지 데이터가 NumPy의 배열로서 반환됩니다. 앞서 저장했던 my_square_function.png 이미지에 대하여 이를 수행해 보겠습니다. . 이제현 주 : 이미지 단독 출력은 pyplot과 object oriented API 사이에 별 차이가 없습니다. Axes를 지정해서 출력하는 것이 다를 뿐입니다. . pyplot과의 중복성이 강하지만 익숙해지는 차원에서 object oriented API를 함께 도시합니다. | . import matplotlib.image as mpimg img = mpimg.imread(&#39;my_square_function.png&#39;) print(img.shape, img.dtype) . (288, 432, 4) float32 . 288x432 크기의 이미지를 불러왔습니다. 각 픽셀은 0~1 사이의 32비트 부동소수 값인 4개의 요소(빨강, 초록, 파랑, 투명도)로 구성된 배열로 표현됩니다. 이번에는 imshow함수를 호출해 보겠습니다: . plt.imshow(img) plt.show() . fig, ax = plt.subplots() ax.imshow(img) plt.show() . 허허허... 이미지 출력에 포함된 축을 숨기고 싶다면 아래와 같이 축을 off 시켜줄 수 있습니다: . plt.imshow(img) plt.axis(&#39;off&#39;) plt.show() . fig, ax = plt.subplots() ax.imshow(img) ax.axis(&#39;off&#39;) plt.show() . 여러분만의 이미지를 생성하는것도 마찬가지로 간단합니다: . img = np.arange(100*100).reshape(100, 100) print(img) plt.imshow(img) plt.show() . [[ 0 1 2 ... 97 98 99] [ 100 101 102 ... 197 198 199] [ 200 201 202 ... 297 298 299] ... [9700 9701 9702 ... 9797 9798 9799] [9800 9801 9802 ... 9897 9898 9899] [9900 9901 9902 ... 9997 9998 9999]] . img = np.arange(100*100).reshape(100, 100) print(img) fig, ax = plt.subplots() ax.imshow(img) plt.show() . [[ 0 1 2 ... 97 98 99] [ 100 101 102 ... 197 198 199] [ 200 201 202 ... 297 298 299] ... [9700 9701 9702 ... 9797 9798 9799] [9800 9801 9802 ... 9897 9898 9899] [9900 9901 9902 ... 9997 9998 9999]] . RGB 수준을 제공하지 않는다면, imshow 함수는 자동으로 값을 색그래디언트에 매핑합니다. 기본적인 동작에서의 색그래디언트는 파랑(낮은 값) 에서 빨강(높은 값)으로 움직입니다. 하지만 아래와 같이 다른 색상맵을 선택할 수도 있습니다: . plt.imshow(img, cmap=&quot;hot&quot;) plt.show() . fig, ax = plt.subplots() ax.imshow(img, cmap=&quot;hot&quot;) plt.show() . RGB 이미지를 직접적으로 생성하는것 또한 가능합니다: . img = np.empty((20,30,3)) img[:, :10] = [0, 0, 0.6] img[:, 10:20] = [1, 1, 1] img[:, 20:] = [0.6, 0, 0] plt.imshow(img, interpolation=&#39;bilinear&#39;) plt.show() . img = np.empty((20,30,3)) img[:, :10] = [0, 0, 0.6] img[:, 10:20] = [1, 1, 1] img[:, 20:] = [0.6, 0, 0] fig, ax = plt.subplots() ax.imshow(img, interpolation=&#39;bilinear&#39;) plt.show() . img 배열이 매우 작기 때문에 (20x30), imshow 함수는 이미지를 figure 크기에 맞도록 늘려버린채 출력합니다. 이러한 늘리기의 기본 동작은 쌍선형 보간법(bilinear interpolation)을 사용하여 추가된 픽셀을 매꿉니다. 테두리가 흐릿한 이유입니다. . 다른 보간법 알고리즘을 선택할 수도 있습니다. 가령 아래와 같이 근접 픽셀을 복사하는 방법이 있습니다: . 이제현 주 : 위 코드의 ax.imshow(img, interpolation=&#39;bilinear&#39;) 부분은 원문에서 ax.imshow(img)로 되어 있습니다. matplotlib 2.0 이전에는 interpolation=&#39;bilinear&#39;가 기본값이기 때문에 경계선이 흐려지는 문제가 있었습니다. . 그러나 이후 interpolation=&#39;nearest&#39;로 기본값이 변경되어 흐려지는 문제가 더 이상 발생하지 않습니다. | 자세한 사항은 이 글을 참고하십시오. | . plt.imshow(img, interpolation=&quot;nearest&quot;) plt.show() . fig, ax = plt.subplots() ax.imshow(img, interpolation=&quot;nearest&quot;) plt.show() . &#50528;&#45768;&#47700;&#51060;&#49496; . matplotlib은 이미지 생성에 주로 사용되지만, 애니메이션의 출력도 가능합니다. 우선 matplotlib.animation을 임포트 해 줘야 합니다. 그 다음은 (주피터 노트북에서) nbagg를 백엔드로 설정하거나, 아래의 코드를 실행해 주면 됩니다. . import matplotlib.animation as animation matplotlib.rc(&#39;animation&#39;, html=&#39;jshtml&#39;) . 다음의 예는 데이터를 생성하는것으로 시작됩니다. 그 다음, 빈 그래프를 생성하고, 애니메이션을 그릴 매 프레임 마다 호출될 갱신(update) 함수를 정의합니다. 마지막으로, FuncAnimation 인스턴스를 생성하여 그래프에 애니메이션을 추가합니다. . FuncAnimation 생성자는 figure, 갱신 함수, 그 외의 파라미터를 수용합니다. 각 프레임간 20ms의 시간차가 있는 100개의 프레임으로 구성된 애니메이션에 대한 인스턴스를 만들었습니다. 애니메이션의 각 프레임마다 FuncAnimation 는 갱신 함수를 호출하고, 프레임 번호를 num (이 예에서는 0~99의 범위) 으로서 전달해 줍니다. 또한 갱신 함수의 추가적인 두 파라미터는 FuncAnimation 생성시 fargs에 넣어준 값이 됩니다. . 작성한 갱신 함수는 선을 구성하는 데이터를 0 ~ num 데이터로 설정합니다 (따라서 데이터가 점진적으로 그려집니다). 그리고 약간의 재미 요소를 위해서, 각 데이터에 약간의 무작위 수를 추가하여 선이 씰룩씰룩 움직이게끔 해 주었습니다. . x = np.linspace(-1, 1, 100) y = np.sin(x**2*25) data = np.array([x, y]) fig = plt.figure() line, = plt.plot([], [], &quot;r-&quot;) # start with an empty plot plt.axis([-1.1, 1.1, -1.1, 1.1]) plt.plot([-0.5, 0.5], [0, 0], &quot;b-&quot;, [0, 0], [-0.5, 0.5], &quot;b-&quot;, 0, 0, &quot;ro&quot;) plt.grid(True) plt.title(&quot;Marvelous animation&quot;) # this function will be called at every iteration def update_line(num, data, line): line.set_data(data[..., :num] + np.random.rand(2, num) / 25) # we only plot the first `num` data points. return line, line_ani = animation.FuncAnimation(fig, update_line, frames=100, fargs=(data, line), interval=67) plt.close() line_ani . &lt;/input&gt; Once Loop Reflect x = np.linspace(-1, 1, 100) y = np.sin(x**2*25) data = np.array([x, y]) fig, ax = plt.subplots() line, = ax.plot([], [], &quot;r-&quot;) # start with an empty plot ax.set_xlim(-1.1, 1.1) ax.set_ylim(-1.1, 1.1) ax.plot([-0.5, 0.5], [0, 0], &quot;b-&quot;, [0, 0], [-0.5, 0.5], &quot;b-&quot;, 0, 0, &quot;ro&quot;) ax.grid(True) ax.set_title(&quot;Marvelous animation&quot;) # this function will be called at every iteration def update_line(num, data, line): line.set_data(data[..., :num] + np.random.rand(2, num) / 25) # we only plot the first `num` data points. return line, line_ani = animation.FuncAnimation(fig, update_line, frames=100, fargs=(data, line), interval=67) plt.close() line_ani . &lt;/input&gt; Once Loop Reflect &#50528;&#45768;&#47700;&#51060;&#49496;&#51012; &#48708;&#46356;&#50724;&#47196; &#51200;&#51109; . 비디오로 저장하기 위해서 Matplotlib은 써드파티 라이브러리(FFMPEG 또는 ImageMagick에 의존합니다. 다음의 예는 FFMPEG를 사용하기 때문에, 이 라이브러리가 먼저 설치되어 있어야만 합니다. 애니메이션을 GIF로 저장하고 싶다면 ImageMagick이 필요할 것입니다. . Writer = animation.writers[&#39;ffmpeg&#39;] writer = Writer(fps=15, metadata=dict(artist=&#39;Me&#39;), bitrate=1800) line_ani.save(&#39;my_wiggly_animation.mp4&#39;, writer=writer) . &#45796;&#51020;&#51008; &#47924;&#50631;&#51012; &#54644;&#50556;&#54624;&#44620;? . 이제 matplotlib의 모든 기본을 습득하셨습니다. 하지만, 그 외에도 수 많은 옵션이 있습니다. 이를 배우기위한 가장 좋은 방법은 갤러리 사이트를 방문하여 흥미로운 그래프를 골라본 다음, 코드를 주피터 노트북에 복사하고 이것저것 가지고 놀아보는 것입니다. . Visualization With Seaborn . seaborn은 matplotlib 기반 파이썬 데이터 시각화 라이브러리이다. | 그것은 매력적이고 유익한 통계 그래픽을 그리기 위한 높은 수준의 인터페이스를 제공한다. 플롯 스타일 및 색상 기본값에 대한 선택권을 제공하고, 일반적인 통계 플롯 유형에 대한 간단한 고급 함수를 정의하며, 팬더 데이터 프레임에서 제공하는 기능과 통합됩니다. | Seaborn의 주요 아이디어는 통계 데이터 탐색 및 일부 통계 모델 적합에 유용한 다양한 플롯 유형을 생성하기 위한 높은 수준의 명령을 제공한다는 것입니다. ### Table of Contents | . Creating basic plots Line Chart | Bar Chart | Histogram | Box plot | Violin plot | Scatter plot | Hue semantic | Bubble plot | Pie Chart | . | Advance Categorical plots in Seaborn | Density plots | Pair plots | import seaborn as sns sns.set() sns.set(style=&quot;darkgrid&quot;) import numpy as np import pandas as pd # importing matplotlib import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&quot;ignore&quot;) plt.rcParams[&#39;figure.figsize&#39;]=(10,10) . In this notebook we will use the Big Mart Sales Data. You can download the data from : https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/download/train-file . Loading dataset . data_BM = pd.read_csv(&#39;data/bigmart_data.csv&#39;) # drop the null values data_BM = data_BM.dropna(how=&quot;any&quot;) # multiply Item_Visibility by 100 to increase size data_BM[&quot;Visibility_Scaled&quot;] = data_BM[&quot;Item_Visibility&quot;] * 100 # view the top results data_BM.head() . Item_Identifier Item_Weight Item_Fat_Content Item_Visibility Item_Type Item_MRP Outlet_Identifier Outlet_Establishment_Year Outlet_Size Outlet_Location_Type Outlet_Type Item_Outlet_Sales Visibility_Scaled . 0 FDA15 | 9.300 | Low Fat | 0.016047 | Dairy | 249.8092 | OUT049 | 1999 | Medium | Tier 1 | Supermarket Type1 | 3735.1380 | 1.604730 | . 1 DRC01 | 5.920 | Regular | 0.019278 | Soft Drinks | 48.2692 | OUT018 | 2009 | Medium | Tier 3 | Supermarket Type2 | 443.4228 | 1.927822 | . 2 FDN15 | 17.500 | Low Fat | 0.016760 | Meat | 141.6180 | OUT049 | 1999 | Medium | Tier 1 | Supermarket Type1 | 2097.2700 | 1.676007 | . 4 NCD19 | 8.930 | Low Fat | 0.000000 | Household | 53.8614 | OUT013 | 1987 | High | Tier 3 | Supermarket Type1 | 994.7052 | 0.000000 | . 5 FDP36 | 10.395 | Regular | 0.000000 | Baking Goods | 51.4008 | OUT018 | 2009 | Medium | Tier 3 | Supermarket Type2 | 556.6088 | 0.000000 | . 1. Creating basic plots . Let&#39;s have a look on how can you create some basic plots in seaborn in a single line for which multiple lines were required in matplotlib. . &#49440; &#44536;&#47000;&#54532; (Line Plot) . 일부 데이터 집합을 사용하면 한 변수의 변화를 시간 함수로 이해하거나 유사한 연속형 변수로 이해할 수 있습니다. | Seaborn에서는 kind=&quot;line&quot;:을 설정하여 lineplot() 함수를 직접 또는 relplot()로 수행할 수 있습니다. | . sns.lineplot(x=&quot;Item_Weight&quot;, y=&quot;Item_MRP&quot;,data=data_BM[:50]); . &#47561;&#45824; &#44536;&#47000;&#54532; (Bar Chart) . Seaborn에서는 barplot 함수를 사용하여 막대 차트를 만들 수 있습니다. | matplotlib에서 동일한 것을 달성하기 위해 데이터 범주를 그룹화하기 위해 추가 코드를 작성해야 했습니다. | 그리고 우리는 플롯이 정확하게 나오는지 확인하기 위해 훨씬 더 많은 코드를 작성해야 했습니다. | . sns.barplot(x=&quot;Item_Type&quot;, y=&quot;Item_MRP&quot;, data=data_BM[:5]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x203c2ae0a30&gt; . &#55176;&#49828;&#53664;&#44536;&#47016; (Histogram) . distplot()를 사용하여 Seaborn에 히스토그램을 만들 수 있습니다. 노트북에서 자세히 볼 수 있는 여러 가지 옵션이 있습니다. | . sns.distplot(data_BM[&#39;Item_MRP&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x203c2c9db50&gt; . &#49345;&#51088; &#44536;&#47548; (Box Plot) . boxplot()를 사용하여 Seaborn에서 상자 그림을 만들 수 있습니다. | Item_의 분포를 시각화해 봅시다.Item_Outlet_Sales of items. | . sns.boxplot(data_BM[&#39;Item_Outlet_Sales&#39;], orient=&#39;vertical&#39;) # orient : 플롯의 방향(수직 또는 수평) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x203c3454df0&gt; . &#48148;&#51060;&#50732;&#47536; &#54540;&#47215; (Violin Plot) . 바이올린 플롯은 박스와 수염 줄거리와 비슷한 역할을 한다. | 분포가 비교될 수 있도록 하나 이상의 범주형 변수의 여러 수준에 걸친 정량적 데이터의 분포를 보여 줍니다. | 모든 플롯 구성 요소가 실제 데이터 포인트에 대응하는 박스 플롯과는 달리, 바이올린 플롯은 기본 분포의 커널 밀도 추정을 특징으로 한다. | Seaborn의 violin 플롯()을 사용하여 바이올린 플롯을 만들 수 있습니다. | . sns.violinplot(data_BM[&#39;Item_Outlet_Sales&#39;], orient=&#39;vertical&#39;, color=&#39;magenta&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x203c324eac0&gt; . &#49328;&#51216;&#46020; . 이것은 점의 구름을 사용하여 두 변수의 분포를 나타내며, 여기서 각 점은 데이터 집합의 관측치를 나타낸다. | 이 묘사를 통해 눈은 그들 사이에 의미 있는 관계가 있는지 여부에 대한 상당한 양의 정보를 추론할 수 있다. | relplot**을 kind=contain 옵션과 함께 사용하여 Seaborn에 산점도를 그릴 수 있습니다. | . 참고: 여기서는 그래프에 데이터의 부분 집합만 사용합니다. . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, data=data_BM[:200], kind=&quot;scatter&quot;); . hue semantic . 세 번째 변수에 따라 점을 색칠하여 그림에 다른 차원을 추가할 수도 있습니다. 시본에서는 이를 hue semantic을 사용한다고 한다. . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, hue=&quot;Item_Type&quot;,data=data_BM[:200]); # hue : 색상 . 이전에 만든 line chart를 기억하십니까? hue semantic을 사용하면 Seaborn에 더 복잡한 선 그림을 만들 수 있다. | 다음 예제에서는 Outlet_Size의 서로 다른 범주의 선 그림이 만들어집니다. | . sns.lineplot(x=&quot;Item_Weight&quot;, y=&quot;Item_MRP&quot;,hue=&#39;Outlet_Size&#39;,data=data_BM[:150]); . Bubble plot . 우리는 Item_Visibility에 따라 거품을 색칠하기 위해 hue semantic을 활용하고 동시에 개별 거품의 크기로 사용한다. | . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, data=data_BM[:200], kind=&quot;scatter&quot;, size=&quot;Visibility_Scaled&quot;, hue=&quot;Visibility_Scaled&quot;); . &#48276;&#51452;&#48324; &#54616;&#50948; &#44536;&#47548; . 또한 Seaborn에서 범주에 기반한 그림을 만들 수 있습니다. | 각 Outlet_Size에 대한 산점도를 만들었습니다. | . sns.relplot(x=&quot;Item_Weight&quot;, y=&quot;Item_Visibility&quot;,hue=&#39;Outlet_Size&#39;,style=&#39;Outlet_Size&#39;,col=&#39;Outlet_Size&#39;,data=data_BM[:100]); . 2. Advance categorical plots in seaborn . 범주형 변수의 경우 Seaborn에는 세 개의 서로 다른 과가 있습니다. . Categorical scatterplots: . stripplot() (with kind=&quot;strip&quot;; the default) | swarmplot() (with kind=&quot;swarm&quot;) | . | Categorical distribution plots: . boxplot() (with kind=&quot;box&quot;) | violinplot() (with kind=&quot;violin&quot;) | boxenplot() (with kind=&quot;boxen&quot;) | . | Categorical estimate plots: . pointplot() (with kind=&quot;point&quot;) | barplot() (with kind=&quot;bar&quot;) | . | . catplot()의 기본 데이터 표현에서는 산점도를 사용합니다. . a. &#48276;&#51452;&#54805; &#49328;&#51216;&#46020; . &#49828;&#53944;&#47549; &#54540;&#47215; . 하나의 변수가 범주형인 산점도를 그립니다. | &#39;catplot&#39;에서 kind=strip을 전달하여 만들 수 있습니다. | . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;, kind=&#39;strip&#39;,data=data_BM[:250]); . &#44400;&#51665;&#46020; . 이 함수는 &#39;스트립플롯()과 유사하지만 점이 겹치지 않도록 (범주 축을 따라) 조정됩니다. | 이렇게 하면 값 분포가 더 잘 표현되지만 많은 수의 관측치로 확장되지는 않습니다. 이런 식의 줄거리는 때때로 &quot;벌의 따뜻함&quot;이라고 불린다. | &#39;catplot&#39;에서 kind=snot를 전달하여 만들 수 있습니다. | . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;, kind=&#39;swarm&#39;,data=data_BM[:250]); . b. &#48276;&#51452;&#54805; &#48516;&#54252;&#46020; . &#49345;&#51088; &#44536;&#47548; . 상자 그림은 분포의 세 사분위수 값과 극단값을 표시합니다. | &quot;whiskers&quot;은 하위 사분위수와 상위 사분위수의 1.5 IQR 내에 있는 점까지 확장되며, 이 범위를 벗어나는 관측치가 독립적으로 표시됩니다. | 즉, 상자 그림의 각 값은 데이터의 실제 관측치에 해당합니다. | . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;box&quot;,data=data_BM); . Violin Plots . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;violin&quot;,data=data_BM); . Boxen plots . 이 유형의 그림은 &quot;문자 값&quot;으로 정의된 많은 수의 분위수를 표시하기 때문에 원래 &quot;문자 값&quot; 그림으로 명명되었습니다. | 모든 형상이 실제 관측치에 해당하는 분포의 비모수 표현을 표시하는 상자 그림과 유사합니다. | 더 많은 분위수를 표시함으로써 분포의 형태, 특히 꼬리에 대한 더 많은 정보를 제공합니다. | . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;boxen&quot;,data=data_BM); . Point plot . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;point&quot;,data=data_BM); . Bar plots . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;bar&quot;,data=data_BM); . 3. &#48128;&#46020; &#44536;&#47548; . 히스토그램 대신, 우리는 Seborn이 sns.kdeplot을 사용하여 수행하는 커널 밀도 추정을 사용하여 분포의 원활한 추정치를 얻을 수 있습니다. . plt.figure(figsize=(10,10)) sns.kdeplot(data_BM[&#39;Item_Visibility&#39;], shade=True); . plt.figure(figsize=(10,10)) sns.kdeplot(data_BM[&#39;Item_MRP&#39;], shade=True); . Histogram and Density Plot . 히스토그램과 KDE는 다음과 같이 distplot을 사용하여 결합할 수 있습니다. . plt.figure(figsize=(10,10)) sns.distplot(data_BM[&#39;Item_Outlet_Sales&#39;]); . 4. Pair plots . 결합 그림을 더 큰 차원의 데이터 집합으로 일반화하면 쌍 그림이 됩니다. 이 기능은 모든 값 쌍을 서로에 대해 표시할 때 다차원 데이터 간의 상관 관계를 탐색하는 데 매우 유용합니다. . | 잘 알려진 iris 데이터 세트를 사용하여 이를 시연할 것입니다. 이 데이터 세 가지 iris 종의 꽃잎과 꽃받침의 측정값을 나열합니다. . | . iris = sns.load_dataset(&quot;iris&quot;) iris.head() . sepal_length sepal_width petal_length petal_width species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . 샘플 간의 다차원 관계를 시각화하는 것은 sns.pairplot을 호출하는 것만큼 쉽습니다. . sns.pairplot(iris, hue=&#39;species&#39;, height=2.5); . seaborn and matplotlib . 코드 출처 - 이제현님 블로그 . https://jehyunlee.github.io/2020/09/30/Python-DS-34-seaborn_matplotlib/ | . 1.1 Load data . 예제로 사용할 펭귄 데이터를 불러옵니다. | seaborn에 내장되어 있습니다. | . import pandas as pd import matplotlib.pyplot as plt import seaborn as sns penguins = sns.load_dataset(&quot;penguins&quot;) penguins.head() . species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex . 0 Adelie | Torgersen | 39.1 | 18.7 | 181.0 | 3750.0 | Male | . 1 Adelie | Torgersen | 39.5 | 17.4 | 186.0 | 3800.0 | Female | . 2 Adelie | Torgersen | 40.3 | 18.0 | 195.0 | 3250.0 | Female | . 3 Adelie | Torgersen | NaN | NaN | NaN | NaN | NaN | . 4 Adelie | Torgersen | 36.7 | 19.3 | 193.0 | 3450.0 | Female | . 1.2 Figure and Axes . matplotlib으로 도화지figure를 깔고 축공간 axes를 만듭니다. | 1 x 2 축공간을 구성합니다. | . fig, axes = plt.subplots(ncols=2, figsize=(8,4)) fig.tight_layout() . 1.3 plot with matplotlib . matplotlib 기능을 이용해서 산점도를 그립니다. x축은 부리 길이 bill length | y축은 부리 위 아래 두께 bill depth | 색상은 종species로 합니다. | Adelie, Chinstrap, Gentoo이 있습니다. | . | 두 축공간 중 왼쪽에만 그립니다. | . 컬러를 다르게 주기 위해 f-string 포맷을 사용했습니다. f-string 포맷에 대한 설명은 https://blockdmask.tistory.com/429를 참고하세요 . fig, axes = plt.subplots(ncols=2,figsize=(8,4)) species_u = penguins[&quot;species&quot;].unique() for i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) # plt.show() fig.tight_layout() . 조금 더 간단히 그리는 방법 matplotlib는 기본적으로 Categorical 변수를 color로 바로 사용하지 못함 . penguins[&quot;species_codes&quot;] = pd.Categorical(penguins[&quot;species&quot;]).codes fig, axes = plt.subplots(ncols=2,figsize=(8,4)) axes[0].scatter(data=penguins, x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, c=&quot;species_codes&quot;, alpha=0.3) . &lt;matplotlib.collections.PathCollection at 0x16a8f61ac40&gt; . 1.4 Plot with seaborn . 단 세 줄로 거의 동일한 그림이 나왔습니다. scatter plot의 점 크기만 살짝 작습니다. | label의 투명도만 살짝 다릅니다. | . | seaborn 명령 scatterplot()을 그대로 사용했습니다. | x축과 y축 label도 바꾸었습니다. ax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다. | matplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다. | . | . fig, axes = plt.subplots(ncols=2,figsize=(8,4)) species_u = penguins[&quot;species&quot;].unique() # plot 0 : matplotlib for i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) # plot 1 : seaborn sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1]) axes[1].set_xlabel(&quot;Bill Length (mm)&quot;) axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;) fig.tight_layout() . 1.5 matplotlib + seaborn &amp; seaborn + matplotlib . matplotlib과 seaborn이 자유롭게 섞일 수 있습니다. matplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고, | seaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다. | . | 파이썬 코드는 다음과 같습니다. | . fig, axes = plt.subplots(ncols=2, figsize=(8, 4)) species_u = penguins[&quot;species&quot;].unique() # plot 0 : matplotlib + seaborn for i, s in enumerate(species_u): # matplotlib 산점도 axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3 ) # seaborn 추세선 sns.regplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, data=penguins.loc[penguins[&quot;species&quot;]==s], scatter=False, ax=axes[0]) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) # plot 1 : seaborn + matplotlib # seaborn 산점도 sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1]) axes[1].set_xlabel(&quot;Bill Length (mm)&quot;) axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;) for i, s in enumerate(species_u): # matplotlib 중심점 axes[1].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), c=f&quot;C{i}&quot;, alpha=1, marker=&quot;x&quot;, s=100 ) fig.tight_layout() . 1.6 seaborn + seaborn + matplotlib . 안 될 이유가 없습니다. | seaborn scatterplot + seaborn kdeplot + matplotlib text입니다 | . fig, ax = plt.subplots(figsize=(6,5)) # plot 0: scatter plot sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, color=&quot;k&quot;, data=penguins, alpha=0.3, ax=ax, legend=False) # plot 1: kde plot sns.kdeplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.5, ax=ax, legend=False) # text: species_u = penguins[&quot;species&quot;].unique() for i, s in enumerate(species_u): ax.text(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), s = s, fontdict={&quot;fontsize&quot;:14, &quot;fontweight&quot;:&quot;bold&quot;,&quot;color&quot;:&quot;k&quot;} ) ax.set_xlabel(&quot;Bill Length (mm)&quot;) ax.set_ylabel(&quot;Bill Depth (mm)&quot;) fig.tight_layout() . &lt;/div&gt; .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/2022/07/23/Visualization_Seaborn_and_Matplotlib.html",
            "relUrl": "/2022/07/23/Visualization_Seaborn_and_Matplotlib.html",
            "date": " • Jul 23, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Python 기본",
            "content": "Python Language Basics . Python Language Basics . Language Semantics . &#51473;&#44292;&#54840; &#50500;&#45772; &#46308;&#50668;&#50416;&#44592; . for x in array: if x &lt; privot: less.append(x) else: greater.append(x) . a = 5; b = 6; c = 7 . c . 7 . &#54632;&#49688; &#48143; &#44061;&#52404; method &#54840;&#52636; . 괄호를 사용하여 함수를 호출하고 0 이상의 인수를 전달합니다.옵션으로 반환된 값을 변수에 할당합니다. . result = f(x,y,z) g() . Python의 거의 모든 Object에는 Object의 내부 콘텐츠에 접근할 수 있는 Method라고 불리는 부가 함수가 있습니다. 다음 구문을 사용하여 호출할 수 있습니다. . obj.some_method(x,y,z) . &#48320;&#49688; &#48143; &#51064;&#49688; &#51204;&#45804; . a = [1,2,3] . b = a . a.append(4) # a라는 object에 4를 추가한다. #append는 a라는 object가 가지고 있는 함수 b . [1, 2, 3, 4] . &#45936;&#51060;&#53552; &#53440;&#51077; . a = 5 type(a) . int . a = &#39;foo&#39; type(a) . str . a = 4.5 type(a) . float . type(a) . float . Attributes and methods . python의 object는 일반적으로 attributes(다른 python object가 object를 &quot;저장&quot;하고 있는 것)과 method(&#39;object의 내부 데이터에 접근할 수 있는 objcet와 관련된 것)를 모두 가지고 있습니다. . a = &#39;foo&#39; . #string type으로 객체를 할당하면 .을 찍으면 사용할 수 있는 함수들이 나온다. . a.capitalize() #capitalize() : 문자열 str의 첫 번째 알파벳을 대문자로 바꾸고, 다른 알파벳은 모두 소문자로 변경한다. . &#39;Foo&#39; . a.upper() #upper() : 문자열 str의 모든 알파벳을 대문자로 변경한다. . &#39;FOO&#39; . Duck Typing . def isiterable(obj): try: iter(obj) return True except TypeError: #not iterable return False . isiterable(&#39;a string&#39;) . True . isiterable([1,2,3]) . True . Imports . Python에서 모듈은 단순히 Python 코드를 포함하는 .py 확장자를 가진 파일입니다. 다음 모듈이 있다고 가정합니다. . ##### some_module.py ##### PI = 3.14159 def f(x): return x + 2 def g(a,b): return a + b . 같은 디렉토리에 있는 다른 파일에서 some_moduel.py에 정의된 변수와 함수에 액세스하려면 다음을 수행합니다. . import some_module result = some_module.f(5) result . 7 . pi = some_module.PI #some_module에 ctrl을 누르면 some_module에 정의된 변수와 함수를 볼 수 있음 pi . 3.14159 . 매번 앞에 some_module을 쓰는 게 힘들 때 함수를 바로 불러오는 법 . from some_module import f, g, PI #some_module에서 direct로 f,g함수와 PI변수를 input 해줌 result = g(5,PI) result . 8.14159 . as 키워드를 사용하면 모듈이름과, 함수와 변수의 이름들을 간단하게 줄일 수 있음 . ex . import pandas ad pd . import some_module as sm from some_module import PI as pi, g as gf r1 = sm.f(pi) r2 = gf(6,pi) . r1 . 5.14159 . r2 . 9.14159 . &#48320;&#44221; &#44032;&#45733;&#54620; object&#50752; &#48520;&#44032;&#45733;&#54620; object . lists, dicts, NumPy arrays 및 대부분의 사용자 정의 유형(classes)과 같은 Python의 대부분의 object는 변경 가능합니다. 즉, 포함된 object 또는 값(value)을 변경할 수 있습니다. . a_list = [&#39;foo&#39;,2,[4,5]] a_list[2] = (3,4) a_list . [&#39;foo&#39;, 2, (3, 4)] . 반면에, 문자열과 tuple은 변경 불가능합니다. 단, 속도는 빠름 . Tuple . # a_tuple[1] = &#39;four&#39; . String . # a[10] = &#39;f&#39; # #string이기 때문에 변경 불가능 . b = a.replace(&#39;string&#39;,&#39;longer string&#39;) b . &#39;this is a longer string&#39; . 문자열은 일련의 유니코드 문자이므로 list나 tuple 등 다른 시퀀스와 동일하게 취급할 수 있습니다 . s = &#39;python&#39; list(s) . [&#39;p&#39;, &#39;y&#39;, &#39;t&#39;, &#39;h&#39;, &#39;o&#39;, &#39;n&#39;] . s[:3] . &#39;pyt&#39; . 두 개의 문자열을 함께 추가하여 그것들을 연결하고 새로운 문자열을 생성합니다. . a = &#39;this is the first half&#39; b = &#39;and this is the second half&#39; a + b . &#39;this is the first halfand this is the second half&#39; . file_dir = &#39;G:/tmp/&#39; file_name = &#39;test.csv&#39; file_dir + file_name . &#39;G:/tmp/test.csv&#39; . 문자열 object에는 포맷된 인수를 문자열로 대체하여 새로운 문자열을 생성하기 위해 사용할 수 있는 format method가 있습니다. . template = &#39;{0:.2f} {1:s} are worth US${2:d}&#39; template . &#39;{0:.2f} {1:s} are worth US${2:d}&#39; . {0:2f} 는 첫 번째 인수의 형식을 소수점 2자리 부동 소수점 숫자로 지정합니다. | {1:s} 는 두 번째 인수의 형식을 문자열로 지정합니다. | {2:d} 는 세 번째 인수의 형식을 정확한 정수로 지정합니다. | . template.format(4.5560, &#39;Argentine Pesos&#39;,1) . &#39;4.56 Argentine Pesos are worth US$1&#39; . template.format(1263.23,&#39;won&#39;,1) . &#39;1263.23 won are worth US$1&#39; . None . Python에서 변수에 아무 값도 넣고 싶지 않을 때 . a = None a is None . True . a #값은 없지만 object로 할당은 되었음 . None은 함수 인수의 공통 기본값이기도 합니다. . def add_and_maybe_multiply(a,b,c=None): result = a + b if c is not None: result = result * c return result . add_and_maybe_multiply(5,3) . 8 . add_and_maybe_multiply(5,3,10) . 80 . &#45216;&#51676;&#50752; &#49884;&#44036; . 내장된 Python datetime 모듈은 datetime, date 및 time types을 제공합니다. datetime 유형은 예상대로 날짜와 시간에 저장된 정보를 조합하여 가장 일반적으로 사용됩니다. . from datetime import datetime, date, time dt = datetime(2011,10,29,20,30,21) dt . datetime.datetime(2011, 10, 29, 20, 30, 21) . dt.day . 29 . datetime 인스턴스를 지정하면 동일한 이름의 datetime method를 호출하여 동일한 날짜 및 시간 개체를 추출할 수 있습니다. . dt.date() . datetime.date(2011, 10, 29) . dt.time() . datetime.time(20, 30, 21) . strftime 메서드는 datetime을 string으로 포맷합니다. . dt.strftime(&#39;%m %d %Y %H : %M&#39;) . &#39;10 29 2011 20 : 30&#39; . dt.strftime(&#39;%Y/%m/%d %H:%M&#39;) . &#39;2011/10/29 20:30&#39; . strptime method는 datetime을 string으로 포맷합니다. . datetime.strptime(&#39;20091031&#39;,&#39;%Y%m%d&#39;) . datetime.datetime(2009, 10, 31, 0, 0) . 시계열 데이터를 집계하거나 그룹화할 때, 분 및 초 field를 0으로 바꾸는 것과 같이 일련의 데이터 시간 field를 바꾸는 것이 유용할 수 있습니다. . dt.replace(minute=0,second=0) . datetime.datetime(2011, 10, 29, 20, 0) . dt2 = datetime(2011,11,15,22,30) delta = dt2- dt delta . datetime.timedelta(days=17, seconds=7179) . type(delta) . datetime.timedelta . &#49340;&#54637; &#50672;&#49328;&#51088;(Ternary Operator) . value = true-expr if condition else false-expr . 여기서 true-expr 및 false-expr은 임의의 Python 식입니다. 자세한 내용은 다음과 같습니다. . if condition: value = true-expr else: value = false-expr . x = 5 &#39;None-negative&#39; if x &gt;= 0 else &#39;Negative&#39; . &#39;None-negative&#39; . x = 5 a = 100 if x &gt;= 0 else -100 a . 100 . &#51228;&#50612;&#47928; . python은 다른 프로그래밍 언어에서 볼 수 있는 조건부 논리, 루프 및 기타 표준 제어 흐름 개념을 위한 여러 bulit-in 키워드를 가지고 있습니다. . if , elif and else . if 문은 가장 잘 알려진 제어 흐름문 유형 중 하나입니다. True일 경우 다음 블록의 코드를 평가하는 조건을 체크합니다. . x = -5 if x &lt; 0: print(&#39;It is negative&#39;) . It is negative . if 문 뒤에 옵션으로 하나 이상의 elif 블록과 모든 조건이 false인 경우 catch all other 블록을 사용할 수 있습니다. . x = -5 if x &lt; 0: print(&#39;It is negative&#39;) elif x == 0: print(&#39;Equal to zero&#39;) elif 0 &lt; x &lt; 5: print(&#39;Positive but smaller than 5&#39;) else: print(&#39;Positive and larger than or equl to 5&#39;) . It is negative . 어느 하나의 조건이 참일 경우 더 이상의 elif 또는 블록에 도달하지 않습니다. and 또는 or 를 사용하는 복합 조건에서는 조건이 왼쪽에서 오른쪽으로 평가되어 단락됩니다. . a = 5; b = 7 c = 8; d = 4 if a &lt; b or c &gt; d: print(&#39;Made it&#39;) . Made it . 이 예에서는 첫 번째 비교가 True였기 때문에 c&gt;d 비교는 평가되지 않습니다. 연쇄 비교도 가능합니다. . 4 &gt; 3 &gt; 2 &gt; 1 . True . 3&gt;5 or 2&gt;1 . True . 3&gt;5&gt;2&gt;1 . False . Pass . pass는 Python에서 &quot;no-op&quot;(&quot;No operation&quot;) 문입니다. 액션이 수행되지 않는 블록(또는 아직 구현되지 않은 코드의 자리 표시자)에서 사용할 수 있습니다. Python은 블록을 구분하기 위해 공백을 사용하기 때문에 필요합니다. . x = -1 if x &lt; 0: print(&quot;negative!&quot;) elif x == 0: # TODO: put something smart here pass else: print(&quot;positive!&quot;) . negative! . For loops . for loop는 컬렉션(list나 tuple 등) 또는 반복기로 반복하기 위한 것입니다. for loop의 표준 구문은 다음과 같습니다. . for value in collection: #do something with value . continue 키워드를 사용하여 for loop를 다음 반복으로 진행하고 나머지 블록을 건너뛸 수 있습니다. 이 코드는 list의 int를 집계하고 None 값을 건너뜁니다. . # total = 0 # for value in sequence: # total += value . sequence = [1,2, None, 4, None, 5] total = 0 for value in sequence: if value is None: continue total += value . total . 12 . for loop는 break 키워드를 사용하여 모두 종료할 수 있습니다. 이 코드는 5에 도달할 때까지 목록의 요소를 집계합니다. . sequence = [1,2,0,4,6,5,2,1] total_until_5 = 0 for value in sequence: if value == 5: break total_until_5 += value . total_until_5 . 13 . break 키워드는 가장 안쪽의 for loop만 종료합니다.outer for loop는 계속 실행됩니다. . for i in range(4): for j in range(4): if j &gt; i: break print((i,j)) . (0, 0) (1, 0) (1, 1) (2, 0) (2, 1) (2, 2) (3, 0) (3, 1) (3, 2) (3, 3) . ? 컬렉션 또는 반복기 내의 요소가 시퀀스(튜플 또는 리스트 등)인 경우 쉽게 for loop 스테이트먼트의 변수로 압축 해제할 수 있습니다. . for a, b, c in iterator: # do something . for a, b, c in [[1,2,3],[4,5,6],[7,8,9]]: print(a,b,c) . 1 2 3 4 5 6 7 8 9 . While loops . while loop문은 조건이 False로 평가되거나 루프가 명시적으로 break으로 종료될 때까지 실행되는 조건 및 코드 블록을 지정합니다. . x = 256 total = 0 while x &gt; 0: if total &gt; 500: break total += x x = x // 2 . total . 504 . x . 4 . Range . range 함수는 균일한 간격의 정수 시퀀스를 생성하는 반복기를 반환합니다. . range(10) . range(0, 10) . list(range(10)) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . 시작, 종료 및 단계(음수일 수 있음)를 모두 지정할 수 있습니다. . list(range(0,20,2)) . [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] . list(range(5,0,-1)) . [5, 4, 3, 2, 1] . 보시다시피 range는 끝점을 포함하지 않는 정수를 생성합니다.range의 일반적인 용도는 인덱스로 시퀀스를 반복하는 것입니다. . seq = [1,2,3,4] for i in range(len(seq)): val = seq[i] . val . 4 . list와 같은 함수를 사용하여 범위별로 생성된 모든 정수를 다른 데이터 구조에 저장할 수 있지만, 종종 기본 반복자 형식이 원하는 것이 됩니다. 이 스니펫에서는 3 또는5의 배수인0 ~ 99,999 의 모든 수치를 집계하고 있습니다. . sum = 0 for i in range(100000): # % is the modulo operator if i % 3 == 0 or i % 5 == 0: sum += i .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/jupyter/python/2022/03/20/python_basic.html",
            "relUrl": "/jupyter/python/2022/03/20/python_basic.html",
            "date": " • Mar 20, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Pandas 기본",
            "content": "도구 - 판다스(pandas) . pandas 라이브러리는 사용하기 쉬운 고성능 데이터 구조와 데이터 분석 도구를 제공합니다. 주 데이터 구조는 DataFrame입니다. 이를 인-메모리(in-memory) 2D 테이블로 생각할 수 있습니다(열 이름과 행 레이블이 있는 스프레드시트와 비슷합니다). 엑셀에 있는 많은 기능을 프로그램에서 사용할 수 있습니다. 여기에는 피봇 테이블이나 다른 열을 기반으로 열을 계산하고 그래프 출력하는 기능 등이 포함됩니다. 열 값으로 행을 그룹핑할 수도 있습니다. 또한 SQL과 비슷하게 테이블을 조인할 수 있습니다. 판다스는 시계열 데이터를 다루는데도 뛰어납니다. . 필요 라이브러리: . 넘파이(NumPy) – 넘파이에 익숙하지 않다면 지금 넘파이 튜토리얼을 둘러 보세요. | . 구글 코랩에서 실행하기 | &#49444;&#51221; . 먼저 pandas를 임포트합니다. 보통 pd로 임포트합니다: . import pandas as pd import numpy as np . Series &#44061;&#52404; . pandas 라이브러리는 다음과 같은 유용한 데이터 구조를 포함하고 있습니다: . Series 객체를 곧 이어서 설명하겠습니다. Series 객체는 1D 배열입니다. (열 이름과 행 레이블을 가진) 스프레드시트의 열과 비슷합니다. | DataFrame 객체는 2D 테이블입니다. (열 이름과 행 레이블을 가진) 스프레드시트와 비슷합니다. | . Series &#47564;&#46308;&#44592; . 첫 번째 Series 객체를 만들어 보죠! . import numpy as np np.array([2,-1,3,5]) . array([ 2, -1, 3, 5]) . s = pd.Series([2,-1,3,5]) s . 0 2 1 -1 2 3 3 5 dtype: int64 . 1D ndarray&#50752; &#48708;&#49847;&#54633;&#45768;&#45796; . Series 객체는 넘파이 ndarray와 비슷하게 동작합니다. 넘파이 함수에 매개변수로 종종 전달할 수 있습니다: . import numpy as np np.exp(s) . 0 7.389056 1 0.367879 2 20.085537 3 148.413159 dtype: float64 . Series 객체에 대한 산술 연산도 가능합니다. ndarray와 비슷하게 원소별로 적용됩니다: . s + [1000,2000,3000,4000] . 0 1002 1 1999 2 3003 3 4005 dtype: int64 . 넘파이와 비슷하게 Series에 하나의 숫자를 더하면 Series에 있는 모든 원소에 더해집니다. 이를 브로드캐스팅(broadcasting)이라고 합니다: . s + 1000 . 0 1002 1 999 2 1003 3 1005 dtype: int64 . *나 / 같은 모든 이항 연산과 심지어 조건 연산에서도 마찬가지입니다: . s &lt; 0 . 0 False 1 True 2 False 3 False dtype: bool . s[s&lt;0] . 1 -1 dtype: int64 . &#51064;&#45937;&#49828; &#47112;&#51060;&#48660; . Series 객체에 있는 각 원소는 인덱스 레이블(index label)이라 불리는 고유한 식별자를 가지고 있습니다. 기본적으로 Series에 있는 원소의 순서입니다(0에서 시작합니다). 하지만 수동으로 인덱스 레이블을 지정할 수도 있습니다: . s2 = pd.Series([68, 83, 112, 68], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;, &quot;darwin&quot;]) s2 . alice 68 bob 83 charles 112 darwin 68 dtype: int64 . 그다음 dict처럼 Series를 사용할 수 있습니다: . s2[&quot;bob&quot;] . 83 . 일반 배열처럼 정수 인덱스를 사용하여 계속 원소에 접근할 수 있습니다: . s2[1] . 83 . 레이블이나 정수를 사용해 접근할 때 명확하게 하기 위해 레이블은 loc 속성을 사용하고 정수는 iloc 속성을 사용하는 것이 좋습니다: . s2.loc[&quot;bob&quot;] . 83 . s2.iloc[1] #정수는 integer loc . 83 . Series는 인덱스 레이블을 슬라이싱할 수도 있습니다: . s2.iloc[1:3] . bob 83 charles 112 dtype: int64 . 기본 정수 레이블을 사용할 때 예상 외의 결과를 만들 수 있기 때문에 주의해야 합니다: . surprise = pd.Series([1000, 1001, 1002, 1003]) surprise . 0 1000 1 1001 2 1002 3 1003 dtype: int64 . surprise_slice = surprise[2:] surprise_slice # index 레이블이 0부터 시작해야하는데 2부터 시작함 . 2 1002 3 1003 dtype: int64 . #surprise_slice.[0] . surprise_slice.iloc[0] . 1002 . 보세요. 첫 번째 원소의 인덱스 레이블이 2입니다. 따라서 슬라이싱 결과에서 인덱스 레이블 0인 원소는 없습니다: . try: surprise_slice[0] except KeyError as e: print(&quot;키 에러:&quot;, e) . 키 에러: 0 . 하지만 iloc 속성을 사용해 정수 인덱스로 원소에 접근할 수 있습니다. Series 객체를 사용할 때 loc와 iloc를 사용하는 것이 좋은 이유입니다: . surprise_slice.iloc[0] . 1002 . dict&#50640;&#49436; &#52488;&#44592;&#54868; . dict에서 Series 객체를 만들 수 있습니다. 키는 인덱스 레이블로 사용됩니다: . weights = {&quot;alice&quot;: 68, &quot;bob&quot;: 83, &quot;colin&quot;: 86, &quot;darwin&quot;: 68} s3 = pd.Series(weights) s3 . alice 68 bob 83 colin 86 darwin 68 dtype: int64 . Series에 포함할 원소를 제어하고 index를 지정하여 명시적으로 순서를 결정할 수 있습니다: . s4 = pd.Series(weights, index = [&quot;colin&quot;, &quot;alice&quot;]) s4 . colin 86 alice 68 dtype: int64 . &#51088;&#46041; &#51221;&#47148; . 여러 개의 Series 객체를 다룰 때 pandas는 자동으로 인덱스 레이블에 따라 원소를 정렬합니다. . s2 . alice 68 bob 83 charles 112 darwin 68 dtype: int64 . s3 . alice 68 bob 83 colin 86 darwin 68 dtype: int64 . # s2에는 colin이 s3 에는 charles가 없기 때문에 NaN값. print(s2.keys()) print(s3.keys()) s2 + s3 . Index([&#39;alice&#39;, &#39;bob&#39;, &#39;charles&#39;, &#39;darwin&#39;], dtype=&#39;object&#39;) Index([&#39;alice&#39;, &#39;bob&#39;, &#39;colin&#39;, &#39;darwin&#39;], dtype=&#39;object&#39;) . alice 136.0 bob 166.0 charles NaN colin NaN darwin 136.0 dtype: float64 . 만들어진 Series는 s2와 s3의 인덱스 레이블의 합집합을 담고 있습니다. s2에 &quot;colin&quot;이 없고 s3에 &quot;charles&quot;가 없기 때문에 이 원소는 NaN 값을 가집니다(Not-a-Number는 누락이란 의미입니다). . 자동 정렬은 구조가 다고 누락된 값이 있는 여러 데이터를 다룰 때 매우 편리합니다. 하지만 올바른 인덱스 레이블을 지정하는 것을 잊는다면 원치않는 결과를 얻을 수 있습니다: . s5 = pd.Series([1000,1000,1000,1000]) print(&quot;s2 =&quot;, s2.values) print(&quot;s5 =&quot;, s5.values) s2 + s5 . s2 = [ 68 83 112 68] s5 = [1000 1000 1000 1000] . alice NaN bob NaN charles NaN darwin NaN 0 NaN 1 NaN 2 NaN 3 NaN dtype: float64 . 레이블이 하나도 맞지 않기 때문에 판다스가 이 Series를 정렬할 수 없습니다. 따라서 모두 NaN이 되었습니다. . &#49828;&#52860;&#46972;&#47196; &#52488;&#44592;&#54868; . 스칼라와 인덱스 레이블의 리스트로 Series 객체를 초기화할 수도 있습니다: 모든 원소가 이 스칼라 값으로 설정됩니다. . meaning = pd.Series(42, [&quot;life&quot;, &quot;universe&quot;, &quot;everything&quot;]) #meaning = pd.Series([42,42,42], [&quot;life&quot;, &quot;universe&quot;, &quot;everything&quot;]) meaning . life 42 universe 42 everything 42 dtype: int64 . Series &#51060;&#47492; . Series는 name을 가질 수 있습니다: . s6 = pd.Series([83, 68], index=[&quot;bob&quot;, &quot;alice&quot;], name=&quot;weights&quot;) s6 . bob 83 alice 68 Name: weights, dtype: int64 . Series &#44536;&#47000;&#54532; &#52636;&#47141; . 맷플롯립을 사용해 Series 데이터를 쉽게 그래프로 출력할 수 있습니다(맷플롯립에 대한 자세한 설명은 맷플롯립 튜토리얼을 참고하세요). 맷플롯립을 임포트하고 plot() 메서드를 호출하면 끝입니다: . %matplotlib inline import matplotlib.pyplot as plt temperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5] s7 = pd.Series(temperatures, name=&quot;Temperature&quot;) s7.plot() plt.show() . 데이터를 그래프로 출력하는데 많은 옵션이 있습니다. 여기에서 모두 나열할 필요는 없습니다. 특정 종류의 그래프(히스토그램, 파이 차트 등)가 필요하면 판다스 문서의 시각화 섹션에서 예제 코드를 참고하세요. . &#49884;&#44036; &#45796;&#47336;&#44592; . 많은 데이터셋에 타임스탬프가 포함되어 있습니다. 판다스는 이런 데이터를 다루는데 뛰어납니다: . (2016Q3 같은) 기간과 (&quot;monthly&quot; 같은) 빈도를 표현할 수 있습니다. | 기간을 실제 타임스탬프로 변환하거나 그 반대로 변환할 수 있습니다. | 데이터를 리샘플링하고 원하는 방식으로 값을 모을 수 있습니다. | 시간대를 다룰 수 있습니다. | . &#49884;&#44036; &#48276;&#50948; . 먼저 pd.date_range()를 사용해 시계열을 만들어 보죠. 이 함수는 2016년 10월 29일 5:30pm에서 시작하여 12시간마다 하나의 datetime을 담고 있는 DatetimeIndex를 반환합니다. . dates = pd.date_range(&#39;2016/10/29 5:30pm&#39;, periods=12, freq=&#39;H&#39;) #freq=뽑는 간격 dates . DatetimeIndex([&#39;2016-10-29 17:30:00&#39;, &#39;2016-10-29 18:30:00&#39;, &#39;2016-10-29 19:30:00&#39;, &#39;2016-10-29 20:30:00&#39;, &#39;2016-10-29 21:30:00&#39;, &#39;2016-10-29 22:30:00&#39;, &#39;2016-10-29 23:30:00&#39;, &#39;2016-10-30 00:30:00&#39;, &#39;2016-10-30 01:30:00&#39;, &#39;2016-10-30 02:30:00&#39;, &#39;2016-10-30 03:30:00&#39;, &#39;2016-10-30 04:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;H&#39;) . pd.date_range(&#39;2016/10/29 5:30pm&#39;, periods=12, freq=&#39;2H&#39;) #freq=뽑는 간격 . DatetimeIndex([&#39;2016-10-29 17:30:00&#39;, &#39;2016-10-29 19:30:00&#39;, &#39;2016-10-29 21:30:00&#39;, &#39;2016-10-29 23:30:00&#39;, &#39;2016-10-30 01:30:00&#39;, &#39;2016-10-30 03:30:00&#39;, &#39;2016-10-30 05:30:00&#39;, &#39;2016-10-30 07:30:00&#39;, &#39;2016-10-30 09:30:00&#39;, &#39;2016-10-30 11:30:00&#39;, &#39;2016-10-30 13:30:00&#39;, &#39;2016-10-30 15:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;2H&#39;) . pd.date_range(&#39;2016/10/29 5:30pm&#39;, periods=6, freq=&#39;2H&#39;) #freq=뽑는 간격 . DatetimeIndex([&#39;2016-10-29 17:30:00&#39;, &#39;2016-10-29 19:30:00&#39;, &#39;2016-10-29 21:30:00&#39;, &#39;2016-10-29 23:30:00&#39;, &#39;2016-10-30 01:30:00&#39;, &#39;2016-10-30 03:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;2H&#39;) . pd.date_range(&#39;2020-10-07&#39;, &#39;2020-10-20&#39;, freq = &#39;D&#39;) # 시작날짜와 끝나는 날짜를 지정 . DatetimeIndex([&#39;2020-10-07&#39;, &#39;2020-10-08&#39;, &#39;2020-10-09&#39;, &#39;2020-10-10&#39;, &#39;2020-10-11&#39;, &#39;2020-10-12&#39;, &#39;2020-10-13&#39;, &#39;2020-10-14&#39;, &#39;2020-10-15&#39;, &#39;2020-10-16&#39;, &#39;2020-10-17&#39;, &#39;2020-10-18&#39;, &#39;2020-10-19&#39;, &#39;2020-10-20&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) . 이 DatetimeIndex를 Series의 인덱스로 사용할수 있습니다: . temp_series = pd.Series(temperatures, dates) temp_series . 2016-10-29 17:30:00 4.4 2016-10-29 18:30:00 5.1 2016-10-29 19:30:00 6.1 2016-10-29 20:30:00 6.2 2016-10-29 21:30:00 6.1 2016-10-29 22:30:00 6.1 2016-10-29 23:30:00 5.7 2016-10-30 00:30:00 5.2 2016-10-30 01:30:00 4.7 2016-10-30 02:30:00 4.1 2016-10-30 03:30:00 3.9 2016-10-30 04:30:00 3.5 Freq: H, dtype: float64 . 이 시리즈를 그래프로 출력해 보죠: . temp_series.plot(kind=&quot;bar&quot;) plt.grid(True) plt.show() . &#47532;&#49368;&#54540;&#47553; . 판다스는 매우 간단하게 시계열을 리샘플링할 수 있습니다. resample() 메서드를 호출하고 새로운 주기를 지정하면 됩니다: . temp_series_freq_2H = temp_series.resample(&quot;2H&quot;) temp_series_freq_2H . &lt;pandas.core.resample.DatetimeIndexResampler object at 0x000001620BB0FC10&gt; . 리샘플링 연산은 사실 지연된 연산입니다. 그래서 Series 객체 대신 DatetimeIndexResampler 객체가 반환됩니다. 실제 리샘플링 연산을 수행하려면 mean() 같은 메서드를 호출할 수 있습니다. 이 메서드는 연속적인 시간 쌍에 대해 평균을 계산합니다: . temp_series_freq_2H = temp_series_freq_2H.mean() . temp_series_freq_2H . 2016-10-29 16:00:00 4.40 2016-10-29 18:00:00 5.60 2016-10-29 20:00:00 6.15 2016-10-29 22:00:00 5.90 2016-10-30 00:00:00 4.95 2016-10-30 02:00:00 4.00 2016-10-30 04:00:00 3.50 Freq: 2H, dtype: float64 . 결과를 그래프로 출력해 보죠: . temp_series_freq_2H.plot(kind=&quot;bar&quot;) plt.show() . 2시간 간격으로 어떻게 값이 수집되었는지 확인해 보세요. 예를 들어 6-8pm 간격을 보면 6:30pm에서 5.1이고 7:30pm에서 6.1입니다. 리샘플링 후에 5.1과 6.1의 평균인 5.6 하나를 얻었습니다. 평균말고 어떤 집계 함수(aggregation function)도 사용할 수 있습니다. 예를 들어 각 기간에서 최솟값을 찾을 수 있습니다: . temp_series_freq_2H = temp_series.resample(&quot;2H&quot;).min() temp_series_freq_2H . 2016-10-29 16:00:00 4.4 2016-10-29 18:00:00 5.1 2016-10-29 20:00:00 6.1 2016-10-29 22:00:00 5.7 2016-10-30 00:00:00 4.7 2016-10-30 02:00:00 3.9 2016-10-30 04:00:00 3.5 Freq: 2H, dtype: float64 . 또는 동일한 효과를 내는 apply() 메서드를 사용할 수 있습니다: . temp_series_freq_2H = temp_series.resample(&quot;2H&quot;).apply(np.min) temp_series_freq_2H . 2016-10-29 16:00:00 4.4 2016-10-29 18:00:00 5.1 2016-10-29 20:00:00 6.1 2016-10-29 22:00:00 5.7 2016-10-30 00:00:00 4.7 2016-10-30 02:00:00 3.9 2016-10-30 04:00:00 3.5 Freq: 2H, dtype: float64 . &#50629;&#49368;&#54540;&#47553;&#44284; &#48372;&#44036; . 다운샘플링의 예를 보았습니다. 하지만 업샘플링(즉, 빈도를 높입니다)도 할 수 있습니다. 하지만 데이터에 구멍을 만듭니다: . temp_series_freq_15min = temp_series.resample(&quot;2H&quot;).mean() temp_series_freq_15min . 2016-10-29 16:00:00 4.40 2016-10-29 18:00:00 5.60 2016-10-29 20:00:00 6.15 2016-10-29 22:00:00 5.90 2016-10-30 00:00:00 4.95 2016-10-30 02:00:00 4.00 2016-10-30 04:00:00 3.50 Freq: 2H, dtype: float64 . temp_series_freq_15min = temp_series.resample(&quot;15Min&quot;).mean() temp_series_freq_15min.head(n=10) # 간격을 줄이면 없는 값이 NaN으로 출력된다. . 2016-10-29 17:30:00 4.4 2016-10-29 17:45:00 NaN 2016-10-29 18:00:00 NaN 2016-10-29 18:15:00 NaN 2016-10-29 18:30:00 5.1 2016-10-29 18:45:00 NaN 2016-10-29 19:00:00 NaN 2016-10-29 19:15:00 NaN 2016-10-29 19:30:00 6.1 2016-10-29 19:45:00 NaN Freq: 15T, dtype: float64 . 한가지 방법은 보간으로 사이를 채우는 것입니다. 이렇게 하려면 interpolate() 메서드를 호출합니다. 기본값은 선형 보간이지만 3차 보간(cubic interpolation) 같은 다른 방법을 선택할 수 있습니다: . temp_series_freq_15min = temp_series.resample(&quot;15Min&quot;).interpolate(method=&quot;cubic&quot;) temp_series_freq_15min.head(n=10) . 2016-10-29 17:30:00 4.400000 2016-10-29 17:45:00 4.452911 2016-10-29 18:00:00 4.605113 2016-10-29 18:15:00 4.829758 2016-10-29 18:30:00 5.100000 2016-10-29 18:45:00 5.388992 2016-10-29 19:00:00 5.669887 2016-10-29 19:15:00 5.915839 2016-10-29 19:30:00 6.100000 2016-10-29 19:45:00 6.203621 Freq: 15T, dtype: float64 . temp_series.plot(label=&quot;Period: 1 hour&quot;) temp_series_freq_15min.plot(label=&quot;Period: 15 minutes&quot;) plt.legend() plt.show() . &#49884;&#44036;&#45824; . 기본적으로 datetime은 단순합니다. 시간대(timezone)을 고려하지 않죠. 따라서 2016-10-30 02:30는 파리나 뉴욕이나 2016년 10월 30일 2:30pm입니다. tz_localize() 메서드로 시간대를 고려한 datetime을 만들 수 있습니다: . temp_series_ny = temp_series.tz_localize(&quot;America/New_York&quot;) temp_series_ny . 2016-10-29 17:30:00-04:00 4.4 2016-10-29 18:30:00-04:00 5.1 2016-10-29 19:30:00-04:00 6.1 2016-10-29 20:30:00-04:00 6.2 2016-10-29 21:30:00-04:00 6.1 2016-10-29 22:30:00-04:00 6.1 2016-10-29 23:30:00-04:00 5.7 2016-10-30 00:30:00-04:00 5.2 2016-10-30 01:30:00-04:00 4.7 2016-10-30 02:30:00-04:00 4.1 2016-10-30 03:30:00-04:00 3.9 2016-10-30 04:30:00-04:00 3.5 dtype: float64 . temp_series_seoul = temp_series.tz_localize(&quot;Asia/Seoul&quot;) temp_series_seoul . 2016-10-29 17:30:00+09:00 4.4 2016-10-29 18:30:00+09:00 5.1 2016-10-29 19:30:00+09:00 6.1 2016-10-29 20:30:00+09:00 6.2 2016-10-29 21:30:00+09:00 6.1 2016-10-29 22:30:00+09:00 6.1 2016-10-29 23:30:00+09:00 5.7 2016-10-30 00:30:00+09:00 5.2 2016-10-30 01:30:00+09:00 4.7 2016-10-30 02:30:00+09:00 4.1 2016-10-30 03:30:00+09:00 3.9 2016-10-30 04:30:00+09:00 3.5 dtype: float64 . 모든 datetime에 -04:00이 추가됩니다. 즉 모든 시간은 UTC - 4시간을 의미합니다. . 다음처럼 파리 시간대로 바꿀 수 있습니다: . temp_series_paris = temp_series_ny.tz_convert(&quot;Europe/Paris&quot;) temp_series_paris . 2016-10-29 23:30:00+02:00 4.4 2016-10-30 00:30:00+02:00 5.1 2016-10-30 01:30:00+02:00 6.1 2016-10-30 02:30:00+02:00 6.2 2016-10-30 02:30:00+01:00 6.1 2016-10-30 03:30:00+01:00 6.1 2016-10-30 04:30:00+01:00 5.7 2016-10-30 05:30:00+01:00 5.2 2016-10-30 06:30:00+01:00 4.7 2016-10-30 07:30:00+01:00 4.1 2016-10-30 08:30:00+01:00 3.9 2016-10-30 09:30:00+01:00 3.5 dtype: float64 . UTC와의 차이가 +02:00에서 +01:00으로 바뀐 것을 알 수 있습니다. 이는 프랑스가 10월 30일 3am에 겨울 시간으로 바꾸기 때문입니다(2am으로 바뀝니다). 따라서 2:30am이 두 번 등장합니다! 시간대가 없는 표현으로 돌아가 보죠(시간대가 없이 지역 시간으로 매시간 로그를 기록하는 경우 이와 비슷할 것입니다): . temp_series_paris_naive = temp_series_paris.tz_localize(None) temp_series_paris_naive . 2016-10-29 23:30:00 4.4 2016-10-30 00:30:00 5.1 2016-10-30 01:30:00 6.1 2016-10-30 02:30:00 6.2 2016-10-30 02:30:00 6.1 2016-10-30 03:30:00 6.1 2016-10-30 04:30:00 5.7 2016-10-30 05:30:00 5.2 2016-10-30 06:30:00 4.7 2016-10-30 07:30:00 4.1 2016-10-30 08:30:00 3.9 2016-10-30 09:30:00 3.5 dtype: float64 . 이렇게 되면 02:30이 정말 애매합니다. 시간대가 없는 datetime을 파리 시간대로 바꿀 때 에러가 발생합니다: . try: temp_series_paris_naive.tz_localize(&quot;Europe/Paris&quot;) except Exception as e: print(type(e)) print(e) . &lt;class &#39;NameError&#39;&gt; name &#39;temp_series_paris_naive&#39; is not defined . 다행히 ambiguous 매개변수를 사용하면 판다스가 타임스탬프의 순서를 기반으로 적절한 DST(일광 절약 시간제)를 추측합니다: . temp_series_paris_naive.tz_localize(&quot;Europe/Paris&quot;, ambiguous=&quot;infer&quot;) . 2016-10-29 23:30:00+02:00 4.4 2016-10-30 00:30:00+02:00 5.1 2016-10-30 01:30:00+02:00 6.1 2016-10-30 02:30:00+02:00 6.2 2016-10-30 02:30:00+01:00 6.1 2016-10-30 03:30:00+01:00 6.1 2016-10-30 04:30:00+01:00 5.7 2016-10-30 05:30:00+01:00 5.2 2016-10-30 06:30:00+01:00 4.7 2016-10-30 07:30:00+01:00 4.1 2016-10-30 08:30:00+01:00 3.9 2016-10-30 09:30:00+01:00 3.5 dtype: float64 . &#44592;&#44036; . pd.period_range() 함수는 DatetimeIndex가 아니라 PeriodIndex를 반환합니다. 예를 들어 2016과 2017년의 전체 분기를 가져와 보죠: . quarters = pd.period_range(&#39;2016Q1&#39;, periods=8, freq=&#39;Q&#39;) #&#39;Q&#39; = quarter분기 #&#39;M&#39; = 한달 quarters . PeriodIndex([&#39;2016Q1&#39;, &#39;2016Q2&#39;, &#39;2016Q3&#39;, &#39;2016Q4&#39;, &#39;2017Q1&#39;, &#39;2017Q2&#39;, &#39;2017Q3&#39;, &#39;2017Q4&#39;], dtype=&#39;period[Q-DEC]&#39;) . PeriodIndex에 숫자 N을 추가하면 PeriodIndex 빈도의 N 배만큼 이동시킵니다: . quarters + 3 # 단위가 quarter이므로 3quarter씩 더해줌. . PeriodIndex([&#39;2016Q4&#39;, &#39;2017Q1&#39;, &#39;2017Q2&#39;, &#39;2017Q3&#39;, &#39;2017Q4&#39;, &#39;2018Q1&#39;, &#39;2018Q2&#39;, &#39;2018Q3&#39;], dtype=&#39;period[Q-DEC]&#39;) . asfreq() 메서드를 사용하면 PeriodIndex의 빈도를 바꿀 수 있습니다. 모든 기간이 늘어나거나 줄어듭니다. 예를 들어 분기 기간을 모두 월별 기간으로 바꾸어 보죠: . quarters.asfreq(&quot;M&quot;) . PeriodIndex([&#39;2016-03&#39;, &#39;2016-06&#39;, &#39;2016-09&#39;, &#39;2016-12&#39;, &#39;2017-03&#39;, &#39;2017-06&#39;, &#39;2017-09&#39;, &#39;2017-12&#39;], dtype=&#39;period[M]&#39;) . quarters . PeriodIndex([&#39;2016Q1&#39;, &#39;2016Q2&#39;, &#39;2016Q3&#39;, &#39;2016Q4&#39;, &#39;2017Q1&#39;, &#39;2017Q2&#39;, &#39;2017Q3&#39;, &#39;2017Q4&#39;], dtype=&#39;period[Q-DEC]&#39;) . 기본적으로 asfreq는 각 기간의 끝에 맞춥니다. 기간의 시작에 맞추도록 변경할 수 있습니다: . quarters.asfreq(&quot;M&quot;, how=&quot;start&quot;) . PeriodIndex([&#39;2016-01&#39;, &#39;2016-04&#39;, &#39;2016-07&#39;, &#39;2016-10&#39;, &#39;2017-01&#39;, &#39;2017-04&#39;, &#39;2017-07&#39;, &#39;2017-10&#39;], dtype=&#39;period[M]&#39;) . 간격을 늘릴 수도 있습니다: . quarters.asfreq(&quot;A&quot;) # &#39;A&#39; = 연도 . PeriodIndex([&#39;2016&#39;, &#39;2016&#39;, &#39;2016&#39;, &#39;2016&#39;, &#39;2017&#39;, &#39;2017&#39;, &#39;2017&#39;, &#39;2017&#39;], dtype=&#39;period[A-DEC]&#39;) . 물론 PeriodIndex로 Series를 만들 수 있습니다: . quarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters) quarterly_revenue . 2016Q1 300 2016Q2 320 2016Q3 290 2016Q4 390 2017Q1 320 2017Q2 360 2017Q3 310 2017Q4 410 Freq: Q-DEC, dtype: int64 . quarterly_revenue.plot(kind=&quot;line&quot;) plt.show() . to_timestamp를 호출해서 기간을 타임스탬프로 변경할 수 있습니다. 기본적으로 기간의 첫 번째 날을 반환합니다. 하지만 how와 freq를 지정해서 기간의 마지막 시간을 얻을 수 있습니다: . quarterly_revenue . 2016Q1 300 2016Q2 320 2016Q3 290 2016Q4 390 2017Q1 320 2017Q2 360 2017Q3 310 2017Q4 410 Freq: Q-DEC, dtype: int64 . last_hours = quarterly_revenue.to_timestamp(how=&quot;end&quot;, freq=&quot;H&quot;) last_hours . 2016-03-31 23:59:59.999999999 300 2016-06-30 23:59:59.999999999 320 2016-09-30 23:59:59.999999999 290 2016-12-31 23:59:59.999999999 390 2017-03-31 23:59:59.999999999 320 2017-06-30 23:59:59.999999999 360 2017-09-30 23:59:59.999999999 310 2017-12-31 23:59:59.999999999 410 dtype: int64 . to_peroid를 호출하면 다시 기간으로 돌아갑니다: . last_hours.to_period() #분기로 다시 바꿔줌 . 2016Q1 300 2016Q2 320 2016Q3 290 2016Q4 390 2017Q1 320 2017Q2 360 2017Q3 310 2017Q4 410 Freq: Q-DEC, dtype: int64 . 판다스는 여러 가지 시간 관련 함수를 많이 제공합니다. 온라인 문서를 확인해 보세요. 예를 하나 들면 2016년 매월 마지막 업무일의 9시를 얻는 방법은 다음과 같습니다: . months_2022 = pd.period_range(&quot;2022&quot;, periods=12, freq = &#39;M&#39;) one_day_after_last_days = months_2022.asfreq(&quot;D&quot;)+1 last_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay() last_bdays.to_period(&quot;H&quot;)+9 . PeriodIndex([&#39;2022-01-31 09:00&#39;, &#39;2022-02-28 09:00&#39;, &#39;2022-03-31 09:00&#39;, &#39;2022-04-29 09:00&#39;, &#39;2022-05-31 09:00&#39;, &#39;2022-06-30 09:00&#39;, &#39;2022-07-29 09:00&#39;, &#39;2022-08-31 09:00&#39;, &#39;2022-09-30 09:00&#39;, &#39;2022-10-31 09:00&#39;, &#39;2022-11-30 09:00&#39;, &#39;2022-12-30 09:00&#39;], dtype=&#39;period[H]&#39;) . BDay()는 business day 주말이면 2일을 빼주고 평일이면 하루를 빼준다. . months_2016 = pd.period_range(&quot;2016&quot;, periods=12, freq=&quot;M&quot;) one_day_after_last_days = months_2016.asfreq(&quot;D&quot;) + 1 #매월의 첫날 last_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay(n=1) last_bdays.to_period(&quot;H&quot;) + 9 . PeriodIndex([&#39;2016-01-29 09:00&#39;, &#39;2016-02-29 09:00&#39;, &#39;2016-03-31 09:00&#39;, &#39;2016-04-29 09:00&#39;, &#39;2016-05-31 09:00&#39;, &#39;2016-06-30 09:00&#39;, &#39;2016-07-29 09:00&#39;, &#39;2016-08-31 09:00&#39;, &#39;2016-09-30 09:00&#39;, &#39;2016-10-31 09:00&#39;, &#39;2016-11-30 09:00&#39;, &#39;2016-12-30 09:00&#39;], dtype=&#39;period[H]&#39;) . DataFrame &#44061;&#52404; . 데이터프레임 객체는 스프레드시트를 표현합니다. 셀 값, 열 이름, 행 인덱스 레이블을 가집니다. 다른 열을 바탕으로 열을 계산하는 식을 쓸 수 있고 피봇 테이블을 만들고, 행을 그룹핑하고, 그래프를 그릴 수 있습니다. DataFrame을 Series의 딕셔너리로 볼 수 있습니다. . DataFrame &#47564;&#46308;&#44592; . Series 객체의 딕셔너리를 전달하여 데이터프레임을 만들 수 있습니다: . people_dict = { &quot;weight&quot;: pd.Series([68, 83, 112], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;]), &quot;birthyear&quot;: pd.Series([1984, 1985, 1992], index=[&quot;bob&quot;, &quot;alice&quot;, &quot;charles&quot;], name=&quot;year&quot;), &quot;children&quot;: pd.Series([0, 3], index=[&quot;charles&quot;, &quot;bob&quot;]), &quot;hobby&quot;: pd.Series([&quot;Biking&quot;, &quot;Dancing&quot;], index=[&quot;alice&quot;, &quot;bob&quot;]), } people = pd.DataFrame(people_dict) people . weight birthyear children hobby . alice 68 | 1985 | NaN | Biking | . bob 83 | 1984 | 3.0 | Dancing | . charles 112 | 1992 | 0.0 | NaN | . people_dict . {&#39;weight&#39;: alice 68 bob 83 charles 112 dtype: int64, &#39;birthyear&#39;: bob 1984 alice 1985 charles 1992 Name: year, dtype: int64, &#39;children&#39;: charles 0 bob 3 dtype: int64, &#39;hobby&#39;: alice Biking bob Dancing dtype: object} . people_dict[&#39;weight&#39;] . alice 68 bob 83 charles 112 dtype: int64 . 몇가지 알아 두어야 할 것은 다음과 같습니다: . Series는 인덱스를 기반으로 자동으로 정렬됩니다. | 누란된 값은 NaN으로 표현됩니다. | Series 이름은 무시됩니다(&quot;year&quot;란 이름은 삭제됩니다). | DataFrame은 주피터 노트북에서 멋지게 출력됩니다! | . 예상하는 방식으로 열을 참조할 수 있고 Serires 객체가 반환됩니다: . people[&quot;birthyear&quot;] . alice 1985 bob 1984 charles 1992 Name: birthyear, dtype: int64 . 동시에 여러 개의 열을 선택할 수 있습니다: . people[[&quot;birthyear&quot;, &quot;hobby&quot;]] . birthyear hobby . alice 1985 | Biking | . bob 1984 | Dancing | . charles 1992 | NaN | . 열 리스트나 행 인덱스 레이블을 DataFrame 생성자에 전달하면 해당 열과 행으로 채워진 데이터프레임이 반환됩니다. 예를 들면: . people_dict . {&#39;weight&#39;: alice 68 bob 83 charles 112 dtype: int64, &#39;birthyear&#39;: bob 1984 alice 1985 charles 1992 Name: year, dtype: int64, &#39;children&#39;: charles 0 bob 3 dtype: int64, &#39;hobby&#39;: alice Biking bob Dancing dtype: object} . d2 = pd.DataFrame( people_dict, columns=[&quot;birthyear&quot;, &quot;weight&quot;, &quot;height&quot;], index=[&quot;bob&quot;, &quot;alice&quot;, &quot;eugene&quot;] ) d2 . birthyear weight height . bob 1984.0 | 83.0 | NaN | . alice 1985.0 | 68.0 | NaN | . eugene NaN | NaN | NaN | . DataFrame을 만드는 또 다른 편리한 방법은 ndarray나 리스트의 리스트로 모든 값을 생성자에게 전달하고 열 이름과 행 인덱스 레이블을 각기 지정하는 것입니다: . values = [ [1985, np.nan, &quot;Biking&quot;, 68], [1984, 3, &quot;Dancing&quot;, 83], [1992, 0, np.nan, 112] ] d3 = pd.DataFrame( values, columns=[&quot;birthyear&quot;, &quot;children&quot;, &quot;hobby&quot;, &quot;weight&quot;], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;] ) d3 . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3.0 | Dancing | 83 | . charles 1992 | 0.0 | NaN | 112 | . 누락된 값을 지정하려면 np.nan이나 넘파이 마스크 배열을 사용합니다: . masked_array = np.ma.asarray(values, dtype=np.object) masked_array[(0, 2), (1, 2)] = np.ma.masked #NaN값으로 들어감 . d3 = pd.DataFrame( masked_array, columns=[&quot;birthyear&quot;, &quot;children&quot;, &quot;hobby&quot;, &quot;weight&quot;], index=[&quot;alice&quot;, &quot;bob&quot;, &quot;charles&quot;] ) d3 . birthyear children hobby weight . alice 1985 | NaN | Biking | 68 | . bob 1984 | 3 | Dancing | 83 | . charles 1992 | 0 | NaN | 112 | . ndarray 대신에 DataFrame 객체를 전달할 수도 있습니다: . d4 = pd.DataFrame( d3, columns=[&quot;hobby&quot;, &quot;children&quot;], index=[&quot;alice&quot;, &quot;bob&quot;] ) d4 . hobby children . alice Biking | NaN | . bob Dancing | 3 | . 딕셔너리의 딕셔너리(또는 리스트의 리스트)로 DataFrame을 만들 수 있습니다: . people = pd.DataFrame({ &quot;birthyear&quot;: {&quot;alice&quot;:1985, &quot;bob&quot;: 1984, &quot;charles&quot;: 1992}, &quot;hobby&quot;: {&quot;alice&quot;:&quot;Biking&quot;, &quot;bob&quot;: &quot;Dancing&quot;}, &quot;weight&quot;: {&quot;alice&quot;:68, &quot;bob&quot;: 83, &quot;charles&quot;: 112}, &quot;children&quot;: {&quot;bob&quot;: 3, &quot;charles&quot;: 0} }) people . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . charles 1992 | NaN | 112 | 0.0 | . &#47680;&#54000; &#51064;&#45937;&#49905; . 모든 열이 같은 크기의 튜플이면 멀티 인덱스로 인식합니다. 열 인덱스 레이블에도 같은 방식이 적용됩니다. 예를 들면: . d5 = pd.DataFrame( { (&quot;public&quot;, &quot;birthyear&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):1985, (&quot;Paris&quot;,&quot;bob&quot;): 1984, (&quot;London&quot;,&quot;charles&quot;): 1992}, (&quot;public&quot;, &quot;hobby&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):&quot;Biking&quot;, (&quot;Paris&quot;,&quot;bob&quot;): &quot;Dancing&quot;}, (&quot;private&quot;, &quot;weight&quot;): {(&quot;Paris&quot;,&quot;alice&quot;):68, (&quot;Paris&quot;,&quot;bob&quot;): 83, (&quot;London&quot;,&quot;charles&quot;): 112}, (&quot;private&quot;, &quot;children&quot;): {(&quot;Paris&quot;, &quot;alice&quot;):np.nan, (&quot;Paris&quot;,&quot;bob&quot;): 3, (&quot;London&quot;,&quot;charles&quot;): 0} } ) d5 . public private . birthyear hobby weight children . Paris alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . London charles 1992 | NaN | 112 | 0.0 | . 이제 &quot;public&quot; 열을 모두 담은 DataFrame을 손쉽게 만들 수 있습니다: . d5[&quot;public&quot;] . birthyear hobby . Paris alice 1985 | Biking | . bob 1984 | Dancing | . London charles 1992 | NaN | . d5[&quot;public&quot;, &quot;hobby&quot;] # d5[&quot;public&quot;][&quot;hobby&quot;]와 같습니다. . Paris alice Biking bob Dancing London charles NaN Name: (public, hobby), dtype: object . &#47112;&#48296; &#45230;&#52628;&#44592; . d5를 다시 확인해 보죠: . d5 . public private . birthyear hobby weight children . Paris alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . London charles 1992 | NaN | 112 | 0.0 | . 열의 레벨(level)이 2개이고 인덱스 레벨이 2개입니다. droplevel()을 사용해 열 레벨을 낮출 수 있습니다(인덱스도 마찬가지입니다): . d5.columns = d5.columns.droplevel(level = 0) d5 . birthyear hobby weight children . Paris alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . London charles 1992 | NaN | 112 | 0.0 | . #d5.columns = d5.columns.droplevel(level=1) #d5 . public public private private . Paris alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . London charles 1992 | NaN | 112 | 0.0 | . # d5.index = d5.index.droplevel(level = 0) # d5 . public private . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . charles 1992 | NaN | 112 | 0.0 | . &#51204;&#52824; . T 속성을 사용해 열과 인덱스를 바꿀 수 있습니다: . d6 = d5.T d6 . Paris London . alice bob charles . birthyear 1985 | 1984 | 1992 | . hobby Biking | Dancing | NaN | . weight 68 | 83 | 112 | . children NaN | 3.0 | 0.0 | . &#47112;&#48296; &#49828;&#53469;&#44284; &#50616;&#49828;&#53469; . stack() 메서드는 가장 낮은 열 레벨을 가장 낮은 인덱스 뒤에 추가합니다: unstack() 메서드는 가장 낮은 인덱스 레벨을 가장 낮은 열 뒤에 추가합니다: . d7 = d6.stack() d7 . London Paris . birthyear alice NaN | 1985 | . bob NaN | 1984 | . charles 1992 | NaN | . hobby alice NaN | Biking | . bob NaN | Dancing | . weight alice NaN | 68 | . bob NaN | 83 | . charles 112 | NaN | . children bob NaN | 3.0 | . charles 0.0 | NaN | . d7.index . MultiIndex([(&#39;birthyear&#39;, &#39;alice&#39;), (&#39;birthyear&#39;, &#39;bob&#39;), (&#39;birthyear&#39;, &#39;charles&#39;), ( &#39;hobby&#39;, &#39;alice&#39;), ( &#39;hobby&#39;, &#39;bob&#39;), ( &#39;weight&#39;, &#39;alice&#39;), ( &#39;weight&#39;, &#39;bob&#39;), ( &#39;weight&#39;, &#39;charles&#39;), ( &#39;children&#39;, &#39;bob&#39;), ( &#39;children&#39;, &#39;charles&#39;)], ) . NaN 값이 생겼습니다. 이전에 없던 조합이 생겼기 때문입니다(예를 들어 London에 bob이 없었습니다). . unstack()을 호출하면 반대가 됩니다. 여기에서도 많은 NaN 값이 생성됩니다. . d8 = d7.unstack() d8 . London Paris . alice bob charles alice bob charles . birthyear NaN | NaN | 1992 | 1985 | 1984 | NaN | . children NaN | NaN | 0.0 | NaN | 3.0 | NaN | . hobby NaN | NaN | NaN | Biking | Dancing | NaN | . weight NaN | NaN | 112 | 68 | 83 | NaN | . unstack을 다시 호출하면 Series 객체가 만들어 집니다: . d9 = d8.unstack() d9 . London alice birthyear NaN children NaN hobby NaN weight NaN bob birthyear NaN children NaN hobby NaN weight NaN charles birthyear 1992 children 0.0 hobby NaN weight 112 Paris alice birthyear 1985 children NaN hobby Biking weight 68 bob birthyear 1984 children 3.0 hobby Dancing weight 83 charles birthyear NaN children NaN hobby NaN weight NaN dtype: object . stack()과 unstack() 메서드를 사용할 때 스택/언스택할 level을 선택할 수 있습니다. 심지어 한 번에 여러 개의 레벨을 스택/언스택할 수도 있습니다: . d10 = d9.unstack(level = (0,1)) d10 . London Paris . alice bob charles alice bob charles . birthyear NaN | NaN | 1992 | 1985 | 1984 | NaN | . children NaN | NaN | 0.0 | NaN | 3.0 | NaN | . hobby NaN | NaN | NaN | Biking | Dancing | NaN | . weight NaN | NaN | 112 | 68 | 83 | NaN | . &#45824;&#48512;&#48516;&#51032; &#47700;&#49436;&#46300;&#45716; &#49688;&#51221;&#46108; &#48373;&#49324;&#48376;&#51012; &#48152;&#54872;&#54633;&#45768;&#45796; . 눈치챘겠지만 stack()과 unstack() 메서드는 객체를 수정하지 않습니다. 대신 복사본을 만들어 반환합니다. 판다스에 있는 대부분의 메서드들이 이렇게 동작합니다. . Pivot . import pandas._testing as tm def unpivot(frame): N, K = frame.shape data = { &quot;value&quot;: frame.to_numpy().ravel(&quot;F&quot;), &quot;variable&quot;: np.asarray(frame.columns).repeat(N), &quot;date&quot;: np.tile(np.asarray(frame.index), K), } return pd.DataFrame(data, columns=[&quot;date&quot;, &quot;variable&quot;, &quot;value&quot;]) df = unpivot(tm.makeTimeDataFrame(3)) df . date variable value . 0 2000-01-03 | A | 0.956514 | . 1 2000-01-04 | A | -0.711841 | . 2 2000-01-05 | A | 0.496248 | . 3 2000-01-03 | B | 0.092486 | . 4 2000-01-04 | B | -1.625856 | . 5 2000-01-05 | B | 0.801648 | . 6 2000-01-03 | C | 1.164039 | . 7 2000-01-04 | C | -1.194786 | . 8 2000-01-05 | C | -0.080289 | . 9 2000-01-03 | D | -0.842007 | . 10 2000-01-04 | D | -0.934912 | . 11 2000-01-05 | D | 0.711753 | . filtered = df[df[&quot;variable&quot;] == &quot;A&quot;] filtered . date variable value . 0 2000-01-03 | A | 0.162390 | . 1 2000-01-04 | A | -0.994437 | . 2 2000-01-05 | A | 0.747082 | . pivoted = df.pivot(index=&quot;date&quot;, columns=&quot;variable&quot;, values=&quot;value&quot;) pivoted . variable A B C D . date . 2000-01-03 0.956514 | 0.092486 | 1.164039 | -0.842007 | . 2000-01-04 -0.711841 | -1.625856 | -1.194786 | -0.934912 | . 2000-01-05 0.496248 | 0.801648 | -0.080289 | 0.711753 | . df[&quot;value2&quot;] = df[&quot;value&quot;] * 2 pivoted = df.pivot(index=&quot;date&quot;, columns=&quot;variable&quot;) pivoted . value value2 . variable A B C D A B C D . date . 2000-01-03 0.956514 | 0.092486 | 1.164039 | -0.842007 | 1.913028 | 0.184971 | 2.328077 | -1.684013 | . 2000-01-04 -0.711841 | -1.625856 | -1.194786 | -0.934912 | -1.423682 | -3.251711 | -2.389572 | -1.869825 | . 2000-01-05 0.496248 | 0.801648 | -0.080289 | 0.711753 | 0.992497 | 1.603296 | -0.160578 | 1.423505 | . pivoted[&quot;value2&quot;] . variable A B C D . date . 2000-01-03 1.913028 | 0.184971 | 2.328077 | -1.684013 | . 2000-01-04 -1.423682 | -3.251711 | -2.389572 | -1.869825 | . 2000-01-05 0.992497 | 1.603296 | -0.160578 | 1.423505 | . &#54665; &#52280;&#51312;&#54616;&#44592; . people DataFrame으로 돌아가 보죠: . people . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . charles 1992 | NaN | 112 | 0.0 | . loc 속성으로 열 대신 행을 참조할 수 있습니다. DataFrame의 열 이름이 행 인덱스 레이블로 매핑된 Series 객체가 반환됩니다: . people[&#39;birthyear&#39;] . alice 1985 bob 1984 charles 1992 Name: birthyear, dtype: int64 . people.loc[&quot;charles&quot;] . birthyear 1992 hobby NaN weight 112 children 0.0 Name: charles, dtype: object . iloc 속성을 사용해 정수 인덱스로 행을 참조할 수 있습니다: . people.iloc[2] . birthyear 1992 hobby NaN weight 112 children 0.0 Name: charles, dtype: object . 행을 슬라이싱할 수 있으며 DataFrame 객체가 반환됩니다: . people.iloc[1:3] . birthyear hobby weight children . bob 1984 | Dancing | 83 | 3.0 | . charles 1992 | NaN | 112 | 0.0 | . 마자믹으로 불리언 배열을 전달하여 해당하는 행을 가져올 수 있습니다: . people[np.array([True, False, True])] . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . charles 1992 | NaN | 112 | 0.0 | . 불리언 표현식을 사용할 때 아주 유용합니다: . people[people[&quot;birthyear&quot;] &lt; 1990] . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . &#50676; &#52628;&#44032;, &#49325;&#51228; . DataFrame을 Series의 딕셔너리처럼 다룰 수 있습니다. 따라서 다음 같이 쓸 수 있습니다: . people . birthyear hobby weight children . alice 1985 | Biking | 68 | NaN | . bob 1984 | Dancing | 83 | 3.0 | . charles 1992 | NaN | 112 | 0.0 | . people[&quot;age&quot;] = 2018 - people[&quot;birthyear&quot;] # &quot;age&quot; 열을 추가합니다 people[&quot;over 30&quot;] = people[&quot;age&quot;] &gt; 30 # &quot;over 30&quot; 열을 추가합니다 birthyears = people.pop(&quot;birthyear&quot;) #pop은 뽑다, birthyear은 제거된다 del people[&quot;children&quot;] people . hobby weight age over 30 . alice Biking | 68 | 33 | True | . bob Dancing | 83 | 34 | True | . charles NaN | 112 | 26 | False | . birthyears . alice 1985 bob 1984 charles 1992 Name: birthyear, dtype: int64 . weights = {&quot;alice&quot;:68,&quot;bob&quot;:83,&quot;colin&quot;:86,&quot;darwin&quot;:68} . weights.pop(&quot;alice&quot;) . 68 . weights . {&#39;bob&#39;: 83, &#39;colin&#39;: 86, &#39;darwin&#39;: 68} . del weights[&quot;bob&quot;] . weights . {&#39;colin&#39;: 86, &#39;darwin&#39;: 68} . 새로운 열을 추가할 때 행의 개수는 같아야 합니다. 누락된 행은 NaN으로 채워지고 추가적인 행은 무시됩니다: . people[&quot;pets&quot;] = pd.Series({&quot;bob&quot;: 0, &quot;charles&quot;: 5, &quot;eugene&quot;:1}) # alice 누락됨, eugene은 무시됨 people . hobby weight age over 30 pets . alice Biking | 68 | 33 | True | NaN | . bob Dancing | 83 | 34 | True | 0.0 | . charles NaN | 112 | 26 | False | 5.0 | . 새로운 열을 추가할 때 기본적으로 (오른쪽) 끝에 추가됩니다. insert() 메서드를 사용해 다른 곳에 열을 추가할 수 있습니다: . . people.insert(1, &quot;height&quot;, [172, 181, 185]) people . hobby height weight age over 30 pets . alice Biking | 172 | 68 | 33 | True | NaN | . bob Dancing | 181 | 83 | 34 | True | 0.0 | . charles NaN | 185 | 112 | 26 | False | 5.0 | . &#49352;&#47196;&#50868; &#50676; &#54624;&#45817;&#54616;&#44592; . assign() 메서드를 호출하여 새로운 열을 만들 수도 있습니다. 이는 새로운 DataFrame 객체를 반환하며 원본 객체는 변경되지 않습니다: . people.assign( body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2, has_pets = people[&quot;pets&quot;] &gt; 0 ) . hobby height weight age over 30 pets body_mass_index has_pets . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . people[&quot;body_mass_index&quot;] = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100)*2 people . hobby height weight age over 30 pets body_mass_index . alice Biking | 172 | 68 | 33 | True | NaN | 79.069767 | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 91.712707 | . charles NaN | 185 | 112 | 26 | False | 5.0 | 121.081081 | . . 할당문 안에서 만든 열은 접근할 수 없습니다: . try: people.assign( body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2, overweight = people[&quot;body_mass_index&quot;] &gt; 25 ) except KeyError as e: print(&quot;키 에러:&quot;, e) . 키 에러: &#39;body_mass_index&#39; . 해결책은 두 개의 연속된 할당문으로 나누는 것입니다: . # 비효율적 d6 = people.assign(body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2) d6.assign(overweight = d6[&quot;body_mass_index&quot;] &gt; 25) . hobby height weight age over 30 pets body_mass_index overweight . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | True | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . 임시 변수 d6를 만들면 불편합니다. assign() 메서드를 연결하고 싶겠지만 people 객체가 첫 번째 할당문에서 실제로 수정되지 않기 때문에 작동하지 않습니다: . try: (people .assign(body_mass_index = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100) ** 2) .assign(overweight = people[&quot;body_mass_index&quot;] &gt; 25) ) except KeyError as e: print(&quot;키 에러:&quot;, e) . 키 에러: &#39;body_mass_index&#39; . 하지만 걱정하지 마세요. 간단한 방법이 있습니다. assign() 메서드에 함수(전형적으로 lambda 함수)를 전달하면 DataFrame을 매개변수로 이 함수를 호출할 것입니다: . (people .assign(body_mass_index = lambda df: df[&quot;weight&quot;] / (df[&quot;height&quot;] / 100) ** 2) .assign(overweight = lambda df: df[&quot;body_mass_index&quot;] &gt; 25) ) . hobby height weight age over 30 pets body_mass_index overweight . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | True | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . people[&quot;body_mass_index&quot;] = people[&quot;weight&quot;] / (people[&quot;height&quot;] / 100)*2 . 문제가 해결되었군요! . &#54364;&#54788;&#49885; &#54217;&#44032; . 판다스가 제공하는 뛰어난 기능 하나는 표현식 평가입니다. 이는 numexpr 라이브러리에 의존하기 때문에 설치가 되어 있어야 합니다. . people.eval(&quot;weight / (height/100) ** 2 &gt; 25&quot;) . alice False bob True charles True dtype: bool . 할당 표현식도 지원됩니다. inplace=True로 지정하면 수정된 복사본을 만들지 않고 바로 DataFrame을 변경합니다: . people.eval(&quot;body_mass_index = weight / (height/100) ** 2&quot;, inplace=True) people . hobby height weight age over 30 pets body_mass_index . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | . &#39;@&#39;를 접두어로 사용하여 지역 변수나 전역 변수를 참조할 수 있습니다: . overweight_threshold = 30 people.eval(&quot;overweight = body_mass_index &gt; @overweight_threshold&quot;, inplace=True) people . hobby height weight age over 30 pets body_mass_index overweight . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . DataFrame &#53244;&#47532;&#54616;&#44592; . query() 메서드를 사용하면 쿼리 표현식에 기반하여 DataFrame을 필터링할 수 있습니다: . people.query(&quot;age &gt; 30 and pets == 0&quot;) . hobby height weight age over 30 pets body_mass_index overweight . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . people[(people[&quot;age&quot;]&gt;30) &amp; (people[&quot;pets&quot;]==0)] . hobby height weight age over 30 pets body_mass_index overweight . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . mask = (people[&quot;age&quot;]&gt;30) &amp; (people[&quot;pets&quot;]==0) people[mask] . hobby height weight age over 30 pets body_mass_index overweight . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . DataFrame &#51221;&#47148; . sort_index 메서드를 호출하여 DataFrame을 정렬할 수 있습니다. 기본적으로 인덱스 레이블을 기준으로 오름차순으로 행을 정렬합니다. 여기에서는 내림차순으로 정렬해 보죠: . people.sort_index(ascending=False) . hobby height weight age over 30 pets body_mass_index overweight . charles NaN | 185 | 112 | 26 | False | 5.0 | 32.724617 | True | . bob Dancing | 181 | 83 | 34 | True | 0.0 | 25.335002 | False | . alice Biking | 172 | 68 | 33 | True | NaN | 22.985398 | False | . sort_index는 DataFrame의 정렬된 복사본을 반환합니다. people을 직접 수정하려면 inplace 매개변수를 True로 지정합니다. 또한 axis=1로 지정하여 열 대신 행을 정렬할 수 있습니다: . people.sort_index(axis=1, inplace=True) people . age body_mass_index height hobby over 30 overweight pets weight . alice 33 | 22.985398 | 172 | Biking | True | False | NaN | 68 | . bob 34 | 25.335002 | 181 | Dancing | True | False | 0.0 | 83 | . charles 26 | 32.724617 | 185 | NaN | False | True | 5.0 | 112 | . 레이블이 아니라 값을 기준으로 DataFrame을 정렬하려면 sort_values에 정렬하려는 열을 지정합니다: . people.sort_values(by=&quot;age&quot;, inplace=True) people . age body_mass_index height hobby over 30 overweight pets weight . charles 26 | 32.724617 | 185 | NaN | False | True | 5.0 | 112 | . alice 33 | 22.985398 | 172 | Biking | True | False | NaN | 68 | . bob 34 | 25.335002 | 181 | Dancing | True | False | 0.0 | 83 | . DataFrame &#44536;&#47000;&#54532; &#44536;&#47532;&#44592; . Series와 마찬가지로 판다스는 DataFrame 기반으로 멋진 그래프를 손쉽게 그릴 수 있습니다. . 예를 들어 plot 메서드를 호출하여 DataFrame의 데이터에서 선 그래프를 쉽게 그릴 수 있습니다: . people.plot.line(x = &quot;body_mass_index&quot;, y = [&quot;height&quot;, &quot;weight&quot;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b72761dc0&gt; . people.plot(kind = &quot;line&quot;, x = &quot;body_mass_index&quot;, y = [&quot;height&quot;, &quot;weight&quot;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b727e82b0&gt; . 맷플롯립의 함수가 지원하는 다른 매개변수를 사용할 수 있습니다. 예를 들어, 산점도를 그릴 때 맷플롯립의 scatter() 함수의 s 매개변수를 사용해 크기를 지정할 수 있습니다: . people.plot(kind = &quot;scatter&quot;, x = &quot;height&quot;, y = &quot;weight&quot;, s=[40, 120, 200]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b72827100&gt; . df4 = pd.DataFrame( { &quot;a&quot;:np.random.randn(1000)+1, &quot;b&quot;:np.random.randn(1000), &quot;c&quot;:np.random.randn(1000)-1, }, columns = [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;], ) #plt.figure() df4.plot.hist(alpha=0.5) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b7390e940&gt; . df4[&quot;a&quot;].plot.hist(alpha=0.5) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b739a0880&gt; . df = pd.DataFrame(np.random.rand(10,5), columns = [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;]) df.plot.box() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x26b73a264f0&gt; . df = pd.DataFrame(np.random.rand(10,2),columns=[&quot;Col1&quot;,&quot;Col2&quot;]) df[&quot;X&quot;] = pd.Series([&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;,&quot;B&quot;]) df . Col1 Col2 X . 0 0.573811 | 0.904532 | A | . 1 0.456202 | 0.791875 | A | . 2 0.027375 | 0.962137 | A | . 3 0.155703 | 0.116007 | A | . 4 0.646041 | 0.749334 | A | . 5 0.168754 | 0.928601 | B | . 6 0.872871 | 0.535532 | B | . 7 0.559621 | 0.786656 | B | . 8 0.874239 | 0.443472 | B | . 9 0.980164 | 0.649904 | B | . bp = df.boxplot(by=&quot;X&quot;) . 선택할 수 있는 옵션이 많습니다. 판다스 문서의 시각화 페이지에서 마음에 드는 그래프를 찾아 예제 코드를 살펴 보세요. . DataFrame &#50672;&#49328; . DataFrame이 넘파이 배열을 흉내내려는 것은 아니지만 몇 가지 비슷한 점이 있습니다. 예제 DataFrame을 만들어 보죠: . grades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]]) grades = pd.DataFrame(grades_array, columns=[&quot;sep&quot;, &quot;oct&quot;, &quot;nov&quot;], index=[&quot;alice&quot;,&quot;bob&quot;,&quot;charles&quot;,&quot;darwin&quot;]) grades . sep oct nov . alice 8 | 8 | 9 | . bob 10 | 9 | 9 | . charles 4 | 8 | 2 | . darwin 9 | 10 | 10 | . DataFrame에 넘파이 수학 함수를 적용하면 모든 값에 이 함수가 적용됩니다: . np.sqrt(grades) . sep oct nov . alice 2.828427 | 2.828427 | 3.000000 | . bob 3.162278 | 3.000000 | 3.000000 | . charles 2.000000 | 2.828427 | 1.414214 | . darwin 3.000000 | 3.162278 | 3.162278 | . 비슷하게 DataFrame에 하나의 값을 더하면 DataFrame의 모든 원소에 이 값이 더해집니다. 이를 브로드캐스팅이라고 합니다: . grades + 1 . sep oct nov . alice 9 | 9 | 10 | . bob 11 | 10 | 10 | . charles 5 | 9 | 3 | . darwin 10 | 11 | 11 | . 물론 산술 연산(*,/,**...)과 조건 연산(&gt;, ==...)을 포함해 모든 이항 연산에도 마찬가지 입니다: . grades &gt;= 5 . sep oct nov . alice True | True | True | . bob True | True | True | . charles False | True | False | . darwin True | True | True | . DataFrame의 max, sum, mean 같은 집계 연산은 각 열에 적용되어 Series 객체가 반환됩니다: . grades.mean() #default = axis=0 . sep 7.75 oct 8.75 nov 7.50 dtype: float64 . grades.values.mean() . 8.0 . all 메서드도 집계 연산입니다: 모든 값이 True인지 아닌지 확인합니다. 모든 학생의 점수가 5 이상인 월을 찾아 보죠: . (grades &gt; 5).all() #전부다 true인지 아닌지 . NameError Traceback (most recent call last) &lt;ipython-input-1-17fdf982d3e7&gt; in &lt;module&gt; -&gt; 1 (grades &gt; 5).all() #전부다 true인지 아닌지 NameError: name &#39;grades&#39; is not defined . Most of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let&#39;s find out which students had all grades greater than 5: . (grades &gt; 5).all(axis = 1) #가로축 단위로 . alice True bob True charles False darwin True dtype: bool . any 메서드는 하나라도 참이면 True를 반환합니다. 한 번이라도 10점을 받은 사람을 찾아 보죠: . (grades == 10).any(axis = 1) . alice False bob True charles False darwin True dtype: bool . DataFrame에 Series 객체를 더하면 (또는 다른 이항 연산을 수행하면) 판다스는 DataFrame에 있는 모든 행에 이 연산을 브로드캐스팅합니다. 이는 Series 객체가 DataFrame의 행의 개수와 크기가 같을 때만 동작합니다. 예를 들어 DataFrame의 mean(Series 객체)을 빼보죠: . grades - grades.mean() # grades - [7.75, 8.75, 7.50] 와 동일 . sep oct nov . alice 0.25 | -0.75 | 1.5 | . bob 2.25 | 0.25 | 1.5 | . charles -3.75 | -0.75 | -5.5 | . darwin 1.25 | 1.25 | 2.5 | . 모든 9월 성적에서 7.75를 빼고, 10월 성적에서 8.75를 빼고, 11월 성적에서 7.50을 뺍니다. 이는 다음 DataFrame을 빼는 것과 같습니다: . pd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns) . sep oct nov . alice 7.75 | 8.75 | 7.5 | . bob 7.75 | 8.75 | 7.5 | . charles 7.75 | 8.75 | 7.5 | . darwin 7.75 | 8.75 | 7.5 | . 모든 성적의 전체 평균을 빼고 싶다면 다음과 같은 방법을 사용합니다: . grades - grades.values.mean() # 모든 점수에서 전체 평균(8.00)을 뺍니다 . sep oct nov . alice 0.0 | 0.0 | 1.0 | . bob 2.0 | 1.0 | 1.0 | . charles -4.0 | 0.0 | -6.0 | . darwin 1.0 | 2.0 | 2.0 | . &#51088;&#46041; &#51221;&#47148; . Series와 비슷하게 여러 개의 DataFrame에 대한 연산을 수행하면 판다스는 자동으로 행 인덱스 레이블로 정렬하지만 열 이름으로도 정렬할 수 있습니다. 10월부터 12월까지 보너스 포인트를 담은 DataFrame을 만들어 보겠습니다: . grades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]]) grades = pd.DataFrame(grades_array, columns=[&quot;sep&quot;, &quot;oct&quot;, &quot;nov&quot;], index=[&quot;alice&quot;,&quot;bob&quot;,&quot;charles&quot;,&quot;darwin&quot;]) grades . sep oct nov . alice 8 | 8 | 9 | . bob 10 | 9 | 9 | . charles 4 | 8 | 2 | . darwin 9 | 10 | 10 | . bonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]]) bonus_points = pd.DataFrame(bonus_array, columns=[&quot;oct&quot;, &quot;nov&quot;, &quot;dec&quot;], index=[&quot;bob&quot;,&quot;colin&quot;, &quot;darwin&quot;, &quot;charles&quot;]) bonus_points . oct nov dec . bob 0.0 | NaN | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . grades + bonus_points . dec nov oct sep . alice NaN | NaN | NaN | NaN | . bob NaN | NaN | 9.0 | NaN | . charles NaN | 5.0 | 11.0 | NaN | . colin NaN | NaN | NaN | NaN | . darwin NaN | 11.0 | 10.0 | NaN | . 덧셈 연산이 수행되었지만 너무 많은 원소가 NaN이 되었습니다. DataFrame을 정렬할 때 일부 열과 행이 한 쪽에만 있기 때문입니다. 다른 쪽에는 누란되었다고 간주합니다(NaN). NaN에 어떤 수를 더하면 NaN이 됩니다. . &#45572;&#46973;&#46108; &#45936;&#51060;&#53552; &#45796;&#47336;&#44592; . 실제 데이터에서 누락된 데이터를 다루는 경우는 자주 발생합니다. 판다스는 누락된 데이터를 다룰 수 있는 몇 가지 방법을 제공합니다. . 위 데이터에 있는 문제를 해결해 보죠. 예를 들어, 누락된 데이터는 NaN이 아니라 0이 되어야 한다고 결정할 수 있습니다. fillna() 메서드를 사용해 모든 NaN 값을 어떤 값으로 바꿀 수 있습니다: . (grades + bonus_points).fillna(0) . dec nov oct sep . alice 0.0 | 0.0 | 0.0 | 0.0 | . bob 0.0 | 0.0 | 9.0 | 0.0 | . charles 0.0 | 5.0 | 11.0 | 0.0 | . colin 0.0 | 0.0 | 0.0 | 0.0 | . darwin 0.0 | 11.0 | 10.0 | 0.0 | . 9월의 점수를 0으로 만드는 것은 공정하지 않습니다. 누락된 점수는 그대로 두고, 누락된 보너스 포인트는 0으로 바꿀 수 있습니다: . fixed_bonus_points = bonus_points.fillna(0) #NA 값 0으로 바꾸기 fixed_bonus_points.insert(loc = 0, column = &quot;sep&quot;,value = 0) # 누락된 컬럼 만들기 fixed_bonus_points.loc[&quot;alice&quot;] = 0 # 누락된 행 만들기 fixed_bonus_points . sep oct nov dec . bob 0 | 0.0 | 0.0 | 2.0 | . colin 0 | 0.0 | 1.0 | 0.0 | . darwin 0 | 0.0 | 1.0 | 0.0 | . charles 0 | 3.0 | 3.0 | 0.0 | . alice 0 | 0.0 | 0.0 | 0.0 | . grades + fixed_bonus_points . dec nov oct sep . alice NaN | 9.0 | 8.0 | 8.0 | . bob NaN | 9.0 | 9.0 | 10.0 | . charles NaN | 5.0 | 11.0 | 4.0 | . colin NaN | NaN | NaN | NaN | . darwin NaN | 11.0 | 10.0 | 9.0 | . 훨씬 낫네요: 일부 데이터를 꾸며냈지만 덜 불공정합니다. . 누락된 값을 다루는 또 다른 방법은 보간입니다. bonus_points DataFrame을 다시 보죠: . bonus_points . oct nov dec . bob 0.0 | NaN | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . interpolate 메서드를 사용해 보죠. 기본적으로 수직 방향(axis=0)으로 보간합니다. 따라서 수평으로(axis=1)으로 보간하도록 지정합니다. . bonus_points.interpolate(axis=1) . oct nov dec . bob 0.0 | 1.0 | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . bob의 보너스 포인트는 10월에 0이고 12월에 2입니다. 11월을 보간하면 평균 보너스 포인트 1을 얻습니다. colin의 보너스 포인트는 11월에 1이지만 9월에 포인트는 얼마인지 모릅니다. 따라서 보간할 수 없고 10월의 포인트는 그대로 누락된 값으로 남아 있습니다. 이를 해결하려면 보간하기 전에 9월의 보너스 포인트를 0으로 설정해야 합니다. . better_bonus_points = bonus_points.copy() better_bonus_points.insert(0, &quot;sep&quot;, 0) better_bonus_points.loc[&quot;alice&quot;] = 0 better_bonus_points = better_bonus_points.interpolate(axis=1) better_bonus_points . sep oct nov dec . bob 0.0 | 0.0 | 1.0 | 2.0 | . colin 0.0 | 0.5 | 1.0 | 0.0 | . darwin 0.0 | 0.0 | 1.0 | 0.0 | . charles 0.0 | 3.0 | 3.0 | 0.0 | . alice 0.0 | 0.0 | 0.0 | 0.0 | . 좋습니다. 이제 모든 보너스 포인트가 합리적으로 보간되었습니다. 최종 점수를 확인해 보죠: . grades + better_bonus_points . dec nov oct sep . alice NaN | 9.0 | 8.0 | 8.0 | . bob NaN | 10.0 | 9.0 | 10.0 | . charles NaN | 5.0 | 11.0 | 4.0 | . colin NaN | NaN | NaN | NaN | . darwin NaN | 11.0 | 10.0 | 9.0 | . 9월 열이 오른쪽에 추가되었는데 좀 이상합니다. 이는 더하려는 DataFrame이 정확히 같은 열을 가지고 있지 않기 때문입니다(grade DataFrame에는 &quot;dec&quot; 열이 없습니다). 따라서 판다스는 알파벳 순서로 최종 열을 정렬합니다. 이를 해결하려면 덧셈을 하기 전에 누락된 열을 추가하면 됩니다: . grades[&quot;dec&quot;] = np.nan final_grades = grades + better_bonus_points final_grades . sep oct nov dec . alice 8.0 | 8.0 | 9.0 | NaN | . bob 10.0 | 9.0 | 10.0 | NaN | . charles 4.0 | 11.0 | 5.0 | NaN | . colin NaN | NaN | NaN | NaN | . darwin 9.0 | 10.0 | 11.0 | NaN | . 12월과 colin에 대해 할 수 있는 것이 많지 않습니다. 보너스 포인트를 만드는 것이 나쁘지만 점수를 합리적으로 올릴 수는 없습니다(어떤 선생님들은 그럴 수 있지만). dropna() 메서드를 사용해 모두 NaN인 행을 삭제합니다: . final_grades_clean = final_grades.dropna(how=&quot;all&quot;) #axis=0 final_grades_clean . sep oct nov dec . alice 8.0 | 8.0 | 9.0 | NaN | . bob 10.0 | 9.0 | 10.0 | NaN | . charles 4.0 | 11.0 | 5.0 | NaN | . darwin 9.0 | 10.0 | 11.0 | NaN | . 그다음 axis 매개변수를 1로 지정하여 모두 NaN인 열을 삭제합니다: . final_grades_clean = final_grades_clean.dropna(axis=1, how=&quot;all&quot;) final_grades_clean . sep oct nov . alice 8.0 | 8.0 | 9.0 | . bob 10.0 | 9.0 | 10.0 | . charles 4.0 | 11.0 | 5.0 | . darwin 9.0 | 10.0 | 11.0 | . groupby&#47196; &#51665;&#44228;&#54616;&#44592; . SQL과 비슷하게 판다스는 데이터를 그룹핑하고 각 그룹에 대해 연산을 수행할 수 있습니다. . 먼저 그루핑을 위해 각 사람의 데이터를 추가로 만들겠습니다. NaN 값을 어떻게 다루는지 보기 위해 final_grades DataFrame을 다시 사용하겠습니다: . final_grades[&quot;hobby&quot;] = [&quot;Biking&quot;, &quot;Dancing&quot;, np.nan, &quot;Dancing&quot;, &quot;Biking&quot;] final_grades . sep oct nov dec hobby . alice 8.0 | 8.0 | 9.0 | NaN | Biking | . bob 10.0 | 9.0 | 10.0 | NaN | Dancing | . charles 4.0 | 11.0 | 5.0 | NaN | NaN | . colin NaN | NaN | NaN | NaN | Dancing | . darwin 9.0 | 10.0 | 11.0 | NaN | Biking | . hobby로 이 DataFrame을 그룹핑해 보죠: . grouped_grades = final_grades.groupby(&quot;hobby&quot;) grouped_grades . &lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000026B73D710A0&gt; . # grouped_grades . 이제 hobby마다 평균 점수를 계산할 수 있습니다: . grouped_grades.mean() . sep oct nov dec . hobby . Biking 8.5 | 9.0 | 10.0 | NaN | . Dancing 10.0 | 9.0 | 10.0 | NaN | . final_grades.groupby(&quot;hobby&quot;).mean() . sep oct nov dec . hobby . Biking 8.5 | 9.0 | 10.0 | NaN | . Dancing 10.0 | 9.0 | 10.0 | NaN | . 아주 쉽네요! 평균을 계산할 때 NaN 값은 그냥 무시됩니다. . &#54588;&#48391; &#53580;&#51060;&#48660; . 판다스는 스프레드시트와 비슷하 피봇 테이블을 지원하여 데이터를 빠르게 요약할 수 있습니다. 어떻게 동작하는 알아 보기 위해 간단한 DataFrame을 만들어 보죠: . bonus_points.stack().reset_index() #index가 사라짐 . level_0 level_1 0 . 0 bob | oct | 0.0 | . 1 bob | dec | 2.0 | . 2 colin | nov | 1.0 | . 3 colin | dec | 0.0 | . 4 darwin | oct | 0.0 | . 5 darwin | nov | 1.0 | . 6 darwin | dec | 0.0 | . 7 charles | oct | 3.0 | . 8 charles | nov | 3.0 | . 9 charles | dec | 0.0 | . bonus_points . oct nov dec . bob 0.0 | NaN | 2.0 | . colin NaN | 1.0 | 0.0 | . darwin 0.0 | 1.0 | 0.0 | . charles 3.0 | 3.0 | 0.0 | . more_grades = final_grades_clean.stack().reset_index() more_grades.columns = [&quot;name&quot;, &quot;month&quot;, &quot;grade&quot;] more_grades[&quot;bonus&quot;] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0] more_grades . name month grade bonus . 0 alice | sep | 8.0 | NaN | . 1 alice | oct | 8.0 | NaN | . 2 alice | nov | 9.0 | NaN | . 3 bob | sep | 10.0 | 0.0 | . 4 bob | oct | 9.0 | NaN | . 5 bob | nov | 10.0 | 2.0 | . 6 charles | sep | 4.0 | 3.0 | . 7 charles | oct | 11.0 | 3.0 | . 8 charles | nov | 5.0 | 0.0 | . 9 darwin | sep | 9.0 | 0.0 | . 10 darwin | oct | 10.0 | 1.0 | . 11 darwin | nov | 11.0 | 0.0 | . 이제 이 DataFrame에 대해 pd.pivot_table() 함수를 호출하고 name 열로 그룹핑합니다. 기본적으로 pivot_table()은 수치 열의 평균을 계산합니다: . pd.pivot_table(more_grades, index=&quot;name&quot;) . bonus grade . name . alice NaN | 8.333333 | . bob 1.000000 | 9.666667 | . charles 2.000000 | 6.666667 | . darwin 0.333333 | 10.000000 | . 집계 함수를 aggfunc 매개변수로 바꿀 수 있습니다. 또한 집계 대상의 열을 리스트로 지정할 수 있습니다: . pd.pivot_table(more_grades, index=&quot;name&quot;, values=[&quot;grade&quot;,&quot;bonus&quot;], aggfunc=np.max) . bonus grade . name . alice NaN | 9.0 | . bob 2.0 | 10.0 | . charles 3.0 | 11.0 | . darwin 1.0 | 11.0 | . columns 매개변수를 지정하여 수평으로 집계할 수 있고 margins=True로 설정해 각 행과 열에 대해 전체 합을 계산할 수 있습니다: . pd.pivot_table(more_grades, index=&quot;name&quot;, values=&quot;grade&quot;, columns=&quot;month&quot;, margins=True) . month nov oct sep All . name . alice 9.00 | 8.0 | 8.00 | 8.333333 | . bob 10.00 | 9.0 | 10.00 | 9.666667 | . charles 5.00 | 11.0 | 4.00 | 6.666667 | . darwin 11.00 | 10.0 | 9.00 | 10.000000 | . All 8.75 | 9.5 | 7.75 | 8.666667 | . 마지막으로 여러 개의 인덱스나 열 이름을 지정하면 판다스가 다중 레벨 인덱스를 만듭니다: . pd.pivot_table(more_grades, index=(&quot;name&quot;, &quot;month&quot;), margins=True) . bonus grade . name month . alice nov NaN | 9.00 | . oct NaN | 8.00 | . sep NaN | 8.00 | . bob nov 2.000 | 10.00 | . oct NaN | 9.00 | . sep 0.000 | 10.00 | . charles nov 0.000 | 5.00 | . oct 3.000 | 11.00 | . sep 3.000 | 4.00 | . darwin nov 0.000 | 11.00 | . oct 1.000 | 10.00 | . sep 0.000 | 9.00 | . All 1.125 | 8.75 | . &#54632;&#49688; . 큰 DataFrame을 다룰 때 내용을 간단히 요약하는 것이 도움이 됩니다. 판다스는 이를 위한 몇 가지 함수를 제공합니다. 먼저 수치 값, 누락된 값, 텍스트 값이 섞인 큰 DataFrame을 만들어 보죠. 주피터 노트북은 이 DataFrame의 일부만 보여줍니다: . much_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26)) large_df = pd.DataFrame(much_data, columns=list(&quot;ABCDEFGHIJKLMNOPQRSTUVWXYZ&quot;)) large_df[large_df % 16 == 0] = np.nan large_df.insert(3,&quot;some_text&quot;, &quot;Blabla&quot;) large_df . A B C some_text D E F G H I ... Q R S T U V W X Y Z . 0 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 1 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 2 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 3 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 4 44.0 | 55.0 | 88.0 | Blabla | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | ... | 55.0 | 44.0 | 55.0 | 88.0 | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 9995 NaN | NaN | 33.0 | Blabla | 88.0 | 165.0 | 77.0 | 11.0 | 154.0 | 132.0 | ... | NaN | NaN | NaN | 33.0 | 88.0 | 165.0 | 77.0 | 11.0 | 154.0 | 132.0 | . 9996 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 9997 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 9998 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 9999 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 10000 rows × 27 columns . head() 메서드는 처음 5개 행을 반환합니다: . large_df.head() . A B C some_text D E F G H I ... Q R S T U V W X Y Z . 0 NaN | 11.0 | 44.0 | Blabla | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | ... | 11.0 | NaN | 11.0 | 44.0 | 99.0 | NaN | 88.0 | 22.0 | 165.0 | 143.0 | . 1 11.0 | 22.0 | 55.0 | Blabla | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | ... | 22.0 | 11.0 | 22.0 | 55.0 | 110.0 | NaN | 99.0 | 33.0 | NaN | 154.0 | . 2 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 3 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 4 44.0 | 55.0 | 88.0 | Blabla | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | ... | 55.0 | 44.0 | 55.0 | 88.0 | 143.0 | 33.0 | 132.0 | 66.0 | 22.0 | NaN | . 5 rows × 27 columns . 마지막 5개 행을 반환하는 tail() 함수도 있습니다. 원하는 행 개수를 전달할 수도 있습니다: . large_df.tail(n=2) . A B C some_text D E F G H I ... Q R S T U V W X Y Z . 9998 22.0 | 33.0 | 66.0 | Blabla | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | ... | 33.0 | 22.0 | 33.0 | 66.0 | 121.0 | 11.0 | 110.0 | 44.0 | NaN | 165.0 | . 9999 33.0 | 44.0 | 77.0 | Blabla | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | ... | 44.0 | 33.0 | 44.0 | 77.0 | 132.0 | 22.0 | 121.0 | 55.0 | 11.0 | NaN | . 2 rows × 27 columns . info() 메서드는 각 열의 내용을 요약하여 출력합니다: . large_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10000 entries, 0 to 9999 Data columns (total 27 columns): # Column Non-Null Count Dtype -- -- 0 A 8823 non-null float64 1 B 8824 non-null float64 2 C 8824 non-null float64 3 some_text 10000 non-null object 4 D 8824 non-null float64 5 E 8822 non-null float64 6 F 8824 non-null float64 7 G 8824 non-null float64 8 H 8822 non-null float64 9 I 8823 non-null float64 10 J 8823 non-null float64 11 K 8822 non-null float64 12 L 8824 non-null float64 13 M 8824 non-null float64 14 N 8822 non-null float64 15 O 8824 non-null float64 16 P 8824 non-null float64 17 Q 8824 non-null float64 18 R 8823 non-null float64 19 S 8824 non-null float64 20 T 8824 non-null float64 21 U 8824 non-null float64 22 V 8822 non-null float64 23 W 8824 non-null float64 24 X 8824 non-null float64 25 Y 8822 non-null float64 26 Z 8823 non-null float64 dtypes: float64(26), object(1) memory usage: 2.1+ MB . 마지막으로 describe() 메서드는 각 열에 대한 주요 집계 연산을 수행한 결과를 보여줍니다: . Finally, the describe() method gives a nice overview of the main aggregated values over each column: . count: null(NaN)이 아닌 값의 개수 | mean: null이 아닌 값의 평균 | std: null이 아닌 값의 표준 편차 | min: null이 아닌 값의 최솟값 | 25%, 50%, 75%: null이 아닌 값의 25번째, 50번째, 75번째 백분위수 | max: null이 아닌 값의 최댓값 | . large_df.describe() . A B C D E F G H I J ... Q R S T U V W X Y Z . count 8823.000000 | 8824.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8823.000000 | 8823.000000 | ... | 8824.000000 | 8823.000000 | 8824.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8824.000000 | 8824.000000 | 8822.000000 | 8823.000000 | . mean 87.977559 | 87.972575 | 87.987534 | 88.012466 | 87.983791 | 88.007480 | 87.977561 | 88.000000 | 88.022441 | 88.022441 | ... | 87.972575 | 87.977559 | 87.972575 | 87.987534 | 88.012466 | 87.983791 | 88.007480 | 87.977561 | 88.000000 | 88.022441 | . std 47.535911 | 47.535523 | 47.521679 | 47.521679 | 47.535001 | 47.519371 | 47.529755 | 47.536879 | 47.535911 | 47.535911 | ... | 47.535523 | 47.535911 | 47.535523 | 47.521679 | 47.521679 | 47.535001 | 47.519371 | 47.529755 | 47.536879 | 47.535911 | . min 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | ... | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | 11.000000 | . 25% 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | ... | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | 44.000000 | . 50% 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | ... | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | 88.000000 | . 75% 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | ... | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | 132.000000 | . max 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | ... | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | 165.000000 | . 8 rows × 26 columns . &#51200;&#51109; &amp; &#47196;&#46377; . 판다스는 DataFrame를 여러 가지 포맷으로 저장할 수 있습니다. CSV, Excel, JSON, HTML, HDF5, SQL 데이터베이스 같은 포맷이 가능합니다. 예제를 위해 DataFrame을 하나 만들어 보겠습니다: . my_df = pd.DataFrame( [[&quot;Biking&quot;, 68.5, 1985, np.nan], [&quot;Dancing&quot;, 83.1, 1984, 3]], columns=[&quot;hobby&quot;,&quot;weight&quot;,&quot;birthyear&quot;,&quot;children&quot;], index=[&quot;alice&quot;, &quot;bob&quot;] ) my_df . hobby weight birthyear children . alice Biking | 68.5 | 1985 | NaN | . bob Dancing | 83.1 | 1984 | 3.0 | . &#51200;&#51109; . CSV, HTML, JSON로 저장해 보죠: . my_df.to_csv(&quot;my_df.csv&quot;) my_df.to_html(&quot;my_df.html&quot;) my_df.to_json(&quot;my_df.json&quot;) . 저장된 내용을 확인해 보죠: . for filename in (&quot;my_df.csv&quot;, &quot;my_df.html&quot;, &quot;my_df.json&quot;): print(&quot;#&quot;, filename) with open(filename, &quot;rt&quot;) as f: print(f.read()) print() . # my_df.csv ,hobby,weight,birthyear,children alice,Biking,68.5,1985, bob,Dancing,83.1,1984,3.0 # my_df.html &lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt; &lt;thead&gt; &lt;tr style=&#34;text-align: right;&#34;&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;hobby&lt;/th&gt; &lt;th&gt;weight&lt;/th&gt; &lt;th&gt;birthyear&lt;/th&gt; &lt;th&gt;children&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;alice&lt;/th&gt; &lt;td&gt;Biking&lt;/td&gt; &lt;td&gt;68.5&lt;/td&gt; &lt;td&gt;1985&lt;/td&gt; &lt;td&gt;NaN&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt;bob&lt;/th&gt; &lt;td&gt;Dancing&lt;/td&gt; &lt;td&gt;83.1&lt;/td&gt; &lt;td&gt;1984&lt;/td&gt; &lt;td&gt;3.0&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; # my_df.json {&#34;hobby&#34;:{&#34;alice&#34;:&#34;Biking&#34;,&#34;bob&#34;:&#34;Dancing&#34;},&#34;weight&#34;:{&#34;alice&#34;:68.5,&#34;bob&#34;:83.1},&#34;birthyear&#34;:{&#34;alice&#34;:1985,&#34;bob&#34;:1984},&#34;children&#34;:{&#34;alice&#34;:null,&#34;bob&#34;:3.0}} . 인덱스는 (이름 없이) CSV 파일의 첫 번째 열에 저장되었습니다. HTML에서는 &lt;th&gt; 태그와 JSON에서는 키로 저장되었습니다. . 다른 포맷으로 저장하는 것도 비슷합니다. 하지만 일부 포맷은 추가적인 라이브러리 설치가 필요합니다. 예를 들어, 엑셀로 저장하려면 openpyxl 라이브러리가 필요합니다: . try: my_df.to_excel(&quot;my_df.xlsx&quot;, sheet_name=&#39;People&#39;) except ImportError as e: print(e) . &#47196;&#46377; . CSV 파일을 DataFrame으로 로드해 보죠: . my_df_loaded = pd.read_csv(&quot;my_df.csv&quot;, index_col=0) my_df_loaded . hobby weight birthyear children . alice Biking | 68.5 | 1985 | NaN | . bob Dancing | 83.1 | 1984 | 3.0 | . 예상할 수 있듯이 read_json, read_html, read_excel 함수도 있습니다. 인터넷에서 데이터를 바로 읽을 수도 있습니다. 예를 들어 깃허브에서 1,000개의 U.S. 도시를 로드해 보죠: . us_cities = None try: csv_url = &quot;https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv&quot; us_cities = pd.read_csv(csv_url, index_col=0) us_cities = us_cities.head() except IOError as e: print(e) us_cities . State Population lat lon . City . Marysville Washington | 63269 | 48.051764 | -122.177082 | . Perris California | 72326 | 33.782519 | -117.228648 | . Cleveland Ohio | 390113 | 41.499320 | -81.694361 | . Worcester Massachusetts | 182544 | 42.262593 | -71.802293 | . Columbia South Carolina | 133358 | 34.000710 | -81.034814 | . 이외에도 많은 옵션이 있습니다. 특히 datetime 포맷에 관련된 옵션이 많습니다. 더 자세한 내용은 온라인 문서를 참고하세요. . DataFrame &#54633;&#52824;&#44592; . SQL &#51312;&#51064; . 판다스의 강력한 기능 중 하나는 DataFrame에 대해 SQL 같은 조인(join)을 수행할 수 있는 것입니다. 여러 종류의 조인이 지원됩니다. 이너 조인(inner join), 레프트/라이트 아우터 조인(left/right outer join), 풀 조인(full join)입니다. 이에 대해 알아 보기 위해 간단한 DataFrame을 만들어 보죠: . city_loc = pd.DataFrame( [ [&quot;CA&quot;, &quot;San Francisco&quot;, 37.781334, -122.416728], [&quot;NY&quot;, &quot;New York&quot;, 40.705649, -74.008344], [&quot;FL&quot;, &quot;Miami&quot;, 25.791100, -80.320733], [&quot;OH&quot;, &quot;Cleveland&quot;, 41.473508, -81.739791], [&quot;UT&quot;, &quot;Salt Lake City&quot;, 40.755851, -111.896657] ], columns=[&quot;state&quot;, &quot;city&quot;, &quot;lat&quot;, &quot;lng&quot;]) city_loc . state city lat lng . 0 CA | San Francisco | 37.781334 | -122.416728 | . 1 NY | New York | 40.705649 | -74.008344 | . 2 FL | Miami | 25.791100 | -80.320733 | . 3 OH | Cleveland | 41.473508 | -81.739791 | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | . city_pop = pd.DataFrame( [ [808976, &quot;San Francisco&quot;, &quot;California&quot;], [8363710, &quot;New York&quot;, &quot;New-York&quot;], [413201, &quot;Miami&quot;, &quot;Florida&quot;], [2242193, &quot;Houston&quot;, &quot;Texas&quot;] ], index=[3,4,5,6], columns=[&quot;population&quot;, &quot;city&quot;, &quot;state&quot;]) city_pop . population city state . 3 808976 | San Francisco | California | . 4 8363710 | New York | New-York | . 5 413201 | Miami | Florida | . 6 2242193 | Houston | Texas | . 이제 merge() 함수를 사용해 이 DataFrame을 조인해 보죠: . pd.merge(left=city_loc, right=city_pop, on=&quot;city&quot;) . state_x city lat lng population state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976 | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710 | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201 | Florida | . 두 DataFrame은 state란 이름의 열을 가지고 있으므로 state_x와 state_y로 이름이 바뀌었습니다. . 또한 Cleveland, Salt Lake City, Houston은 두 DataFrame에 모두 존재하지 않기 때문에 삭제되었습니다. SQL의 INNER JOIN과 동일합니다. 도시를 삭제하지 않고 NaN으로 채우는 FULL OUTER JOIN을 원하면 how=&quot;outer&quot;로 지정합니다: . all_cities = pd.merge(left=city_loc, right=city_pop, on=&quot;city&quot;, how=&quot;outer&quot;) all_cities . state_x city lat lng population state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976.0 | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710.0 | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201.0 | Florida | . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | NaN | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | NaN | NaN | . 5 NaN | Houston | NaN | NaN | 2242193.0 | Texas | . 물론 LEFT OUTER JOIN은 how=&quot;left&quot;로 지정할 수 있습니다. 왼쪽의 DataFrame에 있는 도시만 남습니다. 비슷하게 how=&quot;right&quot;는 오른쪽 DataFrame에 있는 도시만 결과에 남습니다. 예를 들면: . pd.merge(left=city_loc, right=city_pop, on=&quot;city&quot;, how=&quot;right&quot;) . state_x city lat lng population state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976 | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710 | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201 | Florida | . 3 NaN | Houston | NaN | NaN | 2242193 | Texas | . 조인할 키가 DataFrame 인덱스라면 left_index=True나 right_index=True로 지정해야 합니다. 키 열의 이름이 다르면 left_on과 right_on을 사용합니다. 예를 들어: . city_pop2 = city_pop.copy() city_pop2.columns = [&quot;population&quot;, &quot;name&quot;, &quot;state&quot;] pd.merge(left=city_loc, right=city_pop2, left_on=&quot;city&quot;, right_on=&quot;name&quot;) . state_x city lat lng population name state_y . 0 CA | San Francisco | 37.781334 | -122.416728 | 808976 | San Francisco | California | . 1 NY | New York | 40.705649 | -74.008344 | 8363710 | New York | New-York | . 2 FL | Miami | 25.791100 | -80.320733 | 413201 | Miami | Florida | . &#50672;&#44208; . DataFrame을 조인하는 대신 그냥 연결할 수도 있습니다. concat() 함수가 하는 일입니다: . result_concat = pd.concat([city_loc, city_pop]) result_concat . state city lat lng population . 0 CA | San Francisco | 37.781334 | -122.416728 | NaN | . 1 NY | New York | 40.705649 | -74.008344 | NaN | . 2 FL | Miami | 25.791100 | -80.320733 | NaN | . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | NaN | . 3 California | San Francisco | NaN | NaN | 808976.0 | . 4 New-York | New York | NaN | NaN | 8363710.0 | . 5 Florida | Miami | NaN | NaN | 413201.0 | . 6 Texas | Houston | NaN | NaN | 2242193.0 | . 이 연산은 (행을 따라) 수직적으로 데이터를 연결하고 (열을 따라) 수평으로 연결하지 않습니다. 이 예에서 동일한 인덱스를 가진 행이 있습니다(예를 들면 3). 판다스는 이를 우아하게 처리합니다: . result_concat.loc[3] . state city lat lng population . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | . 3 California | San Francisco | NaN | NaN | 808976.0 | . 또는 인덱스를 무시하도록 설정할 수 있습니다: . pd.concat([city_loc, city_pop], ignore_index=True) . state city lat lng population . 0 CA | San Francisco | 37.781334 | -122.416728 | NaN | . 1 NY | New York | 40.705649 | -74.008344 | NaN | . 2 FL | Miami | 25.791100 | -80.320733 | NaN | . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | NaN | . 5 California | San Francisco | NaN | NaN | 808976.0 | . 6 New-York | New York | NaN | NaN | 8363710.0 | . 7 Florida | Miami | NaN | NaN | 413201.0 | . 8 Texas | Houston | NaN | NaN | 2242193.0 | . 한 DataFrame에 열이 없을 때 NaN이 채워져 있는 것처럼 동작합니다. join=&quot;inner&quot;로 설정하면 양쪽의 DataFrame에 존재하는 열만 반환됩니다: . pd.concat([city_loc, city_pop], join=&quot;inner&quot;) . state city . 0 CA | San Francisco | . 1 NY | New York | . 2 FL | Miami | . 3 OH | Cleveland | . 4 UT | Salt Lake City | . 3 California | San Francisco | . 4 New-York | New York | . 5 Florida | Miami | . 6 Texas | Houston | . axis=1로 설정하면 DataFrame을 수직이 아니라 수평으로 연결할 수 있습니다: . pd.concat([city_loc, city_pop], axis=1) . state city lat lng population city state . 0 CA | San Francisco | 37.781334 | -122.416728 | NaN | NaN | NaN | . 1 NY | New York | 40.705649 | -74.008344 | NaN | NaN | NaN | . 2 FL | Miami | 25.791100 | -80.320733 | NaN | NaN | NaN | . 3 OH | Cleveland | 41.473508 | -81.739791 | 808976.0 | San Francisco | California | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | 8363710.0 | New York | New-York | . 5 NaN | NaN | NaN | NaN | 413201.0 | Miami | Florida | . 6 NaN | NaN | NaN | NaN | 2242193.0 | Houston | Texas | . 이 경우 인덱스가 잘 정렬되지 않기 때문에 의미가 없습니다(예를 들어 Cleveland와 San Francisco의 인덱스 레이블이 3이기 때문에 동일한 행에 놓여 있습니다). 이 DataFrame을 연결하기 전에 도시로 인덱스를 재설정해 보죠: . pd.concat([city_loc.set_index(&quot;city&quot;), city_pop.set_index(&quot;city&quot;)], axis=1) . state lat lng population state . city . San Francisco CA | 37.781334 | -122.416728 | 808976.0 | California | . New York NY | 40.705649 | -74.008344 | 8363710.0 | New-York | . Miami FL | 25.791100 | -80.320733 | 413201.0 | Florida | . Cleveland OH | 41.473508 | -81.739791 | NaN | NaN | . Salt Lake City UT | 40.755851 | -111.896657 | NaN | NaN | . Houston NaN | NaN | NaN | 2242193.0 | Texas | . FULL OUTER JOIN을 수행한 것과 비슷합니다. 하지만 state 열이 state_x와 state_y로 바뀌지 않았고 city 열이 인덱스가 되었습니다. . append() 메서드는 DataFrame을 수직으로 연결하는 단축 메서드입니다: . city_loc.append(city_pop) . state city lat lng population . 0 CA | San Francisco | 37.781334 | -122.416728 | NaN | . 1 NY | New York | 40.705649 | -74.008344 | NaN | . 2 FL | Miami | 25.791100 | -80.320733 | NaN | . 3 OH | Cleveland | 41.473508 | -81.739791 | NaN | . 4 UT | Salt Lake City | 40.755851 | -111.896657 | NaN | . 3 California | San Francisco | NaN | NaN | 808976.0 | . 4 New-York | New York | NaN | NaN | 8363710.0 | . 5 Florida | Miami | NaN | NaN | 413201.0 | . 6 Texas | Houston | NaN | NaN | 2242193.0 | . 판다스의 다른 메서드와 마찬가지로 append() 메서드는 실제 city_loc을 수정하지 않습니다. 복사본을 만들어 수정한 다음 반환합니다. . &#48276;&#51452; . 범주로 표현된 값을 가진 경우가 흔합니다. 예를 들어 1은 여성, 2는 남성이거나 &quot;A&quot;는 좋은 것, &quot;B&quot;는 평균, &quot;C&quot;는 나쁜 것 등입니다. 범주형 값을 읽기 힘들고 다루기 번거롭습니다. 하지만 판다스에서는 간단합니다. 예를 들기 위해 앞서 만든 city_pop DataFrame에 범주를 표현하는 열을 추가해 보겠습니다: . city_eco = city_pop.copy() city_eco[&quot;eco_code&quot;] = [17, 17, 34, 20] city_eco . population city state eco_code . 3 808976 | San Francisco | California | 17 | . 4 8363710 | New York | New-York | 17 | . 5 413201 | Miami | Florida | 34 | . 6 2242193 | Houston | Texas | 20 | . 이제 eco_code열은 의미없는 코드입니다. 이를 바꿔 보죠. 먼저 eco_code를 기반으로 새로운 범주형 열을 만듭니다: . city_eco[&quot;economy&quot;] = city_eco[&quot;eco_code&quot;].astype(&#39;category&#39;) city_eco[&quot;economy&quot;].cat.categories . Int64Index([17, 20, 34], dtype=&#39;int64&#39;) . 의미있는 이름을 가진 범주를 지정할 수 있습니다: . city_eco[&quot;economy&quot;].cat.categories = [&quot;Finance&quot;, &quot;Energy&quot;, &quot;Tourism&quot;] city_eco . population city state eco_code economy . 3 808976 | San Francisco | California | 17 | Finance | . 4 8363710 | New York | New-York | 17 | Finance | . 5 413201 | Miami | Florida | 34 | Tourism | . 6 2242193 | Houston | Texas | 20 | Energy | . 범주형 값은 알파벳 순서가 아니라 범주형 순서로 정렬합니다: . city_eco.sort_values(by=&quot;economy&quot;, ascending=False) . population city state eco_code economy . 5 413201 | Miami | Florida | 34 | Tourism | . 6 2242193 | Houston | Texas | 20 | Energy | . 3 808976 | San Francisco | California | 17 | Finance | . 4 8363710 | New York | New-York | 17 | Finance | . &#44536; &#45796;&#51020;&#50644;? . 이제 알았겠지만 판다스는 매우 커다란 라이브러리이고 기능이 많습니다. 가장 중요한 기능들을 둘러 보았지만 빙산의 일각일 뿐입니다. 더 많은 것을 익히려면 실전 데이터로 직접 실습해 보는 것이 제일 좋습니다. 판다스의 훌륭한 문서와 쿡북을 보는 것도 좋습니다. .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/jupyter/python/2022/03/16/pandas.html",
            "relUrl": "/jupyter/python/2022/03/16/pandas.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Numpy 기본",
            "content": "도구 - 넘파이(NumPy) . *넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.&quot; . 구글 코랩에서 실행하기 | &#48176;&#50676; &#49373;&#49457; . numpy를 임포트해 보죠. 대부분의 사람들이 np로 알리아싱하여 임포트합니다: . import numpy as np . np.zeros . zeros 함수는 0으로 채워진 배열을 만듭니다: . np.zeros(5) . array([0., 0., 0., 0., 0.]) . 2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 $3 times 4$ 크기의 행렬입니다: . np.zeros((3,4)) . array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . &#50857;&#50612; . 넘파이에서 각 차원을 축(axis) 이라고 합니다 | 축의 개수를 랭크(rank) 라고 합니다. 예를 들어, 위의 $3 times 4$ 행렬은 랭크 2인 배열입니다(즉 2차원입니다). | 첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다. | . | 배열의 축 길이를 배열의 크기(shape)라고 합니다. 예를 들어, 위 행렬의 크기는 (3, 4)입니다. | 랭크는 크기의 길이와 같습니다. | . | 배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, $3 times 4=12$). | . a = np.zeros((3,4)) a . array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . a.shape . (3, 4) . a.ndim # len(a.shape)와 같습니다 #차원 . 2 . a.size #전체 개수 . 12 . N-&#52264;&#50896; &#48176;&#50676; . 임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다: . np.zeros((2,3,4)) . array([[[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]], [[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]]) . &#48176;&#50676; &#53440;&#51077; . 넘파이 배열의 타입은 ndarray입니다: . type(np.zeros((3,4))) . numpy.ndarray . np.ones . ndarray를 만들 수 있는 넘파이 함수가 많습니다. . 다음은 1로 채워진 $3 times 4$ 크기의 행렬입니다: . np.ones((3,4)) . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . np.full . 주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 $3 times 4$ 크기의 행렬입니다. . np.full((3,4), np.pi) . array([[3.14159265, 3.14159265, 3.14159265, 3.14159265], [3.14159265, 3.14159265, 3.14159265, 3.14159265], [3.14159265, 3.14159265, 3.14159265, 3.14159265]]) . np.full((3,4),1) . array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]) . np.empty . 초기화되지 않은 $2 times 3$ 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다): . np.empty((2,3)) . array([[0., 0., 0.], [0., 0., 0.]]) . np.array . array 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다: . . np.array([[1,2,3,4], [10, 20, 30, 40]]) # np.array(a) . array([[ 1, 2, 3, 4], [10, 20, 30, 40]]) . np.arange . 파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다: . np.arange(1, 5) . array([1, 2, 3, 4]) . np.arange(2,4) . array([2, 3]) . float type도 가능. 부동 소수도 가능합니다: . np.arange(1.0, 5.0) . array([1., 2., 3., 4.]) . 파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다: . np.arange(1, 5, 0.5) #step = 0.5 . array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5]) . 부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다: . print(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다. print(np.arange(0, 5/3, 0.333333333)) print(np.arange(0, 5/3, 0.333333334)) . [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] [0. 0.33333333 0.66666667 1. 1.33333334] . np.linspace . 이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다): . print(np.linspace(0, 5/3, 6)) # 6 은 간격이 아닌 개수이다. . [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] . for loops&#47484; &#49324;&#50857;&#54616;&#51648; &#50506;&#44256; &#51204;&#52404; array&#50640; &#45824;&#54620; &#50672;&#49328; &#49688;&#54665;&#51060; &#44032;&#45733;&#54633;&#45768;&#45796;. . &#54217;&#44512;&#51201;&#51004;&#47196; Numpy-based &#50508;&#44256;&#47532;&#51608;&#51008; 10~100&#48176; &#51221;&#46020; &#49549;&#46020;&#44032; &#45908; &#48736;&#47476;&#44256; &#51201;&#51008; &#47700;&#47784;&#47532;&#47484; &#49324;&#50857;&#54633;&#45768;&#45796;. . a = [1,2,3] y = [x * 2 for x in a] y . [2, 4, 6] . a = [1,2,3] y = [] for x in a: y.append(x*2) y . [2, 4, 6] . a = [1,2,3] np.array(a)*2 . array([2, 4, 6]) . my_arr = np.arange(1000000) my_list = list(range(1000000)) %time for _ in range(10): my_arr2 = my_arr * 2 %time for _ in range(10): my_list2 = [x * 2 for x in my_list] . Wall time: 61.2 ms Wall time: 2.34 s . For loop&#47484; &#46028;&#47540; &#46412;&#51032; &#49549;&#46020; &#48708;&#44368; . import sys size = 10 #2 %timeit for x in range(size): x**2 #3 #avoid this %timeit for x in np.arange(size): x**2 #1 #use this %timeit np.arange(size) **2 . 5.46 µs ± 889 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) 9.31 µs ± 461 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) 2.86 µs ± 397 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) . np.rand&#50752; np.randn . 넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 $3 times 4$ 행렬을 초기화합니다: . np.random.rand(3,4) . array([[0.65526043, 0.80498314, 0.22479524, 0.46524575], [0.41151092, 0.68156667, 0.62349229, 0.81872836], [0.79738128, 0.01164764, 0.35223684, 0.53457608]]) . 다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 $3 times 4$ 행렬입니다: . np.random.randn(3,4) . array([[-0.66627161, -0.26008568, 0.93423287, 2.41279496], [-0.86771483, 0.06933125, 0.65375341, -1.03246611], [-1.19211741, 1.04805127, 0.63491485, 0.32639203]]) . 이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요): . %matplotlib inline import matplotlib.pyplot as plt . plt.hist(np.random.rand(100000), density=True, bins=100, histtype=&quot;step&quot;, color=&quot;blue&quot;, label=&quot;rand&quot;) plt.hist(np.random.randn(100000), density=True, bins=100, histtype=&quot;step&quot;, color=&quot;red&quot;, label=&quot;randn&quot;) plt.axis([-2.5, 2.5, 0, 1.1]) plt.legend(loc = &quot;upper left&quot;) plt.title(&quot;Random distributions&quot;) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Density&quot;) plt.show() . np.fromfunction . 함수를 사용하여 ndarray를 초기화할 수도 있습니다: . def my_function(z, y, x): return x + 10 * y + 100 * z np.fromfunction(my_function, (3, 2, 10)) . array([[[ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], [ 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]], [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.], [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]], [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.], [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]]) . def my_function(z, y, x): return x + 50 * y + 50 * z np.fromfunction(my_function, (5, 2, 10)) # for x in range(10): # for y in range(2): # for z in range(5): # x,y,z . array([[[ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], [ 50., 51., 52., 53., 54., 55., 56., 57., 58., 59.]], [[ 50., 51., 52., 53., 54., 55., 56., 57., 58., 59.], [100., 101., 102., 103., 104., 105., 106., 107., 108., 109.]], [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.], [150., 151., 152., 153., 154., 155., 156., 157., 158., 159.]], [[150., 151., 152., 153., 154., 155., 156., 157., 158., 159.], [200., 201., 202., 203., 204., 205., 206., 207., 208., 209.]], [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.], [250., 251., 252., 253., 254., 255., 256., 257., 258., 259.]]]) . 넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다: . [[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] [[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.] [ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]] . 위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다. . &#48176;&#50676; &#45936;&#51060;&#53552; . dtype . 넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다: . c = np.arange(1, 5) print(c.dtype, c) . int32 [1 2 3 4] . c = np.arange(1.0, 5.0) print(c.dtype, c) . float64 [1. 2. 3. 4.] . 넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다: . d = np.arange(1, 5, dtype=np.float64) print(d.dtype, d) . float64 [1. 2. 3. 4.] . d = np.arange(1, 5, dtype=np.complex64) print(d.dtype, d) . complex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j] . 가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요. . itemsize . itemsize 속성은 각 아이템의 크기(바이트)를 반환합니다: . e = np.arange(1, 5, dtype=np.complex64) e.itemsize . 8 . data &#48260;&#54140; . 배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요). . f = np.array([[1,2],[1000, 2000]], dtype=np.int32) f.data . &lt;memory at 0x00000208FF5D0040&gt; . 파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다. . if (hasattr(f.data, &quot;tobytes&quot;)): data_bytes = f.data.tobytes() # python 3 else: data_bytes = memoryview(f.data).tobytes() # python 2 data_bytes . b&#39; x01 x00 x00 x00 x02 x00 x00 x00 xe8 x03 x00 x00 xd0 x07 x00 x00&#39; . 여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다. . &#48176;&#50676; &#53356;&#44592; &#48320;&#44221; . &#51088;&#49888;&#51012; &#48320;&#44221; . ndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다. . g = np.arange(24) print(g) print(&quot;랭크:&quot;, g.ndim) . [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] 랭크: 1 . g.shape . (24,) . g.shape = (6, 4) print(g) print(&quot;랭크:&quot;, g.ndim) . [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15] [16 17 18 19] [20 21 22 23]] 랭크: 2 . g.shape = (2, 3, 4) print(g) print(&quot;랭크:&quot;, g.ndim) . [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] 랭크: 3 . reshape . reshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다. . g2 = g.reshape(4,6) print(g2) print(&quot;랭크:&quot;, g2.ndim) . [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] 랭크: 2 . g[0,0,0] = 10 g2 . array([[10, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]) . 행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요). . g2[1, 2] = 999 g2 . array([[ 10, 1, 2, 3, 4, 5], [ 6, 7, 999, 9, 10, 11], [ 12, 13, 14, 15, 16, 17], [ 18, 19, 20, 21, 22, 23]]) . 이에 상응하는 g의 원소도 수정됩니다. . g . array([[[ 10, 1, 2, 3], [ 4, 5, 6, 7], [999, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]]) . 완전히 다른 공간에 값만 같게 복사를 하고 싶다면 copy를 사용. 이렇게 할 경우 두 객체는 독립적인 객체로 존재함 . g2 = g.copy() . ravel . 마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다: . g.ravel() . array([ 10, 1, 2, 3, 4, 5, 6, 7, 999, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . &#49328;&#49696; &#50672;&#49328; . 일반적인 산술 연산자(+, -, *, /, //, ** 등)는 모두 ndarray와 사용할 수 있습니다. 이 연산자는 원소별로 적용됩니다: . a = np.array([14, 23, 32, 41]) b = np.array([5, 4, 3, 2]) print(&quot;a + b =&quot;, a + b) print(&quot;a - b =&quot;, a - b) print(&quot;a * b =&quot;, a * b) print(&quot;a / b =&quot;, a / b) print(&quot;a // b =&quot;, a // b) #몫 print(&quot;a % b =&quot;, a % b) #나머지 print(&quot;a ** b =&quot;, a ** b) . a + b = [19 27 35 43] a - b = [ 9 19 29 39] a * b = [70 92 96 82] a / b = [ 2.8 5.75 10.66666667 20.5 ] a // b = [ 2 5 10 20] a % b = [4 3 2 1] a ** b = [537824 279841 32768 1681] . 여기 곱셈은 행렬 곱셈이 아닙니다. 행렬 연산은 아래에서 설명합니다. . 배열의 크기는 같아야 합니다. 그렇지 않으면 넘파이가 브로드캐스팅 규칙을 적용합니다. . &#48652;&#47196;&#46300;&#52880;&#49828;&#54021; . 일반적으로 넘파이는 동일한 크기의 배열을 기대합니다. 그렇지 않은 상황에는 브로드캐스팅 규칙을 적용합니다: . &#44508;&#52825; 1 . 배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다. . h = np.arange(5).reshape(1, 1, 5) h . array([[[0, 1, 2, 3, 4]]]) . 여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다! . h + [10, 20, 30, 40, 50] # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]] . array([[[10, 21, 32, 43, 54]]]) . &#44508;&#52825; 2 . 특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다. . k = np.arange(6).reshape(2, 3) k . array([[0, 1, 2], [3, 4, 5]]) . (2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다: . k + [[100], [200]] # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]] . array([[100, 101, 102], [203, 204, 205]]) . 규칙 1과 2를 합치면 다음과 같이 동작합니다: . k + [100, 200, 300] # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]] . array([[100, 201, 302], [103, 204, 305]]) . 또 매우 간단히 다음 처럼 해도 됩니다: . k + 1000 # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]] . array([[1000, 1001, 1002], [1003, 1004, 1005]]) . &#44508;&#52825; 3 . 규칙 1 &amp; 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다. . k . array([[0, 1, 2], [3, 4, 5]]) . try: k + [33, 44] except ValueError as e: print(e) . operands could not be broadcast together with shapes (2,3) (2,) . 브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요. . a = np.array([0.0,10.0,20.0,30.0]) a = a[:, np.newaxis] #차원을 추가해주는 함수 a . array([[ 0.], [10.], [20.], [30.]]) . &#50629;&#52880;&#49828;&#54021; . dtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다. . k1 = np.arange(0, 5, dtype=np.uint8) print(k1.dtype, k1) . uint8 [0 1 2 3 4] . k2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8) print(k2.dtype, k2) . int16 [ 5 7 9 11 13] . 모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다. . k3 = k1 + 1.5 print(k3.dtype, k3) . float64 [1.5 2.5 3.5 4.5 5.5] . &#51312;&#44148; &#50672;&#49328;&#51088; . 조건 연산자도 원소별로 적용됩니다: . m = np.array([20, -5, 30, 40]) m &lt; [15, 16, 35, 36] . array([False, True, True, False]) . 브로드캐스팅을 사용합니다: . m &lt; 25 # m &lt; [25, 25, 25, 25] 와 동일 . array([ True, True, False, False]) . 불리언 인덱싱과 함께 사용하면 아주 유용합니다(아래에서 설명하겠습니다). . m[m &lt; 25] . array([20, -5]) . &#49688;&#54617; &#54632;&#49688;&#50752; &#53685;&#44228; &#54632;&#49688; . ndarray에서 사용할 수 있는 수학 함수와 통계 함수가 많습니다. . ndarray &#47700;&#49436;&#46300; . 일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면: . a = &quot;data_mining&quot; . a.upper() . &#39;DATA_MINING&#39; . a = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) print(a) print(&quot;평균 =&quot;, a.mean()) . [[-2.5 3.1 7. ] [10. 11. 12. ]] 평균 = 6.766666666666667 . 이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다. . 다음은 유용한 ndarray 메서드입니다: . for func in (a.min, a.max, a.sum, a.prod, a.std, a.var): print(func.__name__, &quot;=&quot;, func()) . min = 14 max = 41 sum = 110 prod = 422464 std = 10.062305898749054 var = 101.25 . 이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면: . c=np.arange(24).reshape(2,3,4) c . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) . c.sum(axis=0) # 첫 번째 축을 따라 더함, 결과는 3x4 배열 # 첫 번째 축 = reshape(2,3,4)의 2 . array([[12, 14, 16, 18], [20, 22, 24, 26], [28, 30, 32, 34]]) . c.sum(axis=1) # 두 번째 축을 따라 더함, 결과는 2x4 배열 . array([[12, 15, 18, 21], [48, 51, 54, 57]]) . c.sum(axis=2) # 세 번째 축을 따라 더함, 결과는 2x3 배열 . array([[ 6, 22, 38], [54, 70, 86]]) . 여러 축에 대해서 더할 수도 있습니다: . c.sum(axis=(0,2)) # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열 . array([ 60, 92, 124]) . 0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23 . (60, 92, 124) . a = np.arange(1,7).reshape(2,3) a.sum(axis=1) . array([ 6, 15]) . &#51068;&#48152; &#54632;&#49688; . 넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다: . a = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) np.square(a) . array([[ 6.25, 9.61, 49. ], [100. , 121. , 144. ]]) . 다음은 유용한 단항 일반 함수들입니다: . print(&quot;원본 ndarray&quot;) print(a) for func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos): print(&quot; n&quot;, func.__name__) print(func(a)) . 원본 ndarray [[-2.5 3.1 7. ] [10. 11. 12. ]] absolute [[ 2.5 3.1 7. ] [10. 11. 12. ]] sqrt [[ nan 1.76068169 2.64575131] [3.16227766 3.31662479 3.46410162]] exp [[8.20849986e-02 2.21979513e+01 1.09663316e+03] [2.20264658e+04 5.98741417e+04 1.62754791e+05]] log [[ nan 1.13140211 1.94591015] [2.30258509 2.39789527 2.48490665]] sign [[-1. 1. 1.] [ 1. 1. 1.]] ceil [[-2. 4. 7.] [10. 11. 12.]] modf (array([[-0.5, 0.1, 0. ], [ 0. , 0. , 0. ]]), array([[-2., 3., 7.], [10., 11., 12.]])) isnan [[False False False] [False False False]] cos [[-0.80114362 -0.99913515 0.75390225] [-0.83907153 0.0044257 0.84385396]] . &lt;ipython-input-83-d791c8e37e6f&gt;:5: RuntimeWarning: invalid value encountered in sqrt print(func(a)) &lt;ipython-input-83-d791c8e37e6f&gt;:5: RuntimeWarning: invalid value encountered in log print(func(a)) . &#51060;&#54637; &#51068;&#48152; &#54632;&#49688; . 두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다: . a = np.array([1, -2, 3, 4]) b = np.array([2, 8, -1, 7]) np.add(a, b) # a + b 와 동일 . array([ 3, 6, 2, 11]) . np.greater(a, b) # a &gt; b 와 동일 . array([False, False, True, False]) . np.maximum(a, b) . array([2, 8, 3, 7]) . np.copysign(a, b) #b의 부호를 사용? . array([ 1., 2., -3., 4.]) . &#48176;&#50676; &#51064;&#45937;&#49905; . 1&#52264;&#50896; &#48176;&#50676; . 1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다: . a = np.array([1, 5, 3, 19, 13, 7, 3]) a[3] . 19 . a[2:5] . array([ 3, 19, 13]) . a[2:-1] #2부터 -1전까지(마지막은 포함하지않으므로) . array([ 3, 19, 13, 7]) . a[:2] #처음부터 2번째까지 . array([1, 5]) . a[2::2] # 2부터 간격 2로 . array([ 3, 13, 3]) . a[::2] . array([ 1, 3, 13, 3]) . a[::-1] # 역순으로 . array([ 3, 7, 13, 19, 3, 5, 1]) . 물론 원소를 수정할 수 있죠: . a[3]=999 a . array([ 1, 5, 3, 999, 13, 7, 3]) . 슬라이싱을 사용해 ndarray를 수정할 수 있습니다: . a[2:5] = [997, 998, 999] a . array([ 1, 5, 997, 998, 999, 7, 3]) . &#48372;&#53685;&#51032; &#54028;&#51060;&#50028; &#48176;&#50676;&#44284; &#52264;&#51060;&#51216; . 보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다. . a[2:5] = -1 a . array([ 1, 5, -1, -1, -1, 7, 3]) . 또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다: . try: a[2:5] = [1,2,3,4,5,6] # 너무 길어요 except ValueError as e: print(e) . cannot copy sequence with size 6 to array axis with dimension 3 . 원소를 삭제할 수도 없습니다: . try: del a[2:5] except ValueError as e: print(e) . cannot delete array elements . b = [1,5,3,19,13,7,3] del b[2:5] b . [1, 5, 7, 3] . 중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다! . a_slice = a[2:6] a_slice[1] = 1000 a # 원본 배열이 수정됩니다! . array([ 1, 5, -1, 1000, -1, 7, 3]) . a[3] = 2000 a_slice # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다! . array([ -1, 2000, -1, 7]) . 데이터를 복사하려면 copy 메서드를 사용해야 합니다: . another_slice = a[2:6].copy() another_slice[1] = 3000 a # 원본 배열이 수정되지 않습니다 . array([ 1, 5, -1, 2000, -1, 7, 3]) . a[3] = 4000 another_slice # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다 . array([ -1, 3000, -1, 7]) . &#45796;&#52264;&#50896; &#48176;&#50676; . 다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다: . b = np.arange(48).reshape(4, 12) b . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]) . b[1, 2] # 행 1, 열 2 . 14 . b[1, :] # 행 1, 모든 열 . array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . b[:, 1] # 모든 행, 열 1 . array([ 1, 13, 25, 37]) . 주의: 다음 두 표현에는 미묘한 차이가 있습니다: . b[1, :] # 1차원 . array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . b[1:2, :] # 2차원 . array([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]) . 첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다. . &#54060;&#49884; &#51064;&#45937;&#49905;(Fancy indexing) . 관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다. . b[(0,2), 2:5] # 행 0과 2, 열 2에서 4(5-1)까지 . array([[ 2, 3, 4], [26, 27, 28]]) . b[:, (-1, 2, -1)] # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로) . array([[11, 2, 11], [23, 14, 23], [35, 26, 35], [47, 38, 47]]) . 여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다. . b[(-1, 2, -1, 2), (5, 9, 1, 9)] # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again) . array([41, 33, 37, 33]) . &#44256;&#52264;&#50896; . 고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다: . c = b.reshape(4,2,6) c . array([[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]], [[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]], [[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]], [[36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47]]]) . c[2, 1, 4] # 행렬 2, 행 1, 열 4 . 34 . c[2, :, 3] # 행렬 2, 모든 행, 열 3 . array([27, 33]) . c[3,1,2:5] . array([44, 45, 46]) . 어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다: . c[2, 1] # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다. . array([30, 31, 32, 33, 34, 35]) . &#49373;&#47029; &#48512;&#54840; (...) . 생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다. . c[2, ...] # 행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일 . array([[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]]) . c[2, 1, ...] # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일 . array([30, 31, 32, 33, 34, 35]) . c[2, ..., 3] # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일 . array([27, 33]) . c[..., 3] # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일 . array([[ 3, 9], [15, 21], [27, 33], [39, 45]]) . &#48520;&#47532;&#50616; &#51064;&#45937;&#49905; . 불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다. . b = np.arange(48).reshape(4, 12) b . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]) . rows_on = np.array([True, False, True, False]) b[rows_on, :] # 행 0과 2, 모든 열. b[(0, 2), :]와 동일 . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]]) . cols_on = np.array([False, True, False] * 4) b[:, cols_on] # 모든 행, 열 1, 4, 7, 10 . array([[ 1, 4, 7, 10], [13, 16, 19, 22], [25, 28, 31, 34], [37, 40, 43, 46]]) . np.ix_ . 여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다: . b[np.ix_(rows_on, cols_on)] # b[(0,2),(1,4,7,10)] # 오류남.(앞과 뒤의 행이 맞지않음) . array([[ 1, 4, 7, 10], [25, 28, 31, 34]]) . np.ix_(rows_on, cols_on) . (array([[0], [2]], dtype=int64), array([[ 1, 4, 7, 10]], dtype=int64)) . ndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다: . b[b % 3 == 1] # 차원에 상관없이 무조건 1차원이 반환된다. . array([ 1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46]) . &#48152;&#48373; . ndarray를 반복하는 것은 일반적인 파이썬 배열을 반복한는 것과 매우 유사합니다. 다차원 배열을 반복하면 첫 번째 축에 대해서 수행됩니다. . c = np.arange(24).reshape(2, 3, 4) # 3D 배열 (두 개의 3x4 행렬로 구성됨) c . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) . for m in c: print(&quot;아이템:&quot;) print(m) . 아이템: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 아이템: [[12 13 14 15] [16 17 18 19] [20 21 22 23]] . # for m2 in m1: # print(m2) . [0 1 2 3] [4 5 6 7] [ 8 9 10 11] [12 13 14 15] [16 17 18 19] [20 21 22 23] . for i in range(len(c)): # len(c) == c.shape[0] print(&quot;아이템:&quot;) print(c[i]) . 아이템: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 아이템: [[12 13 14 15] [16 17 18 19] [20 21 22 23]] . ndarray에 있는 모든 원소를 반복하려면 flat 속성을 사용합니다: . for i in c.flat: print(&quot;아이템:&quot;, i) . 아이템: 0 아이템: 1 아이템: 2 아이템: 3 아이템: 4 아이템: 5 아이템: 6 아이템: 7 아이템: 8 아이템: 9 아이템: 10 아이템: 11 아이템: 12 아이템: 13 아이템: 14 아이템: 15 아이템: 16 아이템: 17 아이템: 18 아이템: 19 아이템: 20 아이템: 21 아이템: 22 아이템: 23 . &#48176;&#50676; &#49939;&#44592; . 종종 다른 배열을 쌓아야 할 때가 있습니다. 넘파이는 이를 위해 몇 개의 함수를 제공합니다. 먼저 배열 몇 개를 만들어 보죠. . q1 = np.full((3,4), 1.0) # 3,4 배열을 1로 채움 q1 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . q2 = np.full((4,4), 2.0) # 4,4 배열을 2로 채움 q2 . array([[2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.]]) . q3 = np.full((3,4), 3.0) # 3,4 배열을 3으로 채움 q3 . array([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . vstack . vstack 함수를 사용하여 수직으로 쌓아보죠: . 세로로 쌓을 때 . q4 = np.vstack((q1, q2, q3)) q4 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . q4.shape . (10, 4) . q1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다). . hstack . hstack을 사용해 수평으로도 쌓을 수 있습니다: . 가로로 쌓기 . q5 = np.hstack((q1, q3)) q5 . array([[1., 1., 1., 1., 3., 3., 3., 3.], [1., 1., 1., 1., 3., 3., 3., 3.], [1., 1., 1., 1., 3., 3., 3., 3.]]) . q5.shape . (3, 8) . q1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다: . try: q5 = np.hstack((q1, q2, q3)) except ValueError as e: print(e) . all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4 . concatenate . concatenate 함수는 지정한 축으로도 배열을 쌓습니다. . q7 = np.concatenate((q1, q2, q3), axis=0) # vstack과 동일 # 세로로 쌓음 q7 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . q7.shape . (10, 4) . 예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다. . stack . stack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다. . q8 = np.stack((q1, q3)) q8 . array([[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], [[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]]) . q8.shape # 축을 하나 더 만듬, 차원 추가함. . (2, 3, 4) . &#48176;&#50676; &#48516;&#54624; . 분할은 쌓기의 반대입니다. 예를 들어 vsplit 함수는 행렬을 수직으로 분할합니다. . 먼저 6x4 행렬을 만들어 보죠: . r = np.arange(24).reshape(6,4) r . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]) . 수직으로 동일한 크기로 나누어 보겠습니다: . r1, r2, r3 = np.vsplit(r, 3) # 3개의 array로 나눠줌. r1 . array([[0, 1, 2, 3], [4, 5, 6, 7]]) . r2 . array([[ 8, 9, 10, 11], [12, 13, 14, 15]]) . r3 . array([[16, 17, 18, 19], [20, 21, 22, 23]]) . split 함수는 주어진 축을 따라 배열을 분할합니다. vsplit는 axis=0으로 split를 호출하는 것과 같습니다. hsplit 함수는 axis=1로 split를 호출하는 것과 같습니다: . np.split(r,3,axis=0) . [array([[0, 1, 2, 3], [4, 5, 6, 7]]), array([[ 8, 9, 10, 11], [12, 13, 14, 15]]), array([[16, 17, 18, 19], [20, 21, 22, 23]])] . np.vsplit(r,3) # = np.split(r,3,axis=0) . [array([[0, 1, 2, 3], [4, 5, 6, 7]]), array([[ 8, 9, 10, 11], [12, 13, 14, 15]]), array([[16, 17, 18, 19], [20, 21, 22, 23]])] . np.hsplit(r,2) # = np.split(r,2,axis = 1) . [array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13], [16, 17], [20, 21]]), array([[ 2, 3], [ 6, 7], [10, 11], [14, 15], [18, 19], [22, 23]])] . r4, r5 = np.hsplit(r, 2) r4 . array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13], [16, 17], [20, 21]]) . r5 . array([[ 2, 3], [ 6, 7], [10, 11], [14, 15], [18, 19], [22, 23]]) . &#48176;&#50676; &#51204;&#52824; . transpose 메서드는 주어진 순서대로 축을 뒤바꾸어 ndarray 데이터에 대한 새로운 뷰를 만듭니다. . 예를 위해 3D 배열을 만들어 보죠: . t = np.arange(24).reshape(4,2,3) t . array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23]]]) . t0 = t.transpose((1,0,2)) t0 . array([[[ 0, 1, 2], [ 6, 7, 8], [12, 13, 14], [18, 19, 20]], [[ 3, 4, 5], [ 9, 10, 11], [15, 16, 17], [21, 22, 23]]]) . 0, 1, 2(깊이, 높이, 너비) 축을 1, 2, 0 (깊이→너비, 높이→깊이, 너비→높이) 순서로 바꾼 ndarray를 만들어 보겠습니다: . t1 = t.transpose((1,2,0)) # t(4,2,3) -&gt; t(2,3,4) t1 . array([[[ 0, 6, 12, 18], [ 1, 7, 13, 19], [ 2, 8, 14, 20]], [[ 3, 9, 15, 21], [ 4, 10, 16, 22], [ 5, 11, 17, 23]]]) . t1.shape . (2, 3, 4) . transpose 기본값은 차원의 순서를 역전시킵니다: . t2 = t.transpose() # t.transpose((2, 1, 0))와 동일 (4,2,3) -&gt; (3,2,4) t2 . array([[[ 0, 6, 12, 18], [ 3, 9, 15, 21]], [[ 1, 7, 13, 19], [ 4, 10, 16, 22]], [[ 2, 8, 14, 20], [ 5, 11, 17, 23]]]) . t2.shape . (3, 2, 4) . 넘파이는 두 축을 바꾸는 swapaxes 함수를 제공합니다. 예를 들어 깊이와 높이를 뒤바꾸어 t의 새로운 뷰를 만들어 보죠: . t3 = t.swapaxes(0,1) # t.transpose((1, 0, 2))와 동일 t3 . array([[[ 0, 1, 2], [ 6, 7, 8], [12, 13, 14], [18, 19, 20]], [[ 3, 4, 5], [ 9, 10, 11], [15, 16, 17], [21, 22, 23]]]) . t3.shape . (2, 4, 3) . &#49440;&#54805; &#45824;&#49688;&#54617; . 넘파이 2D 배열을 사용하면 파이썬에서 행렬을 효율적으로 표현할 수 있습니다. 주요 행렬 연산을 간단히 둘러 보겠습니다. 선형 대수학, 벡터와 행렬에 관한 자세한 내용은 Linear Algebra tutorial를 참고하세요. . &#54665;&#47148; &#51204;&#52824; . T 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다: . m1 = np.arange(10).reshape(2,5) m1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . m1.T . array([[0, 5], [1, 6], [2, 7], [3, 8], [4, 9]]) . T 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다: . m2 = np.arange(5) m2 . array([0, 1, 2, 3, 4]) . m2.T . array([0, 1, 2, 3, 4]) . 먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다: . m2r = m2.reshape(1,5) m2r . array([[0, 1, 2, 3, 4]]) . m2r.T . array([[0], [1], [2], [3], [4]]) . &#54665;&#47148; &#44273;&#49480; . 두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠. . n1 = np.arange(10).reshape(2, 5) n1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . n2 = np.arange(15).reshape(5,3) n2 . array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) . n1.dot(n2) . array([[ 90, 100, 110], [240, 275, 310]]) . 주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다. . &#50669;&#54665;&#47148;&#44284; &#50976;&#49324; &#50669;&#54665;&#47148; . numpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다: . import numpy.linalg as linalg m3 = np.array([[1,2,3],[5,7,11],[21,29,31]]) m3 . array([[ 1, 2, 3], [ 5, 7, 11], [21, 29, 31]]) . linalg.inv(m3) . array([[-2.31818182, 0.56818182, 0.02272727], [ 1.72727273, -0.72727273, 0.09090909], [-0.04545455, 0.29545455, -0.06818182]]) . pinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다: . linalg.pinv(m3) . array([[-2.31818182, 0.56818182, 0.02272727], [ 1.72727273, -0.72727273, 0.09090909], [-0.04545455, 0.29545455, -0.06818182]]) . &#45800;&#50948; &#54665;&#47148; . 행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다): . m3.dot(linalg.inv(m3)) . array([[ 1.00000000e+00, -1.66533454e-16, 0.00000000e+00], [ 6.31439345e-16, 1.00000000e+00, -1.38777878e-16], [ 5.21110932e-15, -2.38697950e-15, 1.00000000e+00]]) . eye 함수는 NxN 크기의 단위 행렬을 만듭니다: . np.eye(3) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . QR &#48516;&#54644; . qr 함수는 행렬을 QR 분해합니다: . q, r = linalg.qr(m3) q . array([[-0.04627448, 0.98786672, 0.14824986], [-0.23137241, 0.13377362, -0.96362411], [-0.97176411, -0.07889213, 0.22237479]]) . r . array([[-21.61018278, -29.89331494, -32.80860727], [ 0. , 0.62427688, 1.9894538 ], [ 0. , 0. , -3.26149699]]) . q.dot(r) # q.r는 m3와 같습니다 . array([[ 1., 2., 3.], [ 5., 7., 11.], [21., 29., 31.]]) . &#54665;&#47148;&#49885; . det 함수는 행렬식을 계산합니다: . linalg.det(m3) # 행렬식 계산 . 43.99999999999997 . &#44256;&#50995;&#44050;&#44284; &#44256;&#50976;&#48289;&#53552; . eig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다: . eigenvalues, eigenvectors = linalg.eig(m3) eigenvalues # λ . array([42.26600592, -0.35798416, -2.90802176]) . eigenvectors # v . array([[-0.08381182, -0.76283526, -0.18913107], [-0.3075286 , 0.64133975, -0.6853186 ], [-0.94784057, -0.08225377, 0.70325518]]) . m3.dot(eigenvectors) - eigenvalues * eigenvectors # m3.v - λ*v = 0 . array([[ 8.88178420e-15, 2.22044605e-16, -3.10862447e-15], [ 3.55271368e-15, 2.02615702e-15, -1.11022302e-15], [ 3.55271368e-14, 3.33413852e-15, -8.43769499e-15]]) . &#53945;&#51079;&#44050; &#48516;&#54644; . svd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다: . m4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]]) m4 . array([[1, 0, 0, 0, 2], [0, 0, 3, 0, 0], [0, 0, 0, 0, 0], [0, 2, 0, 0, 0]]) . U, S_diag, V = linalg.svd(m4) U . array([[ 0., 1., 0., 0.], [ 1., 0., 0., 0.], [ 0., 0., 0., -1.], [ 0., 0., 1., 0.]]) . S_diag . array([3. , 2.23606798, 2. , 0. ]) . svd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다: . S = np.zeros((4, 5)) S[np.diag_indices(4)] = S_diag S # Σ . array([[3. , 0. , 0. , 0. , 0. ], [0. , 2.23606798, 0. , 0. , 0. ], [0. , 0. , 2. , 0. , 0. ], [0. , 0. , 0. , 0. , 0. ]]) . V . array([[-0. , 0. , 1. , -0. , 0. ], [ 0.4472136 , 0. , 0. , 0. , 0.89442719], [-0. , 1. , 0. , -0. , 0. ], [ 0. , 0. , 0. , 1. , 0. ], [-0.89442719, 0. , 0. , 0. , 0.4472136 ]]) . U.dot(S).dot(V) # U.Σ.V == m4 . array([[1., 0., 0., 0., 2.], [0., 0., 3., 0., 0.], [0., 0., 0., 0., 0.], [0., 2., 0., 0., 0.]]) . &#45824;&#44033;&#50896;&#49548;&#50752; &#45824;&#44033;&#54633; . np.diag(m3) # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래) . array([ 1, 7, 31]) . np.trace(m3) # np.diag(m3).sum()와 같습니다 . 39 . &#49440;&#54805; &#48169;&#51221;&#49885; &#54400;&#44592; . solve 함수는 다음과 같은 선형 방정식을 풉니다: . $2x + 6y = 6$ | $5x + 3y = -9$ | . coeffs = np.array([[2, 6], [5, 3]]) depvars = np.array([6, -9]) solution = linalg.solve(coeffs, depvars) solution . array([-3., 2.]) . solution을 확인해 보죠: . coeffs.dot(solution), depvars # 네 같네요 . (array([ 6., -9.]), array([ 6, -9])) . 좋습니다! 다른 방식으로도 solution을 확인해 보죠: . np.allclose(coeffs.dot(solution), depvars) . True . &#48289;&#53552;&#54868; . 한 번에 하나씩 개별 배열 원소에 대해 연산을 실행하는 대신 배열 연산을 사용하면 훨씬 효율적인 코드를 만들 수 있습니다. 이를 벡터화라고 합니다. 이를 사용하여 넘파이의 최적화된 성능을 활용할 수 있습니다. . 예를 들어, $sin(xy/40.5)$ 식을 기반으로 768x1024 크기 배열을 생성하려고 합니다. 중첩 반복문 안에 파이썬의 math 함수를 사용하는 것은 나쁜 방법입니다: . import math data = np.empty((768, 1024)) for y in range(768): for x in range(1024): data[y, x] = math.sin(x*y/40.5) # 매우 비효율적입니다! . 작동은 하지만 순수한 파이썬 코드로 반복문이 진행되기 때문에 아주 비효율적입니다. 이 알고리즘을 벡터화해 보죠. 먼저 넘파이 meshgrid 함수로 좌표 벡터를 사용해 행렬을 만듭니다. . x_coords = np.arange(0, 1024) # [0, 1, 2, ..., 1023] y_coords = np.arange(0, 768) # [0, 1, 2, ..., 767] X, Y = np.meshgrid(x_coords, y_coords) X . array([[ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], ..., [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023]]) . Y . array([[ 0, 0, 0, ..., 0, 0, 0], [ 1, 1, 1, ..., 1, 1, 1], [ 2, 2, 2, ..., 2, 2, 2], ..., [765, 765, 765, ..., 765, 765, 765], [766, 766, 766, ..., 766, 766, 766], [767, 767, 767, ..., 767, 767, 767]]) . X.shape . (768, 1024) . Y.shape . (768, 1024) . 여기서 볼 수 있듯이 X와 Y 모두 768x1024 배열입니다. X에 있는 모든 값은 수평 좌표에 해당합니다. Y에 있는 모든 값은 수직 좌표에 해당합니다. . 이제 간단히 배열 연산을 사용해 계산할 수 있습니다: . data = np.sin(X*Y/40.5) data . array([[0. , 0. , 0. , ..., 0. , 0. , 0. ], [0. , 0.02468885, 0.04936265, ..., 0.07705885, 0.1016508 , 0.12618078], [0. , 0.04936265, 0.09860494, ..., 0.15365943, 0.20224852, 0.25034449], ..., [0. , 0.03932283, 0.07858482, ..., 0.6301488 , 0.59912825, 0.56718092], [0. , 0.06398059, 0.12769901, ..., 0.56844086, 0.51463783, 0.45872596], [0. , 0.08859936, 0.17650185, ..., 0.50335246, 0.42481591, 0.34293805]]) . data.shape . (768, 1024) . 맷플롯립의 imshow 함수를 사용해 이 데이터를 그려보죠(matplotlib tutorial을 참조하세요). . import matplotlib.pyplot as plt import matplotlib.cm as cm fig = plt.figure(1, figsize=(7, 6)) plt.imshow(data, cmap=cm.hot) plt.show() . &#51200;&#51109;&#44284; &#47196;&#46377; . 넘파이는 ndarray를 바이너리 또는 텍스트 포맷으로 손쉽게 저장하고 로드할 수 있습니다. . &#48148;&#51060;&#45320;&#47532; .npy &#54252;&#47607; . 랜덤 배열을 만들고 저장해 보죠. . save와 load를 가장 많이 사용함. . a = np.random.rand(2,3) a . array([[0.41071161, 0.12492639, 0.43912818], [0.67160608, 0.97457204, 0.69943444]]) . np.save(&quot;my_array&quot;, a) # array 형태 그대로 저장된다. . 끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다: . # with open(&quot;my_array.npy&quot;, &quot;rb&quot;) as f: # content = f.read() # content . 이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다: . a_loaded = np.load(&quot;my_array.npy&quot;) # npy형태를 가장 많이 사용함. a_loaded . array([[0.41071161, 0.12492639, 0.43912818], [0.67160608, 0.97457204, 0.69943444]]) . &#53581;&#49828;&#53944; &#54252;&#47607; . 배열을 텍스트 포맷으로 저장해 보죠: . np.savetxt(&quot;my_array.csv&quot;, a) # csv로 바꾸고 싶을 때 . 파일 내용을 확인해 보겠습니다: . with open(&quot;my_array.csv&quot;, &quot;rt&quot;) as f: print(f.read()) . 4.107116093661447032e-01 1.249263912897139450e-01 4.391281798106657641e-01 6.716060788980859897e-01 9.745720364833169169e-01 6.994344406345085474e-01 . 이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다: . np.savetxt(&quot;my_array.csv&quot;, a, delimiter=&quot;,&quot;) # 탭으로 구분된 csv 파일 . 이 파일을 로드하려면 loadtxt 함수를 사용합니다: . a_loaded = np.loadtxt(&quot;my_array.csv&quot;, delimiter=&quot;,&quot;) a_loaded . array([[0.41071161, 0.12492639, 0.43912818], [0.67160608, 0.97457204, 0.69943444]]) . &#50517;&#52629;&#46108; .npz &#54252;&#47607; . 여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다: . b = np.arange(24, dtype=np.uint8).reshape(2, 3, 4) b . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]], dtype=uint8) . np.savez(&quot;my_arrays&quot;, my_a=a, my_b=b) . 파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다. . # with open(&quot;my_arrays.npz&quot;, &quot;rb&quot;) as f: # content = f.read() # repr(content)[:180] + &quot;[...]&quot; . 다음과 같이 이 파일을 로드할 수 있습니다: . my_arrays = np.load(&quot;my_arrays.npz&quot;) my_arrays . &lt;numpy.lib.npyio.NpzFile at 0x190f9663d00&gt; . 게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다: . 메모리적으로 효율적임. . . [&#39;my_a&#39;, &#39;my_b&#39;] . my_arrays.keys() . KeysView(&lt;numpy.lib.npyio.NpzFile object at 0x00000190F9663D00&gt;) . my_arrays[&quot;my_a&quot;] . array([[0.41071161, 0.12492639, 0.43912818], [0.67160608, 0.97457204, 0.69943444]]) . &#44536; &#45796;&#51020;&#51008;? . 넘파이 기본 요소를 모두 배웠지만 훨씬 더 많은 기능이 있습니다. 이를 배우는 가장 좋은 방법은 넘파이를 직접 실습해 보고 훌륭한 넘파이 문서에서 필요한 함수와 기능을 찾아 보세요. .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/jupyter/python/2022/03/16/numpy.html",
            "relUrl": "/jupyter/python/2022/03/16/numpy.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Markdown Cheat Sheet",
            "content": "Markdown Cheat Sheet . Thanks for visiting The Markdown Guide! . This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for basic syntax and extended syntax. . Basic Syntax . These are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements. . Heading . H1 . H2 . H3 . Bold . bold text . Italic . italicized text . Blockquote . blockquote . Ordered List . First item | Second item | Third item | Unordered List . First item | Second item | Third item | . Code . code . Horizontal Rule . . Link . Markdown Guide . Image . . Extended Syntax . These elements extend the basic syntax by adding additional features. Not all Markdown applications support these elements. . Table . Syntax Description . Header | Title | . Paragraph | Text | . Fenced Code Block . { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;age&quot;: 25 } . Footnote . Here’s a sentence with a footnote. 1 . Heading ID . My Great Heading . Definition List . term definition Strikethrough . The world is flat. . Task List . Write the press release | Update the website | Contact the media | . Emoji . That is so funny! :joy: . (See also Copying and Pasting Emoji) . Highlight . I need to highlight these ==very important words==. . Subscript . H~2~O . Superscript . X^2^ . This is the footnote. &#8617; . |",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/markdown/2022/03/11/Markdown-basics.md.html",
            "relUrl": "/markdown/2022/03/11/Markdown-basics.md.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://yangjunghyun.github.io/yangjunghyun_/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "About me . 빅데이터 응용학과 20202797 양정현입니다. . 관심분야 . 빅데이터분석 .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Untitled",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . {r} library(glue) library(XML) library(stringr) library(lubridate) library(dplyr) library(httr) library(jsonlite) . 대전지역 전체 사고다발지역 도출 . 대전 군/구 코드 도출 . -법정동 코드 조회를 위해 다음 API를 사용 -https://juso.dev/docs/reg-code-api/ . library(sf) api_key &lt;- &quot;WzPY1XSX%2BX7rgalcrRKQHOGSMG%2FCCotytqLjdMrl3PvKXhXc00G5Y0Fv4wObva75VD%2Fe5wQ32Gnd%2B0JeIwmItg%3D%3D&quot; api_url &lt;- &quot;http://apis.data.go.kr/B552061/frequentzoneLg/getRestFrequentzoneLg&quot; res &lt;- &quot;https://grpc-proxy-server-mkvo6j4wsq-du.a.run.app/v1/regcodes?regcode_pattern=30*&quot; json &lt;- res %&gt;% fromJSON() daejeon_region &lt;- json$regcodes[-1,] daejeon_gugun &lt;- str_sub(daejeon_region$code,start = 3,end = 5) %&gt;% unique() get_acc_hotspots &lt;- function(year = &quot;2020&quot;, sido_code, gugun_code){ res &lt;- GET( url =api_url, query = list( serviceKey = api_key %&gt;% I(), searchYearCd = &quot;2020&quot;, siDo = sido_code, guGun = gugun_code, type = &quot;json&quot;, numOfRows = 10, pageNo = 1 ) ) json &lt;- res %&gt;% content(as = &quot;text&quot;, encoding = &quot;UTF-8&quot;) %&gt;% fromJSON() acc_hotspots &lt;- json$items$item return(acc_hotspots) } daejeon_acc_hotspots &lt;- lapply(daejeon_gugun, get_acc_hotspots, year = &quot;2020&quot;, sido_code = &quot;30&quot;) %&gt;% bind_rows() daejeon_acc_hotspots_sf&lt;- st_as_sf(daejeon_acc_hotspots, coords=c(&quot;lo_crd&quot;,&quot;la_crd&quot;), crs= 4236) . 대전지역의 사고다발 지역을 시각화 해 봅시다 . library(tmap) tmap_mode(&quot;view&quot;) tm_shape(daejeon_acc_hotspots_sf %&gt;% relocate(spot_nm))+ tm_bubbles(size = &quot;occrrnc_cnt&quot;, col = &quot;caslt_cnt&quot;) . 기타 - Point를 Polygon으로 변환하기 . library(geojsonsf) tmp2 &lt;- geojson_sf(daejeon_acc_hotspots$geom_json) st_geometry(daejeon_acc_hotspots_sf) &lt;- tmp2$geometry . 교수님 코드 . api_key &lt;- &quot;WzPY1XSX%2BX7rgalcrRKQHOGSMG%2FCCotytqLjdMrl3PvKXhXc00G5Y0Fv4wObva75VD%2Fe5wQ32Gnd%2B0JeIwmItg%3D%3D&quot; api_url &lt;- &quot;http://apis.data.go.kr/B552061/frequentzoneLg/getRestFrequentzoneLg&quot; res &lt;- GET(url = api_url, query = list( serviceKey = api_key %&gt;% I(), searchYearCd = &quot;2020&quot;, siDo = &quot;30&quot;, guGun = &quot;200&quot;, type = &quot;json&quot;, numOfRows = 10, pageNo = 1 )) print(x = res) json &lt;- res %&gt;% content(as=&quot;text&quot;, encoding = &quot;UTF-8&quot;) %&gt;% fromJSON() acc_hotspots &lt;- json$items$item ## 대전 군/구 코드 도출 res &lt;- &quot;https://grpc-proxy-server-mkvo6j4wsq-du.a.run.app/v1/regcodes?regcode_pattern=30*&quot; json &lt;- res %&gt;% fromJSON() daejeon_region &lt;- json$regcodes[-1,] daejeon_gugun &lt;- str_sub(daejeon_region$code,start = 3,end = 5) %&gt;% unique() #3번째부터 5번째까지 추출 . 대전 지역 전체 사고다발지역 도출 . get_acc_hotspots &lt;- function(year = &quot;2020&quot;, sido_code, gugun_code){ res &lt;- GET(url = api_url, query = list( serviceKey = api_key %&gt;% I(), searchYearCd = &quot;2020&quot;, siDo = sido_code, guGun = gugun_code, type = &quot;json&quot;, numOfRows = 10, pageNo = 1 )) json &lt;- res %&gt;% content(as = &quot;text&quot;, encoding = &quot;UTF-8&quot;) %&gt;% fromJSON() acc_hotspots &lt;- json$items$item return(acc_hotspots) } daejeon_acc_hotspots &lt;- lapply(daejeon_gugun, get_acc_hotspots, year = &quot;2020&quot;, sido_code = &quot;30&quot;) %&gt;% bind_rows() . 대전지역의 사고다발 지역을 시각화 해 봅시다 . library(sf) library(tmap) daejeon_acc_hotspots_sf &lt;- st_as_sf(daejeon_acc_hotspots, coords = c(&quot;lo_crd&quot;,&quot;la_crd&quot;),crs = 4236) tmap_mode(&quot;view&quot;) #relocate : 변수를 맨앞으로 보여줌 tm_shape(daejeon_acc_hotspots_sf %&gt;% relocate(spot_nm))+ tm_bubbles(size = &quot;occrrnc_cnt&quot;, col = &quot;caslt_cnt&quot;) . 기타 - Point를 Polygon으로 변환하기 . library(geojsonsf) tmp2 &lt;- geojson_sf(daejeon_acc_hotspots$geom_json) st_geometry(daejeon_acc_hotspots_sf) &lt;- tmp2$geometry .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/hotspot_.Rmd",
          "relUrl": "/2022_01/open_data_analysis/hotspot_.Rmd",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "",
          "content": "Dates and times . 강의자료 출처 - https://r4ds.had.co.nz/dates-and-times.html . Introduction . This chapter will show you how to work with dates and times in R. At first glance, dates and times seem simple. You use them all the time in your regular life, and they don’t seem to cause much confusion. However, the more you learn about dates and times, the more complicated they seem to get. To warm up, try these three seemingly simple questions: . Does every year have 365 days? | Does every day have 24 hours? | Does every minute have 60 seconds? | . I’m sure you know that not every year has 365 days, but do you know the full rule for determining if a year is a leap year? (It has three parts.) You might have remembered that many parts of the world use daylight savings time (DST), so that some days have 23 hours, and others have 25. You might not have known that some minutes have 61 seconds because every now and then leap seconds are added because the Earth’s rotation is gradually slowing down. . Dates and times are hard because they have to reconcile two physical phenomena (the rotation of the Earth and its orbit around the sun) with a whole raft of geopolitical phenomena including months, time zones, and DST. . This chapter won’t teach you every last detail about dates and times, but it will give you a solid grounding of practical skills that will help you with common data analysis challenges. . Prerequisites . This chapter will focus on the lubridate package, which makes it easier to work with dates and times in R. lubridate is not part of core tidyverse because you only need it when you’re working with dates/times. We will also need nycflights13 for practice data. . {r setup, message = FALSE} library(tidyverse) library(lubridate) library(nycflights13) . ## Creating date/times There are three types of date/time data that refer to an instant in time: - A **date**. Tibbles print this as `&lt;date&gt;`. - A **time** within a day. Tibbles print this as `&lt;time&gt;`. - A **date-time** is a date plus a time: it uniquely identifies an instant in time (typically to the nearest second). Tibbles print this as `&lt;dttm&gt;`. Elsewhere in R these are called POSIXct, but I don&#39;t think that&#39;s a very useful name. In this chapter we are only going to focus on dates and date-times as R doesn&#39;t have a native class for storing times. If you need one, you can use the **hms** package. You should always use the simplest possible data type that works for your needs. That means if you can use a date instead of a date-time, you should. Date-times are substantially more complicated because of the need to handle time zones, which we&#39;ll come back to at the end of the chapter. To get the current date or date-time you can use `today()` or `now()`: {r} today() %&gt;% class() now() . Otherwise, there are three ways you’re likely to create a date/time: . From a string. | From individual date-time components. | From an existing date/time object. | . They work as follows. . From strings . Date/time data often comes as strings. You’ve seen one approach to parsing strings into date-times in date-times. . Another approach is to use the helpers provided by lubridate. They automatically work out the format once you specify the order of the component. . To use them, identify the order in which year, month, and day appear in your dates, then arrange “y”, “m”, and “d” in the same order. . That gives you the name of the lubridate function that will parse your date. . For example: . ymd(&quot;2017-01-31&quot;) mdy(&quot;January 31st, 2017&quot;) dmy(&quot;31-Jan-2017&quot;) ?ymd . These functions also take unquoted numbers. This is the most concise way to create a single date/time object, as you might need when filtering date/time data. ymd() is short and unambiguous: . ymd(20170131) . ymd() and friends create dates. To create a date-time, add an underscore and one or more of “h”, “m”, and “s” to the name of the parsing function: . ymd_hms(&quot;2017-01-31 20:11:59&quot;) mdy_hm(&quot;01/31/2017 08:01&quot;) . You can also force the creation of a date-time from a date by supplying a timezone: . ymd(20170131, tz = &quot;UTC&quot;) ymd(20210423, tz=&#39;Asia/Seoul&#39;) . From individual components . Instead of a single string, sometimes you’ll have the individual components of the date-time spread across multiple columns. This is what we have in the flights data: . flights %&gt;% select(year, month, day, hour, minute) . To create a date/time from this sort of input, use make_date() for dates, or make_datetime() for date-times: . flights %&gt;% select(year, month, day, hour, minute) %&gt;% mutate(departure = make_datetime(year, month, day, hour, minute)) . Let’s do the same thing for each of the four time columns in flights. The times are represented in a slightly odd format, so we use modulus arithmetic to pull out the hour and minute components. . Once I’ve created the date-time variables, I focus in on the variables we’ll explore in the rest of the chapter. . make_datetime_100 &lt;- function(year, month, day, time) { make_datetime(year, month, day, time %/% 100, time %% 100) } flights_dt &lt;- flights %&gt;% filter(!is.na(dep_time), !is.na(arr_time)) %&gt;% mutate( dep_time = make_datetime_100(year, month, day, dep_time), arr_time = make_datetime_100(year, month, day, arr_time), sched_dep_time = make_datetime_100(year, month, day, sched_dep_time), sched_arr_time = make_datetime_100(year, month, day, sched_arr_time) ) %&gt;% select(origin, dest, ends_with(&quot;delay&quot;), ends_with(&quot;time&quot;)) flights_dt . With this data, I can visualise the distribution of departure times across the year: . flights_dt %&gt;% ggplot(aes(x=dep_time)) + geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day . Or within a single day: . flights_dt %&gt;% filter(dep_time &lt; ymd(20130102)) %&gt;% ggplot(aes(dep_time)) + geom_freqpoly(binwidth = 600) # 600 s = 10 minutes . Note that when you use date-times in a numeric context (like in a histogram), 1 means 1 second, so a binwidth of 86400 means one day. For dates, 1 means 1 day. . From other types . You may want to switch between a date-time and a date. That’s the job of as_datetime() and as_date(): . today() now() as_datetime(today()) as_date(now()) . Sometimes you’ll get date/times as numeric offsets from the “Unix Epoch”, 1970-01-01. If the offset is in seconds, use as_datetime(); if it’s in days, use as_date(). . as_datetime(60 * 60 * 10) as_date(365 * 10 + 2) . Date-time components . Now that you know how to get date-time data into R’s date-time data structures, let’s explore what you can do with them. . This section will focus on the accessor functions that let you get and set individual components. . The next section will look at how arithmetic works with date-times. . exercise() . Use the appropriate lubridate function to parse each of the following dates: . d1 &lt;- &quot;January 1, 2010&quot; # &quot;2010-01-20&quot; d2 &lt;- &quot;2015-Mar-07&quot; # &quot;2015-03-07&quot; d3 &lt;- &quot;06-June-2017&quot; # &quot;2017-06-06&quot; d4 &lt;- c(&quot;August 19 (2015)&quot;, &quot;July 1 (2015)&quot;) # &quot;2015-01-09&quot; &quot;2015-01-20&quot; d5 &lt;- &quot;12/30/14&quot; # Dec 30, 2014 # &quot;2014-12-30&quot; mdy(d1) ymd(d2) dmy(d3) mdy(d4) mdy(d5) . Getting components . You can pull out individual parts of the date with the accessor functions year(), month(), mday() (day of the month), yday() (day of the year), wday() (day of the week), hour(), minute(), and second(). . datetime &lt;- ymd_hms(&quot;2021-04-23 12:34:56&quot;) year(datetime) month(datetime) mday(datetime) yday(datetime) wday(datetime) . For month() and wday() you can set label = TRUE to return the abbreviated name of the month or day of the week. Set abbr = FALSE to return the full name. . month(datetime, label = TRUE) wday(datetime, label = TRUE, abbr = TRUE) . We can use wday() to see that more flights depart during the week than on the weekend: . flights_dt %&gt;% mutate(wday = wday(dep_time, label = TRUE)) %&gt;% ggplot(aes(x = wday)) + geom_bar() . There’s an interesting pattern if we look at the average departure delay by minute within the hour. . It looks like flights leaving in minutes 20-30 and 50-60 have much lower delays than the rest of the hour! . flights_dt %&gt;% mutate(minute = minute(dep_time)) %&gt;% group_by(minute) %&gt;% summarise( avg_delay = mean(arr_delay, na.rm = TRUE), n = n()) %&gt;% ggplot(aes(minute, avg_delay)) + geom_line() . Interestingly, if we look at the scheduled departure time we don’t see such a strong pattern: . sched_dep &lt;- flights_dt %&gt;% mutate(minute = minute(sched_dep_time)) %&gt;% group_by(minute) %&gt;% summarise( avg_delay = mean(arr_delay, na.rm = TRUE), n = n()) ggplot(sched_dep, aes(minute, avg_delay)) + geom_line() . So why do we see that pattern with the actual departure times? Well, like much data collected by humans, there’s a strong bias towards flights leaving at “nice” departure times. . Always be alert for this sort of pattern whenever you work with data that involves human judgement! . ggplot(sched_dep, aes(minute, n)) + geom_line() . Rounding . An alternative approach to plotting individual components is to round the date to a nearby unit of time, with floor_date(), round_date(), and ceiling_date(). Each function takes a vector of dates to adjust and then the name of the unit round down (floor), round up (ceiling), or round to. This, for example, allows us to plot the number of flights per week: . # 시간 반올림, 내림 flights_dt %&gt;% count(week = floor_date(dep_time, &quot;week&quot;)) %&gt;% #일주일 단위로 시간 내림 ggplot(aes(week, n)) + geom_line() . Computing the difference between a rounded and unrounded date can be particularly useful. . Setting components . You can also use each accessor function to set the components of a date/time: . (datetime &lt;- ymd_hms(&quot;2016-07-08 12:34:56&quot;)) year(datetime) &lt;- 2020 datetime month(datetime) &lt;- 01 datetime hour(datetime) &lt;- hour(datetime) + 1 datetime . Alternatively, rather than modifying in place, you can create a new date-time with update(). This also allows you to set multiple values at once. . update(datetime, year = 2020, month = 2, mday = 2, hour = 2) . If values are too big, they will roll-over: . ymd(&quot;2015-02-01&quot;) %&gt;% update(mday = 30) ymd(&quot;2015-02-01&quot;) %&gt;% update(hour = 400) . You can use update() to show the distribution of flights across the course of the day for every day of the year: . flights_dt %&gt;% mutate(dep_hour = update(dep_time, yday = 1)) %&gt;% ggplot(aes(dep_hour)) + geom_freqpoly(binwidth = 300) . Setting larger components of a date to a constant is a powerful technique that allows you to explore patterns in the smaller components. . Time spans . Next you’ll learn about how arithmetic with dates works, including subtraction, addition, and division. Along the way, you’ll learn about three important classes that represent time spans: . durations, which represent an exact number of seconds. | periods, which represent human units like weeks and months. | intervals, which represent a starting and ending point. | . Durations . In R, when you subtract two dates, you get a difftime object: . # How old is Hadley? h_age &lt;- today() - ymd(19791014) h_age h_age %&gt;% class() . A difftime class object records a time span of seconds, minutes, hours, days, or weeks. This ambiguity can make difftimes a little painful to work with, so lubridate provides an alternative which always uses seconds: the duration. . as.duration(h_age) . Durations come with a bunch of convenient constructors: . dseconds(15) dminutes(10) dhours(c(12, 24)) ddays(0:5) dweeks(3) dyears(1) . Durations always record the time span in seconds. Larger units are created by converting minutes, hours, days, weeks, and years to seconds at the standard rate (60 seconds in a minute, 60 minutes in an hour, 24 hours in day, 7 days in a week, 365 days in a year). . You can add and multiply durations: . 2 * dyears(1) dyears(1) + dweeks(12) + dhours(15) . You can add and subtract durations to and from days: . tomorrow &lt;- today() + ddays(1) last_year &lt;- today() - dyears(1) . However, because durations represent an exact number of seconds, sometimes you might get an unexpected result: . one_pm &lt;- ymd_hms(&quot;2016-03-12 13:00:00&quot;, tz = &quot;America/New_York&quot;) one_pm one_pm + ddays(1) . Why is one day after 1pm on March 12, 2pm on March 13?! If you look carefully at the date you might also notice that the time zones have changed. Because of DST, March 12 only has 23 hours, so if we add a full days worth of seconds we end up with a different time. . Periods . To solve this problem, lubridate provides periods. Periods are time spans but don’t have a fixed length in seconds, instead they work with “human” times, like days and months. That allows them to work in a more intuitive way: . one_pm one_pm + days(1) . Like durations, periods can be created with a number of friendly constructor functions. . seconds(15) minutes(10) hours(c(12, 24)) days(7) months(1:6) weeks(3) years(1) . You can add and multiply periods: . 10 * (months(6) + days(1)) days(50) + hours(25) + minutes(2) . And of course, add them to dates. Compared to durations, periods are more likely to do what you expect: . # A leap year ymd(&quot;2016-01-01&quot;) + dyears(1) ymd(&quot;2016-01-01&quot;) + years(1) # Daylight Savings Time one_pm + ddays(1) one_pm + days(1) . Let’s use periods to fix an oddity related to our flight dates. Some planes appear to have arrived at their destination before they departed from New York City. . flights_dt %&gt;% filter(arr_time &lt; dep_time) . These are overnight flights. We used the same date information for both the departure and the arrival times, but these flights arrived on the following day. We can fix this by adding days(1) to the arrival time of each overnight flight. . flights_dt &lt;- flights_dt %&gt;% mutate( overnight = arr_time &lt; dep_time, arr_time = arr_time + days(overnight * 1), sched_arr_time = sched_arr_time + days(overnight * 1) ) . Now all of our flights obey the laws of physics. . flights_dt %&gt;% filter(overnight, arr_time &lt; dep_time) .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_10_datetimes.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_10_datetimes.Rmd",
          "date": ""
      }
      
  

  
      ,"page9": {
          "title": "text_analysis",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . # Load dependencies {r} # install.packages(&quot;tidytext&quot;) # install.packages(&quot;janeaustenr&quot;) library(tidyverse) library(tidytext) library(janeaustenr) library(stringr) . Data load &amp; preprocessing . # rm(list = ls()) books &lt;- austen_books() original_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect( text, regex(&quot;^chapter d&quot;, ignore_case = TRUE) ))) %&gt;% ungroup() original_books . Text 데이터의 Tokenization 수행 . #토큰 : 단어별로 쪼개기? . tidy_books &lt;- original_books %&gt;% unnest_tokens(word, text) #단어를 text로 word라는 컬럼을 생성 tidy_books . Remove stop words 불용어 제거하기 ex) to, of, a, the … 의미가 없는 단어들 . data(stop_words) tidy_books &lt;- tidy_books %&gt;% anti_join(stop_words) #anti_join(): 겹치는 단어를 제거해줌 . Visualize word frequency . 단어 빈도수 시각화 모든 책에 대해서 어떠한 단어가 많이 등장했는지 관찰 . vis_text &lt;- tidy_books %&gt;% count(word, sort = TRUE) %&gt;% slice_max(n, n=15) %&gt;% mutate(word = reorder(word, n)) ggplot(vis_text, aes(n, word))+ geom_col()+ labs(y = NULL) . 연습문제 : 책 별로 단어 빈도 시각화해보기 . Hint : 위의 코드에서 group_by를 활용하고, ggplot에서는 facet_wrap을 활용 . vis_text &lt;- tidy_books %&gt;% group_by(book) %&gt;% count(word, sort = TRUE) %&gt;% slice_max(n, n=15) %&gt;% mutate(word = reorder(word, n)) ggplot(vis_text, aes(n, word))+ geom_col()+ labs(y = NULL) + facet_wrap(vars(book),scales = &quot;free&quot;) # 일케하면 정렬 제대로댐 ggplot(vis_text, aes(n, reorder_within(word,n,book),fill = book))+ geom_col(show.legend =FALSE)+ labs(y = NULL)+ facet_wrap(vars(book), scales = &quot;free&quot;)+ scale_y_reordered() vis_text$word_reorder_within &lt;- reorder_within(vis_text$word, vis_text$n, vis_text$book) vis_text$word_fac_reorder &lt;- fct_reorder(vis_text$word, vis_text$n) . Review of tf-idf score # 단어의 상대적인 빈도를 보여줌 . Load dependencies . # library(tidyverse) # library(janeaustenr) # library(tidytext) . Data load &amp; Tokenization . book_tokens &lt;- austen_books() %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words) #책 별로 단어 수 count book_words &lt;- book_tokens %&gt;% count(book, word) %&gt;% arrange(book, desc(n)) book_words . Calculate tf-idf score . book_tf_idf &lt;- book_words %&gt;% bind_tf_idf(term = word, document = book, n = n) . Visualize tf-idf score . library(forcats) # 책별로 tf-idf 가 높은 단어 15개씩 자름 book_tf_idf_vis &lt;- book_tf_idf %&gt;% group_by(book) %&gt;% slice_max(tf_idf, n = 15) %&gt;% ungroup() ggplot(book_tf_idf_vis, aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) + geom_col(show.leg.end = FALSE) + facet_wrap(~book, ncol = 3, scales = &quot;free&quot;) + labs(x = &quot;tf-idf&quot;, y = NULL) . Sentiment Analysis # 감성분석 (단어의 긍정,부정) . Import libraries . library(tidytext) library(tidyverse) library(janeaustenr) # install.packages(&quot;textdata&quot;) library(textdata) . 감성어 사전 읽어오기 . bing &lt;- get_sentiments(&quot;bing&quot;) nrc &lt;- get_sentiments(&quot;nrc&quot;) affin &lt;- get_sentiments(&quot;afinn&quot;) head(bing) head(nrc) head(affin) table(affin$value) table(nrc$sentiment) . Tokenization . tidy_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate( linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter d&quot;, ignore_case = TRUE)))) %&gt;% ungroup() %&gt;% unnest_tokens(word, text) tidy_books . Sentiment join (bing 감성어 사전 사용) . # 책별로, 80줄 단위(index)별로, negative &amp; positive 사용빈도(n) 추출 # inner_join : 매칭이 안되는 행은 전부 없앰 # left_join : 매칭이 안되면 NULL jane_austen_sentiment_count &lt;- tidy_books %&gt;% inner_join(bing, by=&quot;word&quot;) %&gt;% count(book, index = linenumber %/% 80, sentiment) jane_austen_sentiment_count # sentiment 변수를 spread하여 positive, negative 컬럼을 새로 생성 # positive 단어수와 negative 단어수의 차이로 sentiment_score 계산 jane_austen_sentiment &lt;- jane_austen_sentiment_count %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment_score = positive - negative) jane_austen_sentiment . Visualize sentiment score . 80줄 단위 간격으로 sentiment score 시각화 . ggplot(jane_austen_sentiment, aes(x=index, y=sentiment_score, fill = book)) + geom_col(show.legend = FALSE) + facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) . Chapter 단위로 sentiment score 시각화 . # 책별로, Chapter 별로, negative &amp; positive 사용빈도(n) 추출 jane_austen_sentiment_count &lt;- tidy_books %&gt;% inner_join(bing, by=&quot;word&quot;) %&gt;% count(book, chapter, sentiment) # sentiment 변수를 spread하여 positive, negative 컬럼을 새로 생성 # positive 단어수와 negative 단어수의 차이로 sentiment_score 계산 jane_austen_sentiment_chapter &lt;- jane_austen_sentiment_count %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment_score = positive - negative) # 시각화 ggplot(jane_austen_sentiment_chapter, aes(x=chapter, y=sentiment_score, fill = book)) + geom_col(show.legend = FALSE) + facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) . Sentiment score 계산 (affin 감성어 사전 사용) . affin 감성어 사전의 경우 bing 사전에 비해 갯수는 적지만 단어의 score가 명시되어 있어서, 긍정/부정의 강도를 표현하기가 더 용이함 . table(affin$value) table(bing$sentiment) . # 책별로, 80줄 단위(index)별로, 등장한 단어와 각 단어의 긍/부정값(value) 추출 # index 별로 value의 합을 계산하여 sentiment_score로 정의 jane_austen_sentiment_affin &lt;- tidy_books %&gt;% inner_join(affin, by=&quot;word&quot;) %&gt;% group_by(book, index = linenumber %/% 80) %&gt;% summarise(sentiment_score=sum(value)) ggplot(jane_austen_sentiment_affin, aes(x=index, y=sentiment_score, fill = book)) + geom_col(show.legend = FALSE) + facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) . Sentiment score 계산 (nrc 감성어 사전 사용) . # nrc 감성어 사전에서 긍정/부정만 추출 nrc_pn &lt;- nrc %&gt;% filter(sentiment %in% c(&quot;positive&quot;,&quot;negative&quot;)) # 책별로, 80줄 단위(index)별로, negative &amp; positive 사용빈도(n) 추출 jane_austen_sentiment_count &lt;- tidy_books %&gt;% inner_join(nrc_pn, by=&quot;word&quot;) %&gt;% count(book, index = linenumber %/% 80, sentiment) # sentiment 변수를 spread하여 positive, negative 컬럼을 새로 생성 # positive 단어수와 negative 단어수의 차이로 sentiment_score 계산 jane_austen_sentiment_nrc &lt;- jane_austen_sentiment_count %&gt;% spread(sentiment, n, fill = 0) %&gt;% mutate(sentiment_score = positive - negative) # 시각화 ggplot(jane_austen_sentiment_nrc, aes(x=index, y=sentiment_score, fill = book)) + geom_col(show.legend = FALSE) + facet_wrap(~book, ncol = 2, scales = &quot;free_x&quot;) . Most common positive and negative words . # Bing 감성어 사전을 Join 한 후, # 단어별, 긍정/부정별로 얼마나 많은 횟수만큼 사용됐는지 count bing_word_counts &lt;- tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment, sort = TRUE) bing_word_counts . 시각화 . # sentiment별(긍정/부정별), 가장 많이 사용된 단어 10개를 추출 # word를 단어빈도 기준으로 재정렬한 후, # ggplot의 geom_col으로 시각화 bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(n=10) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(x = &quot;Contribution to sentiment&quot;, y = NULL) . 이상한 단어에 대한 보정 . miss라는 단어는 사실 부정적 표현이라고 볼 수 없음. 이에 대한 보정을 수행 . stop_words custom_stop_words &lt;- bind_rows(tibble(word = c(&quot;miss&quot;), lexicon = c(&quot;custom&quot;)), stop_words) custom_stop_words . # 단어 보정 # stop_words들을 제거함 tidy_books &lt;- tidy_books %&gt;% anti_join(custom_stop_words) #&gt; Joining, by = &quot;word&quot; # 단어별, 긍정/부정별로 얼마나 많은 횟수만큼 사용됐는지 count bing_word_counts &lt;- tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(word, sentiment, sort = TRUE) #&gt; Joining, by = &quot;word&quot; # sentiment별(긍정/부정별), 가장 많이 사용된 단어 10개를 추출 # word를 단어빈도 기준으로 재정렬한 후, # ggplot의 geom_col으로 시각화 bing_word_counts %&gt;% group_by(sentiment) %&gt;% top_n(n=15) %&gt;% ungroup() %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(n, word, fill = sentiment)) + geom_col(show.legend = FALSE) + facet_wrap(~sentiment, scales = &quot;free_y&quot;) + labs(x = &quot;Contribution to sentiment&quot;, y = NULL) . Wordcloud . library(wordcloud2) library(viridis) viridis_pal(option = &quot;plasma&quot;)(6) # wordcloud용 데이터 생성 dat_wordcloud &lt;- bing_word_counts %&gt;% mutate(col=ifelse(sentiment==&quot;positive&quot;, viridis_pal(option = &quot;plasma&quot;)(6)[5], viridis_pal(option = &quot;plasma&quot;)(6)[2])) %&gt;% select(word,n,col) wordcloud2(dat_wordcloud, color = dat_wordcloud$col, backgroundColor = &quot;black&quot;, fontFamily = &#39;나눔바른고딕&#39;, minSize=10, shape = &quot;circle&quot;, size=0.6) . wordcloud 라이브러리를 써도 무방함 . library(wordcloud) library(reshape2) bing_word_counts %&gt;% acast(word ~ sentiment, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = 100, scale=c(3,.1)) . tmp2 &lt;- bing_word_counts %&gt;% spread(sentiment, n, fill = 0) tmp2 &lt;- tmp2 %&gt;% data.frame() row.names(tmp2) &lt;- tmp2$word tmp2$word&lt;-NULL tmp2 %&gt;% comparison.cloud(colors = c(&quot;gray20&quot;, &quot;gray80&quot;), max.words = 100, scale=c(2,.1)) . 책별로 긍/부정 단어 시각화 . # 책별로, 단어별로, 긍정/부정별로 단어수 count # negative 단어는 음수로 표시 # 책별로, 감정별로 그룹해서 가장 많이 등장한 단어 10개씩 뽑기 pos_neg &lt;- tidy_books %&gt;% inner_join(get_sentiments(&quot;bing&quot;)) %&gt;% count(book, word, sentiment, sort = TRUE) %&gt;% mutate(n = ifelse(sentiment == &quot;negative&quot;, -n, n)) %&gt;% group_by(book,sentiment) %&gt;% slice_max(abs(n), n = 10, with_ties = F) # 시각화 ggplot(data=pos_neg, aes(x=n, y=reorder(word,n), fill = sentiment)) + geom_col() + labs(x = &quot;Contribution to sentiment&quot;, y = NULL) + facet_wrap(.~book, scales = &quot;free&quot;) . .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_13_textdata_analysis.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_13_textdata_analysis.Rmd",
          "date": ""
      }
      
  

  
      ,"page10": {
          "title": "",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . # 정규표현식에 대한 부가설명 ^ : 이걸로 시작함 $ : 이걸로 끝남 . : 임의의 글자 하나 ? : 앞에 있는 문자가 없거나 하나 + : 앞에 있는 문자가 하나 이상 * : 앞에 있는 문자가 없거나 하나 이상 `grep` (찾고자하는 패턴, 대상벡터) {r} library(stringr) data&lt;-c(&quot;apple&quot;,&quot;banana&quot;,&quot;banano&quot;) grep(&quot;banana&quot;, data) # 패턴과 일치하는 데이터가 있으면 해당 index를 출력 . gsub(&quot;bana&quot;, &quot;rep&quot;, data) # 패턴과 일치하는 부분의 데이터를 변경 . “banana” 라는 단어가 들어가 있기만 하면 찾음 . data&lt;-c(&quot;apple&quot;,&quot;banana&quot;,&quot;banano&quot;, &quot;a banana&quot;) grep(&quot;banana&quot;, data) . ^을 맨 앞에 같이 사용하면 그 뒤의 글자로 시작하는 데이터만 찾음 . grep(&quot;^banana&quot;, data) . $을 맨 뒤에 같이 사용하면 그 앞의 글자로 끝나는 데이터만 찾음 . data&lt;-c(&quot;apple&quot;,&quot;banana&quot;,&quot;banano&quot;, &quot;a banana&quot;, &quot;a banana a&quot;) grep(&quot;banana&quot;, data) . grep(&quot;banana$&quot;, data) . gsub(&quot;banana$&quot;, &quot;aa&quot;, data) . 완전히 일치하는 단어만 찾기 . data&lt;-c(&quot;apple&quot;,&quot;banana&quot;,&quot;banano&quot;, &quot;a banana&quot;, &quot;a banana a&quot;) grep(&quot;banana&quot;, data) . grep(&quot;^banana$&quot;, data) . . 은 정규표현식에서 무엇이든 한 개의 글자를 의미 . data &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) grep(&quot;.a.&quot;, data) . str_extract(data, pattern=&quot;.a.&quot;) . str_extract(data, pattern=&quot;.a.$&quot;) . str_extract(data, pattern=&quot;^.a.&quot;) . []는 대괄호 안에 있는 글자 하나하나가 문자클래스로 가능한 경우입니다. 예를 들어 [02468]이라고 하면 0, 2, 4, 6, 8 중 하나의 글자면 같은 패턴으로 이해합니다. . x &lt;- c(&quot;123&quot;, &quot;1357&quot;,&quot;999990&quot;,&quot;1133&quot;) grep(&quot;[02468]&quot;, x) . 문자 클래스 내에서는 ^가 지정한 글자들을 제외하고라는 뜻입니다. . x &lt;- c(&quot;123&quot;, &quot;1357&quot;,&quot;999990&quot;,&quot;0200&quot;,&quot;02468&quot;, &quot;023&quot;, &quot;999024&quot;) grep(&quot;[^02468]&quot;, x) . str_extract(x, pattern=&quot;[^02468]&quot;) . ?는 글자 뒤에 붙어서 그 글자가 한개 있거나 없는 경우 모두를 표현할 때 사용합니다. . x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;aple&quot;) grep(&quot;app?&quot;, x) . str_extract(x, pattern=&quot;app?&quot;) . +는 글자 뒤에 붙어서 그 글자가 한개 이상 연속하는 모두를 표현할 때 사용합니다. . x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;aple&quot;) grep(&quot;p+&quot;, x) . str_extract(x, pattern=&quot;p+&quot;) . grep(&quot;ap+&quot;, x) . str_extract(x, pattern=&quot;ap+&quot;) . *는 글자 뒤에 붙어서 그 글자가 없는 경우부터 여러 개 연속하는 모두를 표현할 때 사용합니다. . x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;aple&quot;, &quot;abble&quot;,&quot;appppppppple&quot;) grep(&quot;app*&quot;, x) . str_extract(string=x, pattern=&quot;app*&quot;) . str_extract(string=x, pattern=&quot;app+&quot;) . x &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;aple&quot;, &quot;abble&quot;,&quot;appppppppple&quot;, &quot;aeepp &quot;) str_extract(string=x, pattern=&quot;ae+p+&quot;) . str_extract(string=x, pattern=&quot;ae*p+&quot;) . Exercise . x &lt;- c(&quot;creed&quot;, &quot;meed&quot;, &quot;med&quot;, &quot;mat&quot;, &quot;creep&quot;, &quot;acreed&quot;) . 위의 텍스트 중, 첫글자가 m으로 시작하는 단어만 추출하시오. | x[grep(pattern = &quot;^m&quot;,x)] . 위의 텍스트 중, e가 한번이라도 들어가는 텍스트를 모두 추출하시오. | x[grep(pattern = &quot;e&quot;,x)] . 위의 텍스트 중, e가 두번이상 연속으로 들어가는 텍스트를 추출하시오. | x[grep(pattern = &quot;ee&quot;,x)] . 위의 텍스트 중, “cr” 로 시작하는 단어를, 모두 추출하시오 | x[grep(pattern = &quot;^cr&quot;,x )] . 위의 텍스트 중, 알파벳 e를 모두 o로 변환하시오 | gsub(pattern = &quot;e&quot;,&quot;o&quot;,x) gsub(&quot;e&quot;,&quot;o&quot;,x) .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_14_regex_practice_student.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_14_regex_practice_student.Rmd",
          "date": ""
      }
      
  

  
      ,"page11": {
          "title": "week_2a_Use_tidyverse",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . {r} rm(list=ls()) . 1. Tidyverse 패키지 . ggplot2, dplyr, tidyr, readr, purr, tibble, stringr, forcats . library(tidyverse) . 코드가 길어지면 이해하기 어려운 한계가 존재 . plot(diff(log(sample(rnorm(10000, mean = 10, sd = 1), size = 100, replace = FALSE))),col = &quot;red&quot;, type = &quot;l&quot;) . 2. 파이프 연산자( %&gt;% )을 통한 코드의 직관화 . 순차적 흐름으로 코드를 이해할 수 있음 중간 변수들을 계속해서 object로 할당할 필요가 없음 . rnorm(10000, mean = 10, sd = 1) %&gt;% #정규분포에서 랜덤넘버 10000개 생성 sample(size = 100, replace =FALSE) %&gt;% log() %&gt;% diff() %&gt;% plot(col = &quot;red&quot;,type=&quot;l&quot;) . 3. Flight 데이터의 파이프 연산자 예시 . library(nycflights13) nrow(flights) #행의 수 head(flights) . flights data를 선택 후, group_by 한 후, summarise를 수행 . flights %&gt;% group_by(year,month,day) %&gt;% summarise(delay = mean(dep_delay, na.rm = TRUE)) mean_dalay_by_day &lt;- flights %&gt;% group_by(year,month,day) %&gt;% summarise(delay = mean(dep_delay, na.rm = TRUE)) . 4. dplyr 패키지에 포함된 유용한 명령어들 . 4.1 select() : 열방향 선택 . 데이터에서 특정 컬럼을 선택하고 싶을 때 사용, 선언된 순서대로 컬럼을 정렬함. . flights %&gt;% select(year,month,day) # (year,month,day)를 제외하고 선택하고 싶을 때 flights %&gt;% select(-(year:day)) # 선언된 순서대로 가져오고 싶을 때 flights %&gt;% select(time_hour, air_time, everything()) # 컬럼에 띄어쓰기가 존재할 때 flights$`not use` &lt;- 0 # 컬럼이름에 되도록 띄어쓰기는 사용X flights %&gt;% select(`not use`) # 제거하고 싶을 때 flights$`not use` &lt;- NULL . 4.2. mutate() : 새로운 컬럼을 만들거나 계산하고 싶을 때 . #year에서 day까지, delay로 끝나는 컬럼, distance, air_time 선택 flight_sample &lt;- flights %&gt;% select(year:day, ends_with(&quot;delay&quot;), distance, air_time) #start_with() : 특정한 단어로 시작하는 컬럼 flight_sample #contains(): 특정한 단어가 포함된 컬럼 flights %&gt;% select(contains(&quot;time&quot;)) # transmute(): mutate와 동일하지만 기존에 있는 column들을 모두 제거 flights %&gt;% transmute( hour = dep_time %/% 100, #몫 minute = dep_time %% 100) #나머지 . 각 컬럼 간의 계산으로 새로운 열(Column)을 만듬 . #기존방식 # flight_sample$net_delay &lt;- flight_sample$arr_delay - flight_sampledep_delay flight_sample %&gt;% mutate(net_delay = arr_delay - dep_delay, speed = distance/air_time*60) . 4.3. filter() : 행방향으로 조건을 설정 . filter() 는 데이터 중에 조건에 해당하는 일부 데이터만 필터해서 사용. 논리 연산자와 결합하여 많이 사용. . a &lt;- c(10,20,30,40,50) b &lt;- 30 #논리 연산자 a == b # FALSE FALSE TRUE FALSE FALSE a &gt; b #FALSE FALSE FALSE TRUE TRUE # 숫자를 사용한 인덱싱 a[1] a[c(1,3)] #10 30 # 논리 연산자를 사용한 인덱싱 a[a&gt;b] # 40 50 a[c(4,5)] # 40 50 . flights %&gt;% filter(month==1) # 1월 또는 2월인 데이터 flights %&gt;% filter(month == 1 | month ==2) # %in% 또한 가능 flights %&gt;% filter(month %in% c(1,2)) . 4.4. bind_rows() : 두 데이터를 행방향으로 묶음 . 데이터를 아래로 붙임 . filter()를 사용한 subset 추출 . jan &lt;- flights %&gt;% filter(month==1) feb &lt;- flights %&gt;% filter(month==2) janfeb &lt;- bind_rows(jan,feb) janfeb # 이 코드와 같음 flights %&gt;% filter(month %in% c(1,2)) . 4.5.arrange(): 데이터 정렬 . 지정되는 컬럼 기준으로 오름차순으로 정렬 . # 오름차순 정렬 flights %&gt;% arrange(dep_delay) # 내림차순 정렬 flights %&gt;% arrange(-dep_delay) flights %&gt;% arrange(desc(dep_delay)) . 4.6. group_by() &amp; summarise() : 그룹별로 계산을 수행 . 그룹별로 통계치를 뽑거나 특정 계산을 수행하고 싶은 경우 . # Group_by를 하지 않았을 때 flights %&gt;% summarise(mean_dep_delay = mean(dep_delay, na.rm =T), count = n()) #group_by flights %&gt;% group_by(month) %&gt;% summarise(mean_dep_delay = mean(dep_delay, na.rm =T), count = n()) # ungroup(): group을 제거하고 싶을 때 flights %&gt;% ungroup() . 4.7. left_join() . # 특정 컬럼만 추출 flight_sample &lt;- flights %&gt;% select(year:day, origin, carrier) flight_sample . airlines . 두 데이터를 특정한 key value를 기준으로 좌우로 붙이고 싶을 때 . flight_sample %&gt;% left_join(airlines, by=&quot;carrier&quot;) . key 값의 컬럼 이름이 다를 경우 . colnames(airlines)[1] &lt;- &quot;company_name&quot; flight_sample %&gt;% left_join(airlines, by = c(&quot;carrier&quot; = &quot;company_name&quot;)) . 4.8. rename():컬럼이름을 변경해줌 . flights %&gt;% rename(tail_num = tailnum) . ##########03/10########## . Data transformation . Missing values . Missing value는 항상 젤 밑으로 감 (arrange를 사용하면 가장 아래 위치) . x &lt;- NA is.na(x) is.na(flights$dep_time) #NA값 개수 sum(is.na(flights$dep_time)) flights$dep_time %&gt;% is.na() %&gt;% sum() # na.rm = TRUE : na값을 포함하지 않음 flights %&gt;% group_by(year,month,day) %&gt;% summarise(mean = mean(dep_delay, na.rm = TRUE)) # !is.na() : na가 아닌 값 not_cancelled &lt;- flights %&gt;% filter(!is.na(dep_delay),!is.na(arr_delay)) . Grouped mutates . 년/월/일자별 가장 arr_delay가 높은 10개의 비행만을 추출 | . flights %&gt;% group_by(year,month,day) %&gt;% filter(rank(desc(arr_delay))&lt; 11) %&gt;% #rank(): select(year,month,day,arr_delay) #slice_max() = rank(desc(arr_delay))&lt; 11 : 상위 10개 값만 뽑음 flights %&gt;% group_by(year,month,day) %&gt;% slice_max(arr_delay, n = 10) %&gt;% select(year,month,day, arr_delay) flights %&gt;% mutate( gain = dep_delay - arr_delay, hours = air_time / 60, gain_per_hour = gain / hours, .after = year ) # .after = 원하는 곳에 data를 추가 . ##########03/15########## . dest (도착공항) 별로 그룹을 해서, 비행기가 10000대 이상 착륙한 공항으로 도착한 비행정보만 추출 | . popular_dests &lt;- flights %&gt;% group_by(dest) %&gt;% filter(n()&gt;10000) table(popular_dests$dest) . tmp &lt;- popular_dests %&gt;% filter(arr_delay&gt;0) %&gt;% mutate(prop_delay = arr_delay / sum(arr_delay)) %&gt;% select(year:day, dest, arr_delay, prop_delay) tmp . group_by로 인해 값이 바뀔 수 있음을 주의 . grade &lt;- tibble( name = c(&quot;Paul&quot;,&quot;James&quot;,&quot;Durant&quot;,&quot;Harden&quot;,&quot;Jordan&quot;), region = c(&quot;west&quot;,&quot;west&quot;,&quot;east&quot;,&quot;east&quot;,&quot;east&quot;), score = c(95,91,87,89,100)) # group_by 하지 않았을 때 score_proportion &lt;- grade %&gt;% mutate(prop_score = score / sum(score)) sum(score_proportion$prop_score) # group_by를 했을 때 score_proportion &lt;- grade %&gt;% group_by(region) %&gt;% mutate(prop_score = score / sum(score)) sum(score_proportion$prop_score) .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_2a_use_tidyverse.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_2a_use_tidyverse.Rmd",
          "date": ""
      }
      
  

  
      ,"page12": {
          "title": "Untitled",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . ## R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;http://rmarkdown.rstudio.com&gt;. When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: {r message = FALSE} #rm(list = ls()) library(tidyverse) . Including Plots . You can also embed plots, for example: . {r pressure, echo=FALSE} heights &lt;- read_csv(“data/heights.csv”) . heights . {r} read_csv(&quot;The first line of metadata The second line of metadata x,y,z 1,2,3&quot;, skip = 2) #앞의 두 줄을 날릴 때 read_csv(&quot;# A comment I want to skip x,y,z 1,2,3&quot;, comment = &quot;#&quot;) #주석을 무시하고 데이터를 읽을 때 . # 새로운 라인을 추가할 때 n read_csv(&quot;1,2,3 n 4,5,6&quot;,col_names = FALSE) #col_names가 없을 때 # 자동으로 X1:X3 컬 네임 생성 . # col_names를 붙여줄 때 read_csv(&quot;1,2,3 n4,5,6&quot;, col_names = c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;)) . .을 na로 취급함 . # .을 na로 지정해줌 (메모리 절약) read_csv(&quot;a,b,c n1,2,.&quot;, na=&quot;.&quot;) . 한글로 된 데이터 깨지지 않고 읽어오기 . Locale 설정 및 확인 . 한글이 깨질 때는 locale을 한글로 설정 | . Sys.getlocale() #Sys.setlocale(&quot;LC_ALL&quot;,&quot;Korean&quot;) # 언어 한글로 #Sys.setlocale(&quot;LC_ALL&quot;,&quot;English&quot;) # 언어 영어로 #Sys.setlocale(&quot;LC_ALL&quot;,&quot;C&quot;) # 강제 언어 삭제 . 한글 파일 읽어오기 . Sys.setlocale(&quot;LC_ALL&quot;,&quot;Korean&quot;) #언어 한글로 exercise &lt;- read_csv(&quot;data/exercise.csv&quot;) exercise #?guess_encoding # 명령어로 인코딩 찾기 guess_encoding(&quot;data/exercise.csv&quot;) # 파일을 읽을 때 인코딩을 입력 exercise &lt;- read_csv(&quot;data/exercise.csv&quot;, locale = locale(encoding=&#39;EUC-KR&#39;)) exercise . locale에 따른 한글 object의 저장 . Sys.setlocale(&quot;LC_ALL&quot;,&quot;Korean&quot;) #언어 한글로 a &lt;- &quot;한글&quot;; print(a) exercise$이름; #데이터프레임에서 한글 컬럼 불러오기 Sys.setlocale(&quot;LC_ALL&quot;,&quot;English&quot;) a &lt;- &quot;한글&quot;; print(a) exercise$이름; . writing to a file . getwd() heights &lt;- read_csv(&quot;data/heights.csv&quot;) write_csv(heights, &quot;data/heights_saved.csv&quot;) . ‘write_rds()’ and ‘read_rds()’ are uniform wrappers around the base functions readRDS() and saveRDS(). | Thses store data in R’s custom binary format called RDS; . write_rds(heights, &quot;data/heights.rds&quot;) read_rds(&quot;data/heights.rds&quot;) . The feather package implements a fast binary file format that can be shared across programming languages: | #install.packages(&quot;feather&quot;) library(feather) write_feather(heights, &quot;data/heights.feather&quot;) read_feather(&quot;data/heights.feather&quot;) . file.remove(&quot;data/heights_saved.csv&quot;) . Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_3a_import_data.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_3a_import_data.Rmd",
          "date": ""
      }
      
  

  
      ,"page13": {
          "title": "week_3c_data_tidying.Rmd",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . ## R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;http://rmarkdown.rstudio.com&gt;. When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: ### Prerequisites {r} library(tidyverse) . ###Tidy data . table1 table2 table3 # Spread acorss two tibbles table4a #cases table4b # population . 1.각 variable은 각각의 column(열)을 가져야 한다. 2.각 관측치는 각각의 row(행)을 가져야 한다. 3.각 value 값을 각각의 cell을 가져야 한다. . #table1만 tidy한 데이터이다. . table1 #compute rate per 10,000 table1 %&gt;% mutate(rate = cases / population * 10000) #같은 작업을 table 2로 해보기 table1 table2 #case 와 population을 뽑아줄 땐 pivot_wider() #table2 %&gt;% pivot_wider() %&gt;% mutate(rate = cases / population * 10000) #compute cases per year table1 %&gt;% count(year, wt = cases) #wt - weight table1 %&gt;% group_by(year) %&gt;% summarise(n=sum(cases)) # visualise changes over time # 년도에 따라 확진자 수를 보고 싶을 때 table1 ggplot(table1, aes(x=year,y=cases)) + geom_line(aes(group=country), colour = &quot;grey50&quot;)+ geom_point(aes(colour = country, shape = country))+ scale_x_continuous(breaks = c(1999,2000)) . Pivoting . tidyr: pivot_longer() and pivot_wider(). #새로운 row를 만들땐 pivot_longer(), 새로운 col을 만들땐 pivot_wider() Longer . {r tidy-pivor-longer-plot-lines, fig.width = 5, echo=FALSE} table4a . View(table4a) . tidy4a &lt;- table4a %&gt;% pivot_longer( cols = c(‘1999’,’2000’),#!country names_to = “year”, values_to = “cases” ) %&gt;% mutate(year = parse_integer(year)) #int로 바꿔줌 . tidy4a . View(tidy4a) . tidy4a %&gt;% ggplot(aes(x=year, y = cases))+ geom_line(colour = “grey50”) + geom_point(aes(colour = country, shape = country)) + scale_x_continuous(breaks = c(1999,2000)) . table4b . View(table4b) . tidy4b &lt;- table4b %&gt;% pivot_longer( cols = c(‘1999’,’2000’),#!country names_to = “year”, values_to = “population” ) %&gt;% mutate(year = parse_integer(year)) . tidy4b . View(tidy4b) . 옆으로 붙여주기 . #1 tidy4a$population &lt;- tidy4b$population #2 left_join(tidy4a, tidy4b) . left_join 키 값을 입력해줌(country, year) . tidy4a %&gt;% left_join(tidy4b, by = c(“country”, “year”)) tidy4a . ### Wider() `pivot_wider()` is the opposite of `pivot_longer()`. # type 안에 있는 cases, population 변수들을 분리하여 새로운 컬럼으로 만들고 그 컬럼 안에 count 변수를 넣는다. {r} table2 # View(tidy2) table2 %&gt;% pivot_wider(names_from = type, #type의 변수를 col로 values_from = count) %&gt;% # col의 변수들은 count의 변수로 채움 mutate(rate = cases / population) tidy2 &lt;- table2 %&gt;% pivot_wider(names_from = type, values_from = count) %&gt;% mutate(rate = cases / population) tidy2 tidy2 %&gt;% ggplot(aes(x=year, y = rate))+ geom_line(aes(group = country), colour = &quot;grey50&quot;)+ geom_point(aes(colour = country, shape = &quot;country&quot;))+ scale_x_continuous(breaks=c(1999,2000)) . Advanced Pivoting(Missing values) . #remotes::install_github(&quot;dcl-docs/dcldata&quot;) library(dcldata) . example_migration . NA 값이 있을 때 pivot_longer의 동작 . example_migration %&gt;% pivot_longer(cols = !dest, names_to = &quot;origin&quot;, values_to = &quot;migrants&quot;) . values_drop_na to exclude these rows. . # 가로로 긴 데이터를 세로로 늘리는 것이기 때문에 # pivot_longer() 를 사용 example_migration %&gt;% pivot_longer(cols = !dest, names_to = &quot;origin&quot;, values_to = &quot;migrants&quot;, values_drop_na = TRUE ) # 혹은 파이프라인을 하나 더 붙여도 됨 example_migration %&gt;% pivot_longer(cols = !dest, names_to = &quot;origin&quot;, values_to = &quot;migrants&quot;) %&gt;% drop_na(migrants) . Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_3c_data_tidying.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_3c_data_tidying.Rmd",
          "date": ""
      }
      
  

  
      ,"page14": {
          "title": "data_visualization",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . {r} library(tidyverse) . ###2. 2. Aesthetic mappings mpg 데이터프레임 안에 class 변수와 같이 x축, y축이 아닌 세 번째 변수를 aesthetic mapping하여 2차원 그래프에 추가할 수 있습니다. 여기서 말하는 aesthetic mapping은 aes() argument에 포함되는 부분으로 크기(size), 모양(shape) 또는 색깔(color, colour) 등 이 포함됩니다. . ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) . #mapping 생략 가능 ggplot(mpg) + geom_point(aes(displ,hwy)) . ggplot(data = mpg, mapping = aes(x = displ, y = hwy))+ geom_point()+ geom_point(data = mpg %&gt;% filter(displ &gt; 5, hwy &gt; 20),color = &quot;red&quot;,size=2.2) . Aesthetic mappings . table(mpg$class) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = class)) . ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, size = class)) . # Left ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, alpha = class)) . # Right ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, shape = class)) . ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), col = &quot;blue&quot;) . Common problems . # + 위치에 따라 오류가 날 수 있음 # ggplot(data = mpg) + # geom_point(mapping = aes(x = displ, y = hwy)) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) . 2. 3. Facets . aes() argument를 이용하여 2차원 외에 추가 변수를 고려하는 방법도 있지만, 범주형 변수에 유용한 또 다른 방법이 존재합니다. 즉, 범주형 변수의 수준에 따라 그래프를 분할하여 표시하는 방법입니다. 이 때 이용하는 대표적인 함수로는 facet_wrap()과 facet_grid() 가 있습니다. facet_wrap(~ variable) 함수는 이산형(범주형) 변수일 때 쓰입니다. facet_grid(variable ~ variable) 함수는 두 변수까지 고려할 수 있습니다. . table(mpg$class) ggplot(data=mpg %&gt;% filter(class==&quot;midsize&quot;)) + geom_point(mapping = aes(x=displ, y = hwy)) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_wrap(~ class, nrow = 2) . table(mpg$cyl) ggplot(data = mpg)+ geom_point(mapping = aes(x = displ, y = hwy))+ facet_grid(drv~cyl) . 2. 4. Geometric objects . geom_smooth()는 ggplot2 패키지에서 추세선(trend line)을 그려 주는 함수입니다. 특히 산포도(산점도, scatter plot)를 그릴 때 xy평면에 산개한 점들의 추세를 함께 나타낼 경우에 많이 사용됩니다. . #geom_point()로 산점도를 먼저 그리고, geom_smooth()를 추가하여 추세선을 함께 그린 것 # left ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) # right ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy)) . # aes() 내부에 linetype = drv argument를 사용하여 변수 drv 수준에 따라 smooth line을 다르게 하여 출력할 수도 있습니다. ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv)) ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy)) ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, group = drv)) #또한 show.legend = FALSE argument를 이용하여 범례 없이 출력할 수도 있습니다. ggplot(data = mpg) + geom_smooth( mapping = aes(x = displ, y = hwy, color = drv), show.legend = FALSE ) ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv)) # View(mpg) . 동일한 그림에 여러 개의 geom을 표시하려면 여러 개의 geom 함수를 ggplot()에 추가합니다. . ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + geom_smooth(mapping = aes(x = displ, y = hwy)) . ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point() + geom_smooth() ggplot(data = mpg, mapping = aes(x = displ, y = cty)) + geom_point() + geom_smooth() . ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point(mapping = aes(color = class)) + geom_smooth() . ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = class)) + geom_point() + geom_smooth() . # 여기서 우리의 평활선 은 mpg 데이터셋의 서브셋인 (subcompact)경차만을 표시한다. ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point(mapping = aes(color = class)) + geom_smooth(data = mpg %&gt;% filter(class == &quot;subcompact&quot;), se = FALSE) #se = : 표준오차 . mpg %&gt;% View() . #Statistical transformations . diamonds table(diamonds$cut) glimpse(diamonds) . ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut)) #y축은 입력안해도댐 . # 값을 계산해서 넣어줌 ggplot(data = diamonds) + stat_count(mapping = aes(x = cut)) . Position adjustments . # 테두리의 컬러 ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, colour = cut)) . # 막대 컬러 ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = cut)) . # x와 fill 각각의 컬러 ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity)) . # position = &quot;identity&quot; 를 하면 각 객체를 그래프 문맥에 해당되는 곳에 정확히 배치한다. 막대와 겹치기 때문에 막대에 대해서는 그다지 유용하지 않다. ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + geom_bar(alpha = 1/5, position = &quot;identity&quot;) . # 겹치는 것을 구별하려면 alpha를 작은 값으로 설정하여 막대들을 약간 투명하게 하거나, fill = NA 로 설정하여 완전히 투명하게 해야 한다. ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + geom_bar(fill = NA, position = &quot;identity&quot;) . # 비율의 차이가 보고 싶을 때 ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;fill&quot;) . # dodge 각각의 비율을 볼 수 있음 ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;dodge&quot;) . ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), position = &quot;jitter&quot;) . # 데이터가 더 퍼지게 해줌 ggplot(data = mpg) + geom_jitter(mapping = aes(x = displ, y = hwy)) . Coordinate systems . ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() . ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() + coord_flip() . library(maps) nz &lt;- map_data(&quot;nz&quot;) ggplot(nz, aes(long, lat, group = group)) + geom_polygon(fill = &quot;white&quot;, colour = &quot;black&quot;) . ggplot(nz, aes(long, lat, group = group)) + geom_polygon(fill = &quot;white&quot;, colour = &quot;black&quot;) + coord_quickmap() . bar &lt;- ggplot(data = diamonds) + geom_bar( mapping = aes(x = cut, fill = cut), show.legend = FALSE, width = 1 ) + theme(aspect.ratio = 1) + labs(x = NULL, y = NULL) bar . bar + coord_flip() . bar + coord_polar() .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_4a_ggplot.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_4a_ggplot.Rmd",
          "date": ""
      }
      
  

  
      ,"page15": {
          "title": "week_4b_plotly.Rmd",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . ## R Markdown {r} # install.packages(&quot;plotly&quot;) . {r, message=FALSE} library(tidyverse) library(knitr) library(plotly) library(readxl) library(scales) . # Load dataset {r} mtcars &lt;- mtcars mtcars . vehicles &lt;- c(&quot;Four Cylinders&quot;,&quot;Six Cylinders&quot;,&quot;Eight Cylinders&quot;) cylinders &lt;- c(sum(mtcars$cyl==4), sum(mtcars$cyl==6), sum(mtcars$cyl==8)) vehicles cylinders . # 하나의 데이터프레임으로 묶음 veh_cyl &lt;- tibble(vehicles, cylinders) veh_cyl . # plot_ly(data= , x = , y = , type = ) # plot_ly는 %&gt;% 를 지원함. plot_ly(data = veh_cyl, x = vehicles, y = cylinders, type = &quot;bar&quot;, text = cylinders, textposition = &quot;auto&quot;) %&gt;% layout(title = &quot;Number of Vehicles in mtcars with 4,6, and 8 Cylinders&quot;, titlefont = list(size = 28, color = &quot;orange&quot;, family = &quot;Calibri&quot;), yaxis = list(title = &quot;Number of Vehicles&quot;, titlefont = list(color = &quot;black&quot;, family = &quot;Arial&quot;, size = 26), tickfont = list(color = &quot;black&quot;, family = &quot;Arial&quot;, size = 20)), xaxis = list(title = &quot;Number of Cylinders&quot;, titlefont = list(color = &quot;red&quot;, family = &quot;Times New Roman&quot;, size = 22), tickfont = list(color = &quot;green&quot;, family = &quot;Cambria&quot;, size = 18)))%&gt;% layout(margin = list( l = 10, r = 10, b = 0, t = 40)) #margin : 여백 . plot_ly(data = veh_cyl, x = vehicles, y = cylinders, type = &quot;bar&quot;, text = paste0(&quot;See &quot;,cylinders)) . Graph a Timeseries in Plotly . library(nycflights13) . head(flights) #일자별로 그룹 dep.delay.by.day &lt;- flights %&gt;% group_by(day) %&gt;% summarise (mean_dep_delay=mean(dep_delay,na.rm=T)) # 가장 기본적인 plotly 시각화 plot_ly( data = dep.delay.by.day, x = ~day, y =~mean_dep_delay) %&gt;% add_trace(type = &quot;scatter&quot; ,mode = &quot;lines+markers&quot;) # add_trace : 추가적으로 그릴 것? . # 다양한 customize plot_ly( data = dep.delay.by.day, x = ~day, y =~mean_dep_delay, text = ~paste(&#39;&lt;/br&gt; Day: &#39;, day, &#39;&lt;/br&gt; Departure Delay: &#39;, round(mean_dep_delay,3)), hoverinfo = &quot;text&quot;) %&gt;% add_trace(type = &quot;scatter&quot; , mode = &quot;lines+markers&quot;) %&gt;% layout( title = &quot;Departure delay by day&quot;, xaxis = list( title = &quot;Day&quot;), yaxis = list( title = &quot;Average departure delay&quot;)) . Animation with plotly . df &lt;- data.frame( x = c(1,2,1), y = c(1,2,1), f = c(1,2,3) ) fig &lt;- df %&gt;% plot_ly( x = ~x, y = ~y, frame = ~f, type = &#39;scatter&#39;, mode = &#39;markers&#39;, showlegend = F ) fig . Mulitple Trace Animations . #install.packages(&quot;gapminder&quot;) library(gapminder) #&gt; Warning: package &#39;gapminder&#39; was built under R version 4.0.5 df &lt;- gapminder # No animation fig &lt;- df %&gt;% plot_ly( x = ~gdpPercap, y = ~lifeExp, size = ~pop, color = ~continent, text = ~country, frame = ~year, hoverinfo = &quot;text&quot;, type = &#39;scatter&#39;, mode = &#39;markers&#39;, fill = ~&#39;&#39; ) %&gt;% layout( xaxis = list(type = &quot;log&quot;)) fig . # With animation fig &lt;- df %&gt;% plot_ly( x = ~gdpPercap, y = ~lifeExp, size = ~pop, color = ~continent, frame = ~year, text = ~country, hoverinfo = &quot;text&quot;, type = &#39;scatter&#39;, mode = &#39;markers&#39;, fill = ~&#39;&#39; ) %&gt;% layout( xaxis = list(type = &quot;log&quot;)) fig . Add Animation Options . fig &lt;- fig %&gt;% animation_opts( 1000, easing = &quot;elastic&quot;, redraw = FALSE ) fig . Add Button Options . fig &lt;- fig %&gt;% animation_button( x = 1, xanchor = &quot;right&quot;, y = 0, yanchor = &quot;bottom&quot; ) fig . Add Slider Options . fig &lt;- fig %&gt;% animation_slider( currentvalue = list(prefix = &quot;YEAR &quot;, font = list(color=&quot;red&quot;)) ) fig . ggplotly . # ggplot과 plotly를 같이 사용 #느림 잘 안씀 # p1&lt;- ggplot(data = diamonds) + # geom_bar(mapping = aes(x = clarity, fill = cut), position = &quot;fill&quot;) # p1 . 예제 연습 . #https://plotly.com/r/ . View(diamonds) fig &lt;- plot_ly(ggplot2::diamonds, y = ~price, color = ~cut, type = &quot;box&quot;) fig . Dropdown menu . library(MASS) covmat &lt;- matrix(c(0.8,0.4,0.3,0.8), nrow = 2, byrow= T) df &lt;- mvrnorm(n = 10000, c(0,0), Sigma = covmat) df &lt;- as.data.frame(df) colnames(df) &lt;- c(&quot;x&quot;,&quot;y&quot;) fig &lt;- plot_ly(df, x = ~x, y = ~y, alpha = 0.3) fig fig &lt;- fig %&gt;% add_markers(marker = list(line = list(color = &quot;black&quot;, width = 1))) fig . Add dropdown . fig &lt;- fig %&gt;% layout( title = &quot;Drop down menus - plot type&quot;, xaxis = list(domain = c(0.1,1)), yaxis = list(title = &quot;y&quot;), updatemenus = list( list( y = 0.8, buttons = list( list(method = &quot;restyle&quot;, args = list(&quot;type&quot;, &quot;scatter&quot;), label= &quot;Scatter&quot;), list(method = &quot;restyle&quot;, args = list(&quot;type&quot;,&quot;histogram2d&quot;), label = &quot;2D Histogram&quot;))))) fig . # library(plotly) dens &lt;- with(diamonds, tapply(price, INDEX = cut, density)) df &lt;- data.frame( x = unlist(lapply(dens, &quot;[[&quot;, &quot;x&quot;)), y = unlist(lapply(dens, &quot;[[&quot;, &quot;y&quot;)), cut = rep(names(dens), each = length(dens[[1]]$x)) ) fig &lt;- plot_ly(df, x = ~x, y = ~y, color = ~cut) fig &lt;- fig %&gt;% add_lines() fig .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_4b_plotly.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_4b_plotly.Rmd",
          "date": ""
      }
      
  

  
      ,"page16": {
          "title": "Spatial_and_Geometry_operation",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . ## Prerequisites - This chapter requires the following packages to be installed and attached: {r, message=FALSE} library(sf) library(dplyr) library(stringr) # for working with strings (pattern matching) library(tidyr) # for unite() and separate() library(spData) library(spDataLarge) . Spatial operations on vector data . This section provides an overview of spatial operations on vector geographic data represented as simple features in the sf package. . Spatial subsetting . Spatial subsetting is the process of selecting features of a spatial object based on whether or not they in some way relate in space to another object. . It is analogous to attribute subsetting (covered in Section @ref(vector-attribute-subsetting)) and can be done with the base R square bracket ([) operator or with the filter() function from the tidyverse index{tidyverse (package)}. . An example of spatial subsetting is provided by the nz and nz_height datasets in spData. . These contain projected data on the 16 main regions and 101 highest points in New Zealand, respectively. . The following code chunk first creates an object representing Canterbury, then uses spatial subsetting to return all high points in the region: . # 간단한 데이터 설명 # 뉴질랜드 지역의 16개 지역과 101개의 높은 지점을 나타내는 Spatial data data(nz) nz_height data(nz_height) class(nz) class(nz_height) plot(nz[1], reset = FALSE) plot(nz_height[2], add=TRUE) . {r 04-spatial-operations-3} . 16개의 main regions . canterbury = nz %&gt;% filter(Name == “Canterbury”) . 101개의 highest points . canterbury_height = nz_height[canterbury, ] . {r} # install.packages(&quot;tmap&quot;) library(tmap) tmap_mode(&quot;plot&quot;) # tmap_mode(&quot;view&quot;) p_hpnz1 = tm_shape(nz) + tm_polygons(col = &quot;white&quot;) + tm_shape(nz_height) + tm_symbols(shape = 2, col = &quot;red&quot;, size = 0.25) + tm_layout(main.title = &quot;High points in New Zealand&quot;, main.title.size = 1, bg.color = &quot;lightblue&quot;) p_hpnz2 = tm_shape(nz) + tm_polygons(col = &quot;white&quot;) + tm_shape(canterbury) + tm_fill(col = &quot;gray&quot;) + tm_shape(canterbury_height) + tm_symbols(shape = 2, col = &quot;red&quot;, size = 0.25) + tm_layout(main.title = &quot;High points in Canterbury&quot;, main.title.size = 1, bg.color = &quot;lightblue&quot;) tmap_arrange(p_hpnz1, p_hpnz2, ncol = 1) . Like attribute subsetting x[y, ] subsets features of a target x using the contents of a source object y. Instead of y being of class logical or integer — a vector of TRUE and FALSE values or whole numbers — for spatial subsetting it is another spatial (sf) object. . Various topological relations can be used for spatial subsetting. These determine the type of spatial relationship that features in the target object must have with the subsetting object to be selected, including touches, crosses or within (see Section @ref(topological-relations)). Intersects is the default spatial subsetting operator, a default that returns TRUE for many types of spatial relations, including touches, crosses and is within. . These alternative spatial operators can be specified with the op = argument, a third argument that can be passed to the [ operator for sf objects. This is demonstrated in the following command which returns the opposite of st_intersects(), points that do not intersect with Canterbury (see Section @ref(topological-relations)): . {r 04-spatial-operations-4, eval=FALSE} nz_height[canterbury, , op = st_disjoint] . 여기서 비어있는 , ,부분은 컬럼을 의미함. 비어놓을 경우 모든 컬럼을 다 가져옴 . names(nz_height) nz_height[canterbury, 2, op = st_disjoint] . For many applications, this is all you&#39;ll need to know about spatial subsetting for vector data. In this case, you can safely skip to Section @ref(topological-relations). If you&#39;re interested in the details, including other ways of subsetting, read on. Another way of doing spatial subsetting uses objects returned by *topological operators*. This is demonstrated in the first command below: {r} sel_sgbp = st_intersects(x = nz_height, y = canterbury) class(sel_sgbp) sel_logical = lengths(sel_sgbp) &gt; 0 canterbury_height2 = nz_height[sel_logical, ] . In the above code chunk, an object of class sgbp (a sparse geometry binary predicate, a list of length x in the spatial operation) is created and then converted into a logical vector sel_logical (containing only TRUE and FALSE values). index{binary predicate|seealso {topological relations}} The function lengths() identifies which features in nz_height intersect with any objects in y. In this case 1 is the greatest possible value but for more complex operations one could use the method to subset only features that intersect with, for example, 2 or more features from the source object. . It should be noted that a logical can also be used with filter() as follows (sparse = FALSE is explained in Section @ref(topological-relations)): . canterbury_height3 = nz_height %&gt;% filter(st_intersects(x = ., y = canterbury, sparse = FALSE)) . At this point, there are three versions of canterbury_height, one created with spatial subsetting directly and the other two via intermediary selection objects. To explore these objects and spatial subsetting in more detail, see the supplementary vignettes on subsetting and tidyverse-pitfalls. . Spatial joining . Joining two non-spatial datasets relies on a shared ‘key’ variable, as described in Section @ref(vector-attribute-joining). . Spatial data joining applies the same concept, but instead relies on shared areas of geographic space (it is also know as spatial overlay). As with attribute data, joining adds a new column to the target object (the argument x in joining functions), from a source object (y). . The process can be illustrated by an example. Imagine you have ten points randomly distributed across the Earth’s surface. Of the points that are on land, which countries are they in? Random points to demonstrate spatial joining are created as follows: . {r 04-spatial-operations-19} set.seed(2018) # set seed for reproducibility data(world) . plot(world[1], col=”grey”) . (bb_world = st_bbox(world)) # the world’s bounds . random_df = tibble( x = runif(n = 10, min = bb_world[1], max = bb_world[3]), y = runif(n = 10, min = bb_world[2], max = bb_world[4]) ) . random_points = random_df %&gt;% st_as_sf(coords = c(“x”, “y”)) %&gt;% # set coordinates st_set_crs(4326) # set geographic CRS . plot(world[1],reset=FALSE, col=”grey”) plot(random_points,add=TRUE,col=”red”,cex=1) . The scenario is illustrated in Figure @ref(fig:spatial-join). The `random_points` object (top left) has no attribute data, while the `world` (top right) does. The spatial join operation is done by `st_join()`, which adds the `name_long` variable to the points, resulting in `random_joined` which is illustrated in Figure @ref(fig:spatial-join) (bottom left see [`04-spatial-join.R`](https://github.com/Robinlovelace/geocompr/blob/master/code/04-spatial-join.R)). Before creating the joined dataset, we use spatial subsetting to create `world_random`, which contains only countries that contain random points, to verify the number of country names returned in the joined dataset should be four (see the top right panel of Figure @ref(fig:spatial-join)). {r 04-spatial-operations-20, message=FALSE} # random points가 위치한 국가만 가지고옴 world_random = world[random_points, ] nrow(world_random) # 시각화 plot(world[1],col=&quot;grey&quot;,reset=FALSE) plot(world_random,add=TRUE,col=&quot;red&quot;) # 현재 Random points는 어떠한 속성도 가지고 있지 않음 # st_join은 random_points에 world object에 있는 속성을 붙여줌 random_joined = st_join(random_points, world[&quot;name_long&quot;]) random_joined # left=FALSE 명령어를 쓸 경우 join이 성공한 row만을 남겨줌 random_joined = st_join(random_points, world[&quot;name_long&quot;],left = FALSE) random_joined . By default, st_join() performs a left join (see Section @ref(vector-attribute-joining)), but it can also do inner joins by setting the argument left = FALSE. . Like spatial subsetting, the default topological operator used by st_join() is st_intersects(). . This can be changed with the join argument (see ?st_join for details). In the example above, we have added features of a polygon layer to a point layer. In other cases, we might want to join point attributes to a polygon layer. There might be occasions where more than one point falls inside one polygon. In such a case st_join() duplicates the polygon feature: it creates a new row for each match. . Non-overlapping joins . Sometimes two geographic datasets do not touch but still have a strong geographic relationship enabling joins. . The datasets cycle_hire and cycle_hire_osm, already attached in the spData package, provide a good example. . Plotting them shows that they are often closely related but they do not touch, as shown in Figure @ref(fig:cycle-hire), a base version of which is created with the following code below: index{join!non-overlapping} . {r 04-spatial-operations-21, eval=FALSE} plot(st_geometry(cycle_hire), col = “blue”) plot(st_geometry(cycle_hire_osm), add = TRUE, pch = 3, col = “red”) . We can check if any points are the same `st_intersects()` as shown below: {r 04-spatial-operations-22, message=FALSE} # blue와 red point중에 서로 겹치는 point가 없음 # 여기서 point와 point가 겹친다는 것은, 두 점이 정확하게 일치하는 것을 의미 any(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE)) st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE) %&gt;% sum() . {r 04-spatial-operations-23, echo=FALSE, eval=FALSE} . included to show alternative ways of showing there’s no overlap . sum(st_geometry(cycle_hire) %in% st_geometry(cycle_hire_osm)) sum(st_coordinates(cycle_hire)[, 1] %in% st_coordinates(cycle_hire_osm)[, 1]) . {r} library(leaflet) leaflet() %&gt;% # addProviderTiles(providers$OpenStreetMap.BlackAndWhite) %&gt;% addCircles(data = cycle_hire) %&gt;% addCircles(data = cycle_hire_osm, col = &quot;red&quot;) . Imagine that we need to join the capacity variable in cycle_hire_osm onto the official ‘target’ data contained in cycle_hire. This is when a non-overlapping join is needed. . names(cycle_hire) names(cycle_hire_osm) . The simplest method is to use the topological operator st_is_within_distance() shown in Section @ref(topological-relations), using a threshold distance of 20 m. . Note that, before performing the relation, both objects are transformed into a projected CRS. . These projected objects are created below (note the affix _P, short for projected): . {r 04-spatial-operations-24} . 위경도 좌표계이기 때문에 Projected CRS로 변환해줘야함 . 좌표계 단위가 도(degree)에서 meter로 변환됨 . 좌표계 보기 . data(cycle_hire_osm) data(cycle_hire) . cycle_hire %&gt;% st_crs() cycle_hire_osm %&gt;% st_crs() . 좌표계 변환 . cycle_hire_P = st_transform(cycle_hire, 27700) cycle_hire_osm_P = st_transform(cycle_hire_osm, 27700) . 20m 내에 있는 point들을 join 시킴 . sel = st_is_within_distance(cycle_hire_P, cycle_hire_osm_P, dist = 20) summary(lengths(sel) &gt; 0) . This shows that there are `r sum(lengths(sel) &gt; 0)` points in the target object `cycle_hire_P` within the threshold distance of `cycle_hire_osm_P`. How to retrieve the *values* associated with the respective `cycle_hire_osm_P` points? The solution is again with `st_join()`, but with an addition `dist` argument (set to 20 m below): {r 04-spatial-operations-25} # st_join 명령어에서 dist argument를 넣어줌 z = st_join(cycle_hire_P, cycle_hire_osm_P, join = st_is_within_distance, dist = 20) # 행의수가 증가하였음 # cycle_hire_P와 cycle_hire_osm_P 간에 여러개가 매칭된 경우가 발생 nrow(cycle_hire) nrow(z) . Note that the number of rows in the joined result is greater than the target. This is because some cycle hire stations in cycle_hire_P have multiple matches in cycle_hire_osm_P. . To aggregate the values for the overlapping points and return the mean, we can use the aggregation methods learned in Chapter @ref(attr), resulting in an object with the same number of rows as the target: . {r 04-spatial-operations-26} z = z %&gt;% group_by(id) %&gt;% summarize(capacity = mean(capacity)) . nrow(z) == nrow(cycle_hire) . The capacity of nearby stations can be verified by comparing a plot of the capacity of the source `cycle_hire_osm` data with the results in this new object (plots not shown): {r 04-spatial-operations-27, eval=FALSE} plot(cycle_hire_osm[&quot;capacity&quot;]) plot(z[&quot;capacity&quot;]) . The result of this join has used a spatial operation to change the attribute data associated with simple features; the geometry associated with each feature has remained unchanged. . Spatial data aggregation . Like attribute data aggregation, covered in Section @ref(vector-attribute-aggregation), spatial data aggregation can be a way of condensing data. . Aggregated data show some statistics about a variable (typically average or total) in relation to some kind of grouping variable. Section @ref(vector-attribute-aggregation) demonstrated how aggregate() and group_by() %&gt;% summarize() condense data based on attribute variables. This section demonstrates how the same functions work using spatial grouping variables. . Returning to the example of New Zealand, imagine you want to find out the average height of high points in each region. This is a good example of spatial aggregation: it is the geometry of the source (y or nz in this case) that defines how values in the target object (x or nz_height) are grouped. This is illustrated using the base aggregate() function below: . {r 04-spatial-operations-28} . 101개의 point 데이터, 뉴질랜드에서의 높은 지역 . plot(nz[1],reset=FALSE,col=”grey”) plot(nz_height[1], add=TRUE) . names(nz_height) nrow(nz_height) . nz_avheight = aggregate(x = nz_height, by = nz, FUN = mean) . The same result can also be generated using the &#39;tidy&#39; functions `group_by()` and `summarize()` (used in combination with `st_join()`): {r} library(tmap) tm_shape(nz_avheight) + tm_fill(&quot;elevation&quot;, breaks = seq(27, 30, by = 0.5) * 1e2) + tm_borders() . {r 04-spatial-operations-29} . aggregate 함수 대신 st_join 함수를 써도 똑같이 할 수 있음 . nz_avheight2 = nz %&gt;% st_join(nz_height) %&gt;% group_by(Name) %&gt;% summarize(elevation = mean(elevation, na.rm = TRUE)) . The resulting `nz_avheight` objects have the same geometry as the aggregating object `nz` but with a new column representing the mean average height of points within each region of New Zealand (other summary functions such as `median()` and `sd()` can be used in place of `mean()`). Note that regions containing no points have an associated `elevation` value of `NA`. For aggregating operations which also create new geometries, see Section @ref(geometry-unions). ### Distance relations While topological relations are binary a feature either intersects with another or does not distance relations are continuous. The distance between two objects is calculated with the `st_distance()` function. This is illustrated in the code chunk below, which finds the distance between the highest point in New Zealand and the geographic centroid of the Canterbury region, created in Section @ref(spatial-subsetting): index{sf!distance relations} {r} # topological relation은 binary 형태로 표현됨 # 예를들어, 두 geometry 데이터가 겹친다 혹은 안겹친다, 이런식으로 표현됨 # 하지만 두 geometry 사이의 거리는 binary가 아니라 continuous한 형태임 # st_distance() 명령어를 통해 두 objects 사이의 거리 계산이 가능 # 뉴질랜드에서 가장 높은 point 추출 nz_heighest = nz_height %&gt;% top_n(n = 1, wt = elevation) # cantebury 지역의 centroid 좌표 추출 canterbury = nz %&gt;% filter(Name == &quot;Canterbury&quot;) canterbury_centroid = st_centroid(canterbury) # 두 점 사이의 거리 계산 st_distance(nz_heighest, canterbury_centroid) . There are two potentially surprising things about the result: . It has units, telling us the distance is 100,000 meters, not 100,000 inches, or any other measure of distance . | It is returned as a matrix, even though the result only contains a single value . | . This second feature hints at another useful feature of st_distance(), its ability to return distance matrices between all combinations of features in objects x and y. . This is illustrated in the command below, which finds the distances between the first three features in nz_height and the Otago and Canterbury regions of New Zealand represented by the object co. . {r 04-spatial-operations-32} . 여러개의 points들 사이의 거리 계산이 가능 . co = filter(nz, grepl(“Canter|Otag”, Name)) st_distance(nz_height[1:3, ], co) . Note that the distance between the second and third features in `nz_height` and the second feature in `co` is zero. This demonstrates the fact that distances between points and polygons refer to the distance to *any part of the polygon*: The second and third points in `nz_height` are *in* Otago, which can be verified by plotting them (result not shown): {r 04-spatial-operations-33, eval=FALSE} # st_distance 명령어를 point와 polygon 대상으로 했을 때는, point가 polygon 안에 들어 있으면, distance를 0으로 계산함 plot(st_geometry(co)[2]) plot(st_geometry(nz_height)[2:3], add = TRUE) .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_5a_spatial_operation.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_5a_spatial_operation.Rmd",
          "date": ""
      }
      
  

  
      ,"page17": {
          "title": "Spatial_and_Geometry_operation",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . ## 필수 조건 - 이 장에서는 다음 패키지를 설치하고 첨부해야 합니다. {r, message=FALSE} library(sf) library(dplyr) library(stringr) # for working with strings (pattern matching) library(tidyr) # for unite() and separate() library(spData) library(spDataLarge) . 벡터 데이터 {#spacial-vec}에 대한 공간 작업 . 이 절에서는 sf 패키지에서 단순 기능으로 표현되는 벡터 지리 데이터에 대한 공간 작업에 대한 개요를 제공합니다. . 공간 부분 집합 설정 . 공간 부분 집합화(spacial subseting)는 공간 객체의 특징을 선택하는 과정이다. . 이것은 속성 부분 집합화(섹션 @ref(벡터-속성-부분 집합)와 유사하며, 기본 R 대괄호(‘[’) 연산자를 사용하거나 *tidyvers index{tidyvers(패키지)}의 ‘filter()’ 함수를 사용하여 수행할 수 있다. . 공간 하위 집합의 예는 *spData의 ‘nz’ 및 ‘nz_height’ 데이터 세트에 의해 제공된다. . 여기에는 뉴질랜드의 16개 주요 지역과 101개 가장 높은 지점에 대한 예측 데이터가 포함되어 있습니다. . 다음 코드 청크는 먼저 캔터베리를 나타내는 객체를 만든 다음 공간 하위 집합을 사용하여 영역의 모든 고점을 반환합니다. . # 간단한 데이터 설명 # 뉴질랜드 지역의 16개 지역과 101개의 높은 지점을 나타내는 Spatial data nz data(nz) nz_height data(nz_height) class(nz) class(nz_height) plot(nz[1], reset = FALSE) plot(nz_height[2], add=TRUE) . {r 04-spatial-operations-3} . 16개의 main regions . canterbury &lt;- nz %&gt;% filter(Name == “Canterbury”) . 101개의 highest points . canterbury_height &lt;- nz_height[canterbury, ] . {r} # install.packages(&quot;tmap&quot;) library(tmap) tmap_mode(&quot;plot&quot;) # tmap_mode(&quot;view&quot;) p_hpnz1 = tm_shape(nz) + tm_polygons(col = &quot;white&quot;) + tm_shape(nz_height) + tm_symbols(shape = 2, col = &quot;red&quot;, size = 0.25) + tm_layout(main.title = &quot;High points in New Zealand&quot;, main.title.size = 1, bg.color = &quot;lightblue&quot;) # gray부분으로 칠해진 곳만 추출되었음 p_hpnz2 = tm_shape(nz) + tm_polygons(col = &quot;white&quot;) + tm_shape(canterbury) + tm_fill(col = &quot;gray&quot;) + tm_shape(canterbury_height) + tm_symbols(shape = 2, col = &quot;red&quot;, size = 0.25) + tm_layout(main.title = &quot;High points in Canterbury&quot;, main.title.size = 1, bg.color = &quot;lightblue&quot;) tmap_arrange(p_hpnz1, p_hpnz2, ncol = 1) #그래프를 아래로 붙여줌 . 한 target xsource 개체 y의 내용을 이용해서의 특성 subsetting x[y,] 하위 집합 특징처럼. 공간 subsetting 대신 y 수업 logical 또는 integer의—의 TRUE과FALSE 값 또는 전체 수의 벡터-그것이 또 다른 공간(sf)개체입니다. . 다양한topological relations 공간 subsetting에 쓰일 수 있습니다. 이 건축 공간 관계의 대상 개체의 특징, 그 토대 타입은 subsetting 개체 선택할 수 있는 touches, crosses 또는within(섹션)@ 심판(topological-relations 보))등이 있어야 한다를 결정한다. touches, crosses과is within 포함 Intersects은 기본 공간subsetting 사업자인 수익 공간의 관계의 많은 유형에 대해 TRUE 기본이다. . 이 대체 공간 운영자들은op = 주장,sf 개체에 대한-LSB-연산자에 전달할 수 있는 세번째 논쟁으로 지정할 수 있다. 이것은 st_intersects()의 반대를 반환합니다 다음과 같은 명령, 캔터베리와 교차하지 않는 경우 포인트(섹션)@ 심판(topological-relations 보))에:증명된다. . {r 04-spatial-operations-4, eval=FALSE} #disjoint = canterbury에 속해있지 않은 부분 nz_height[canterbury, , op = st_disjoint] . 여기서 비어있는 , ,부분은 컬럼을 의미함. 비어놓을 경우 모든 컬럼을 다 가져옴 . names(nz_height) nz_height[canterbury, 2, op = st_disjoint] . 많은 응용 프로그램에서 벡터 데이터의 공간 부분 집합화에 대해 알아야 할 것은 이것뿐입니다. 이 경우 섹션 @ref(위상 관계)로 건너뛸 수 있습니다. 다른 부분 집합화 방법을 포함한 세부 사항에 관심이 있으면 계속 읽어 보십시오. 공간 부분 집합을 수행하는 또 다른 방법은 *위상 연산자*가 반환하는 개체를 사용합니다. 이는 아래의 첫 번째 명령에서 확인할 수 있습니다. {r} # 두 공간정보의 공통된 부분을 가져오고 싶을 때 sel_sgbp = st_intersects(x = nz_height, y = canterbury) class(sel_sgbp) # 겹치는 부분은 1이다. 따라서 1 이상인 곳들을 뽑는다 sel_logical = lengths(sel_sgbp) &gt; 0 canterbury_height2 = nz_height[sel_logical, ] #반대 # canterbury_height2 = nz_height[!sel_logical, ] . 위의 코드 청크에서 클래스 ‘sgbp’(희소 기하학 이진 술어, 공간 연산에서 길이 ‘x’의 목록)의 객체가 생성된 다음 논리적 벡터 ‘sel_logical’(‘TRUE’와 ‘FALSE’ 값만 포함)로 변환된다. index{유사 술어|{위상관계} 참조 함수 ‘lengths()는 ‘nz_height’의 어떤 형상이 ‘y’의 any 객체와 교차하는지 식별한다. 이 경우 1은 가능한 최대 값이지만 더 복잡한 연산의 경우, 예를 들어 소스 객체의 두 개 이상의 피쳐와 교차하는 피쳐만 부분 집합화하는 방법을 사용할 수 있다. . 논리는 다음과 같이 “filter”와 함께 사용될 수 있음에 유의해야 한다. (‘filter = FALSE’는 @ref(위상관계) 절에서 설명된다.) . canterbury_height3 = nz_height %&gt;% filter(st_intersects(x = ., y = canterbury, sparse = FALSE)) # x = . : nz_height %&gt;% 를 사용하여 생략가능. # sparse = FALSE : 바로 TRUE,FALSE 형태가 나옴 . 이 시점에서 공간 하위 집합으로 직접 만들고 중간 선택 개체를 통해 나머지 두 버전을 만드는 ‘cantterbury_height’의 세 가지 버전이 있다. 이러한 개체와 공간 하위 집합을 보다 자세히 탐색하려면 https://geocompr.github.io/geocompkg/articles/) 및 [version-develops]에 있는 추가 정보를 참조하십시오. . 공간 결합 . 두 개의 비공간 데이터 세트를 결합하려면 섹션 @ref(벡터 속성 결합)에 설명된 대로 공유 ‘키’ 변수에 의존한다. . 공간 데이터 결합은 동일한 개념을 적용하지만 대신 지리적 공간의 공유 영역(공간 오버레이라고도 함)에 의존한다. 속성 데이터와 마찬가지로 결합은 소스 객체(‘y’)에서 대상 객체(결합 함수의 인수 ‘x’)에 새로운 열을 추가한다. . 이 과정은 예를 들어 설명할 수 있다. 여러분이 지구 표면에 무작위로 10개의 점들이 분포되어 있다고 상상해 보세요. 육지에 있는 지점들 중, 어느 나라에 있나요? 공간 결합을 입증하기 위한 임의의 점은 다음과 같이 생성된다. . {r 04-spatial-operations-19} set.seed(2018) # set seed for reproducibility data(world) . plot(world[1], col=”grey”) . (bb_world = st_bbox(world)) # the world’s bounds . random_df = tibble( x = runif(n = 10, min = bb_world[1], max = bb_world[3]), y = runif(n = 10, min = bb_world[2], max = bb_world[4]) ) . random_points = random_df %&gt;% st_as_sf(coords = c(“x”, “y”)) %&gt;% # set coordinates st_set_crs(4326) # set geographic CRS . plot(world[1],reset=FALSE, col=”grey”) plot(random_points,add=TRUE,col=”red”,cex=1) . 시나리오는 그림 @ref(그림:spacial-join)에 나와 있습니다. random_points 객체(왼쪽 위)에는 속성 데이터가 없는 반면 world(오른쪽 위)에는 속성 데이터가 있습니다. 공간 결합 연산은 &#39;st_join()에 의해 수행되며, 이는 &#39;name_long&#39; 변수를 포인트에 추가함으로써 그림 @ref(그림:spacial-join)(왼쪽 하단 참조)에 표시된 &#39;random_join&#39;을 생성한다.R&#39;](https://github.com/Robinlovelace/geocompr/blob/master/code/04-spatial-join.R))의. 조인된 데이터 세트를 만들기 전에 공간 하위 집합을 사용하여 랜덤 점만 포함하는 &#39;world_random&#39;을 생성하여 조인된 데이터 세트에 반환된 국가 이름 수가 4개인지 확인합니다(그림 @ref(그림:spacial-join)의 오른쪽 상단 패널 참조). {r 04-spatial-operations-20, message=FALSE} # random points가 위치한 국가만 가지고옴 world_random = world[random_points, ] nrow(world_random) # 시각화 plot(world[1],col=&quot;grey&quot;,reset=FALSE) plot(world_random,add=TRUE,col=&quot;red&quot;) # 현재 Random points는 어떠한 속성도 가지고 있지 않음 # st_join은 random_points에 world object에 있는 속성을 붙여줌 random_joined = st_join(random_points, world[&quot;name_long&quot;]) random_joined # left=FALSE 명령어를 쓸 경우 join이 성공한 row만을 남겨줌 random_joined = st_join(random_points, world[&quot;name_long&quot;],left = FALSE) random_joined . 기본적으로 ‘st_parament’는 왼쪽 조인을 수행하지만(섹션 @ref(section @ref-parament-parament) 참조), ‘left = FALSE’ 인수를 설정하여 내부 조인을 수행할 수도 있습니다. . 공간 부분 집합과 마찬가지로, ‘st_join()이 사용하는 기본 위상 연산자는 ‘st_inters() rocked’이다. . 이것은 “join” 인수로 변경할 수 있습니다(자세한 내용은 “?st_join” 참조). 위의 예에서는 점 도면층에 폴리곤 도면층의 피쳐를 추가했습니다. 다른 경우에는 점 특성을 폴리곤 도면층에 결합할 수 있습니다. 두 개 이상의 점이 한 폴리곤 안에 포함되는 경우가 있을 수 있습니다. 이러한 경우 ‘st_join()’은 폴리곤 피쳐를 복제합니다. 즉, 각 일치 항목에 대해 새 행을 생성합니다. . 겹치지 않는 조인 . 때때로 두 개의 지리적 데이터 세트는 접촉하지 않지만 결합을 가능하게 하는 강력한 지리적 관계를 가지고 있다. . *spData 패키지에 이미 첨부되어 있는 데이터 세트 ‘cycle_hire’와 ‘cycle_hire_osm’이 좋은 예이다. . 그림 @ref(fig:cycle-hire)와 같이 이들을 표시하면 서로 밀접한 관련이 있지만 접촉하지는 않는다는 것을 알 수 있으며, 아래의 코드를 사용하여 기본 버전이 생성됩니다. index{discripts! 비중첩} . {r 04-spatial-operations-21, eval=FALSE} plot(st_geometry(cycle_hire), col = “blue”) plot(st_geometry(cycle_hire_osm), add = TRUE, pch = 3, col = “red”) . 우리는 어떤 점이 아래와 같이 같은 &#39;st_intersects()&#39;인지 확인할 수 있다. {r 04-spatial-operations-22, message=FALSE} # blue와 red point중에 서로 겹치는 point가 없음 # 여기서 point와 point가 겹친다는 것은, 두 점이 정확하게 일치하는 것을 의미 any(st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE)) st_touches(cycle_hire, cycle_hire_osm, sparse = FALSE) %&gt;% sum() . {r 04-spatial-operations-23, echo=FALSE, eval=FALSE} . included to show alternative ways of showing there’s no overlap . sum(st_geometry(cycle_hire) %in% st_geometry(cycle_hire_osm)) sum(st_coordinates(cycle_hire)[, 1] %in% st_coordinates(cycle_hire_osm)[, 1]) . {r} library(leaflet) leaflet() %&gt;% # addProviderTiles(providers$OpenStreetMap.BlackAndWhite) %&gt;% addCircles(data = cycle_hire) %&gt;% addCircles(data = cycle_hire_osm, col = &quot;red&quot;) . cycle_hire_osm의 capacity 변수를 cycle_hire에 포함된 공식적인 target 데이터에 결합해야 한다고 가정해 보자. 겹치지 않는 조인(join)이 필요한 경우입니다. . names(cycle_hire) names(cycle_hire_osm) . 가장 간단한 방법은 20m의 임계값 거리를 사용하여 섹션 @ref(위상 관계)에 표시된 위상 연산자 ‘st_is_within_distance()’를 사용하는 것이다. . 이 관계를 수행하기 전에 두 개체는 투영된 CRS로 변환됩니다. . 이러한 투영 객체는 아래에 생성된다(접사 ‘_P’는 투영 객체의 줄임말이다). . {r 04-spatial-operations-24} . 위경도 좌표계이기 때문에 Projected CRS로 변환해줘야함 . 좌표계 단위가 도(degree)에서 meter로 변환됨 . 좌표계 보기 . data(cycle_hire_osm) data(cycle_hire) . cycle_hire %&gt;% st_crs() cycle_hire_osm %&gt;% st_crs() . 좌표계 변환 . cycle_hire_P = st_transform(cycle_hire, 27700) cycle_hire_osm_P = st_transform(cycle_hire_osm, 27700) . 20m 내에 있는 point들을 join 시킴 . sel = st_is_within_distance(cycle_hire_P, cycle_hire_osm_P, dist = 20) summary(lengths(sel) &gt; 0) . 이는 대상 객체 cycle_hire_osm_P의 임계값 거리 내에 &#39;rsum(lengths(sel) &gt; 0)&#39; 점이 있음을 보여준다. 각 &#39;cycle_hire_osm_&#39;과 관련된 *값*을 검색하는 방법무슨 소리죠? 들려요? 해결책은 다시 &#39;st_join()을 사용하지만 추가 &#39;dist&#39; 인수를 사용한다(아래 20m로 설정). {r 04-spatial-operations-25} # st_join 명령어에서 dist argument를 넣어줌 z = st_join(cycle_hire_P, cycle_hire_osm_P, join = st_is_within_distance, dist = 20) # 행의수가 증가하였음 # cycle_hire_P와 cycle_hire_osm_P 간에 여러개가 매칭된 경우가 발생 nrow(cycle_hire) nrow(z) . 조인된 결과의 행 수가 목표값보다 큽니다. cycle_hire_P의 일부 사이클 대여 스테이션에서 cycle_hire_osm_P의 일치가 여러 개 있기 때문이다. . 중복되는 점의 값을 집계하고 평균을 반환하기 위해 @ref(attr) 장에서 학습한 집계 방법을 사용하면 대상과 같은 수의 행이 있는 개체를 만들 수 있다. . {r 04-spatial-operations-26} z = z %&gt;% group_by(id) %&gt;% summarize(capacity = mean(capacity)) . nrow(z) == nrow(cycle_hire) . 인근 스테이션의 용량은 소스 &#39;cycle_hire_osm&#39; 데이터의 용량 그림과 이 새 개체의 결과를 비교하여 확인할 수 있습니다(그림은 표시되지 않음). {r 04-spatial-operations-27, eval=FALSE} plot(cycle_hire_osm[&quot;capacity&quot;]) plot(z[&quot;capacity&quot;]) . 이 결합의 결과는 공간 연산을 사용하여 단순 형상과 관련된 속성 데이터를 변경했으며, 각 형상과 관련된 형상은 변경되지 않았다. . 공간 데이터 집계 . 섹션 @ref(벡터-속성-집약)에서 다루는 속성 데이터 집합과 마찬가지로 공간 데이터 집합은 데이터를 축소하는 방법이 될 수 있다. . 집계된 데이터는 특정 종류의 그룹화 변수와 관련된 변수(일반적으로 평균 또는 총)에 대한 일부 통계량을 보여 줍니다. 섹션 @ref(vector-attribute-aggregation)는 ‘aggregate() 및 ‘group_by() %&gt;% summarize()’ 데이터를 속성 변수를 기반으로 어떻게 축약하는지를 보여주었다. 이 섹션에서는 공간 그룹화 변수를 사용하여 동일한 기능이 작동하는 방식을 설명합니다. . 뉴질랜드의 예로 돌아가면, 각 지역의 평균 고점 높이를 알아보려고 합니다. 이것은 공간 집계의 좋은 예이다: 대상 객체의 값(‘x’ 또는 ‘nz_height’)이 그룹화되는 방식을 정의하는 소스(이 경우 ‘y’ 또는 ‘nz’)의 기하학이다. 이는 아래의 기본 ‘aggregate() 함수’를 사용하여 설명된다. . {r 04-spatial-operations-28} . 101개의 point 데이터, 뉴질랜드에서의 높은 지역 . plot(nz[1],reset=FALSE,col=”grey”) plot(nz_height[1], add=TRUE) . names(nz_height) nrow(nz_height) . nz_avheight = aggregate(x = nz_height, by = nz, FUN = mean) . tidy 함수 group_by()와 summarize()를 사용하여 동일한 결과를 생성할 수도 있다. {r} library(tmap) tm_shape(nz_avheight) + tm_fill(&quot;elevation&quot;, breaks = seq(27, 30, by = 0.5) * 1e2) + tm_borders() . {r 04-spatial-operations-29} . aggregate 함수 대신 st_join 함수를 써도 똑같이 할 수 있음 . nz_avheight2 = nz %&gt;% st_join(nz_height) %&gt;% group_by(Name) %&gt;% summarize(elevation = mean(elevation, na.rm = TRUE)) . 결과 &quot;nz_avheight&quot; 객체는 집계 객체 &quot;nz&quot;와 동일한 형상을 가지지만 뉴질랜드의 각 영역 내의 점들의 평균 높이를 나타내는 새로운 열을 가진다(&#39;median()과 &#39;sd()&#39;와 같은 다른 요약 함수를 mean() 대신 사용할 수 있다. 점을 포함하지 않는 영역에는 연관된 &#39;상승&#39; 값이 &#39;NA&#39;라는 점에 유의한다. 새 지오메트리를 생성하는 집계 연산은 섹션 @ref(기하-연합)를 참조하십시오. ### 거리 관계 위상 관계는 이항 관계인 반면 - 피쳐는 다른 피쳐와 교차하거나 교차하지 않습니다 - 거리 관계는 연속적입니다. 두 물체 사이의 거리는 &#39;st_distance()&#39; 함수로 계산된다. 이는 뉴질랜드에서 가장 높은 지점과 캔터베리 지역의 지리적 중심 사이의 거리를 찾는 아래의 코드 청크에 나타나 있다. index{discription!거리 관계} {r} # topological relation은 binary 형태로 표현됨 # 예를들어, 두 geometry 데이터가 겹친다 혹은 안겹친다, 이런식으로 표현됨 # 하지만 두 geometry 사이의 거리는 binary가 아니라 continuous한 형태임 # st_distance() 명령어를 통해 두 objects 사이의 거리 계산이 가능 # 뉴질랜드에서 가장 높은 point 추출 nz_heighest = nz_height %&gt;% top_n(n = 1, wt = elevation) # cantebury 지역의 centroid 좌표 추출 canterbury = nz %&gt;% filter(Name == &quot;Canterbury&quot;) canterbury_centroid = st_centroid(canterbury) # 두 점 사이의 거리 계산 st_distance(nz_heighest, canterbury_centroid) . 이 결과에 대해 두 가지 잠재적으로 놀라운 사실이 있습니다. . 거리는 10만 인치가 아닌 10만 미터 또는 기타 거리 측정 단위인 ‘단위’를 가지고 있습니다. . | 결과가 단일 값만 포함하더라도 행렬로 반환됩니다. . | . 이 두 번째 기능은 객체 ‘x’와 ‘y’의 모든 기능 조합 간에 거리 행렬을 반환하는 기능을 제공하는 ‘st_distance()의 또 다른 유용한 기능을 암시한다. . 이는 nz_height의 처음 세 형상과 객체 co로 대표되는 뉴질랜드의 오타고 및 캔터베리 지역 사이의 거리를 찾는 아래 명령어에 나와 있다. . {r 04-spatial-operations-32} . 여러개의 points들 사이의 거리 계산이 가능 . co = filter(nz, grepl(“Canter|Otag”, Name)) st_distance(nz_height[1:3, ], co) . nz_height의 두 번째 피처와 세 번째 피처와 co의 두 번째 피처 사이의 거리는 0이다. 이는 점과 다각형 사이의 거리가 *다각형 중 임의의 부분*까지의 거리를 의미한다는 사실을 보여준다. nz_height의 두 번째와 세 번째 포인트는 *in* Otago이며, 이 포인트는 그림으로 확인할 수 있습니다(결과는 표시되지 않음). {r 04-spatial-operations-33, eval=FALSE} # st_distance 명령어를 point와 polygon 대상으로 했을 때는, point가 polygon 안에 들어 있으면, distance를 0으로 계산함 plot(st_geometry(co)[2]) plot(st_geometry(nz_height)[2:3], add = TRUE) .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_5a_spatial_operation_kor.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_5a_spatial_operation_kor.Rmd",
          "date": ""
      }
      
  

  
      ,"page18": {
          "title": "",
          "content": "(PART) 확장 {-} . R {#adv-map}을(를) 사용하여 맵 만들기 . 필수 구성 요소 {-} . 이 장에서는 이미 사용하고 있는 다음 패키지가 필요합니다. | . {r 08-mapping-1, message=FALSE } library(sf) library(tidyverse) library(spData) library(spDataLarge) . library(raster) . - 또한 다음과 같은 시각화 패키지를 사용합니다(인터랙티브 매핑 응용 프로그램을 개발하려면 반짝이 설치). {r 08-mapping-2, message=FALSE} library(tmap) # for static and interactive maps library(leaflet) # for interactive maps library(ggplot2) # tidyverse data visualization package . 정적 지도 . {r 08-mapping-3, eval=FALSE} . ggplot2 시각화 형태와 마찬가지로 data를 input으로 입력한 뒤, . 표출 형태는 뒤에 Layer를 추가해서 자유롭게 선택 가능 . Add fill layer to nz shape . polygon 형태 . tm_shape(nz) + tm_fill() . Add border layer to nz shape . tm_shape(nz) + tm_borders() . Add fill and border layers to nz shape . tm_shape(nz) + tm_fill() + tm_borders() . ### map objects (맵 개체) {#map-obj} **tmap*의 유용한 기능은 맵을 나타내는 *개체*를 저장할 수 있다는 것입니다. 아래의 코드 청크는 그림 @ref (fig:tmshape)의 마지막 플롯을 클래스 &#39;tmap&#39;의 객체로 저장함으로써 이를 보여준다. {r 08-mapping-4} map_nz &lt;- tm_shape(nz) + tm_polygons() class(map_nz) . ‘map_nz’는 나중에 플로팅할 수 있습니다. 예를 들어, 아래 그림과 같이 계층을 추가하거나 콘솔에서 ‘map_nz’를 실행하면 됩니다. . ’+ tm_shape(new_obj)’를 사용하여 새 shape를 추가할 수 있습니다. 이 경우 ‘new_obj’는 이전 레이어 위에 플로팅할 새로운 공간 객체를 나타낸다. 이러한 방식으로 새로운 도형이 추가되면 다른 도형이 추가될 때까지 이후의 모든 미적 기능은 도형을 참조합니다. 이 구문을 사용하면 다음 코드 청크에서와 같이 여러 모양과 레이어를 가진 맵을 만들 수 있는데, 이는 함수 ‘tm_raster()’를 사용하여 래스터 레이어를 플롯하는 데 사용된다. . {r 08-mapping-5, results=’hide’} . tmap object위에 다른 map을 추가할 수 있음 . data(nz_elev) # Zew Zeleanmd elevation raster data . map_nz1 &lt;- map_nz + tm_shape(nz_elev) + tm_raster(alpha = 0.7) . map_nz1 . 아래 코드 청크에서 볼 수 있듯이 더 많은 모양과 층을 추가할 수 있으며, 이는 뉴질랜드의 [해상수]를 나타내는 &#39;nz_water&#39;를 만들고 그 결과 생긴 선을 기존 지도 객체에 추가한다. {r 08-mapping-6} # st_union : 하나의 polygon으로 합쳐줌 nz_water &lt;- st_union(nz) %&gt;% st_buffer(22200) %&gt;% st_cast(to = &quot;LINESTRING&quot;) map_nz2 &lt;- map_nz1 + tm_shape(nz_water) + tm_lines(col=&quot;blue&quot;) . tmap 객체에 추가할 수 있는 도면층 또는 도형의 수에 제한은 없습니다. 같은 모양을 여러 번 사용할 수도 있습니다. . {r 08-mapping-7} map_nz3 &lt;- map_nz2 + tm_shape(nz_height) + tm_dots() . &#39;tmap_arrange()가 추출된 단일 &#39;메타플롯&#39;에 여러 맵 개체를 정렬할 수 있다는 것입니다. {r} tmap_arrange(map_nz1, map_nz2, map_nz3) . 미학 . 채우기 레이어와 테두리 레이어에 가장 일반적으로 사용되는 미학으로는 색상, 투명도, 선폭, 선종류 등이 있으며 각각 col, alpha, lwd, lty 인수로 설정된다. . {r tmstatic, message=FALSE, fig.cap=”The impact of changing commonly used fill and border aesthetics to fixed values.”, fig.scap=”The impact of changing commonly used aesthetics.”} ma1 = tm_shape(nz) + tm_fill(col = “red”) ma2 = tm_shape(nz) + tm_fill(col = “red”, alpha = 0.3) ma3 = tm_shape(nz) + tm_borders(col = “blue”) ma4 = tm_shape(nz) + tm_borders(lwd = 3) ma5 = tm_shape(nz) + tm_borders(lty = 2) ma6 = tm_shape(nz) + tm_fill(col = “red”, alpha = 0.3) + tm_borders(col = “blue”, lwd = 3, lty = 2) tmap_arrange(ma1, ma2, ma3, ma4, ma5, ma6,ncol = 3) . 기본 R 플롯과 마찬가지로 미학을 정의하는 인수도 다양한 값을 수신할 수 있습니다. {r 08-mapping-9, eval=FALSE} # ggplot과 마찬가지로 aesthetics값에 데이터 변수 값을 넣어도 됨 # 변수의 값에 따라 도형의 크기나, 색을 달리 표현할 수 있음 plot(st_geometry(nz), col = nz$Land_area) # works # tm_shape(nz) + tm_fill(col = nz$Land_area) # fails #&gt; Error: Fill argument neither colors nor valid variable name(s) . 대신 ‘col’(및 선 레이어의 경우 ‘lwd’, 점 레이어의 경우 ‘size’와 같이 다양할 수 있는 다른 미학)은 표시할 지오메트리와 관련된 속성을 명명하는 문자열을 필요로 한다. . 따라서 다음과 같이 원하는 결과를 얻을 수 있다(그림 @ref(fig:tmcol)) . {r 08-mapping-10, fig.show=’hide’, message=FALSE} . ggplot과 다르게, 변수 이름을 Character 형태로 넣어주어야 함 . tm_shape(nz) + tm_fill(col = “Land_area”) . {r} plot(nz[&quot;Land_area&quot;]) tm_shape(nz) + tm_fill(col = &quot;Land_area&quot;) . ‘tm_fill()’과 같은 미적 층을 정의하는 함수의 중요한 논점은 ‘title’로, 관련 범례의 제목을 설정한다. 다음 코드 청크는 변수 이름 ‘Land_area’보다 더 매력적인 이름을 제공함으로써 이 기능을 보여준다. . {r 08-mapping-11} legend_title = expression(“Area (km”^2*”)”) map_nza = tm_shape(nz) + tm_fill(col = “Land_area”, title = legend_title) + tm_borders() . title 사이즈와 text 사이즈도 지정해 줄 수 있음 . map_nza = tm_shape(nz) + . tm_fill(col = “Land_area”, title = legend_title) + . tm_borders()+ . tm_layout(legend.outside = TRUE, . legend.title.size= 2, . legend.text.size = 0.5) . map_nza . ### 색상 설정 - 기본 설정은 다음 단락에서 설명하는 &#39;pretty&#39; 구절을 사용합니다. - &#39;breaks&#39;를 사용하면 수동으로 중단 시간을 설정할 수 있습니다. - n&#39;은 숫자 변수가 분류되는 빈의 수를 설정합니다. - &#39;palette&#39;는 색상표를 정의합니다(예: &#39;BuGn&#39;). {r 08-mapping-12, eval=FALSE} tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;) breaks = c(0, 3, 4, 5) * 10000 tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, breaks = breaks) tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, n = 10) tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, palette = &quot;BuGn&quot;) . 색상 설정을 변경하는 또 다른 방법은 색상 구분(또는 빈) 설정을 변경하는 것입니다. 수동으로 ‘breaks’를 설정하는 것 외에도 *tmap을 사용하면 사용자가 ‘style’ 인수로 자동으로 브레이크가 생성되도록 알고리즘을 지정할 수 있습니다. index{tmap(표준)!브레이크 스타일} 다음은 가장 유용한 브레이크 스타일 6가지입니다. . 기본 설정에서 ‘style = “pretty”는 가능한 한 정수로 나누고 균등하게 간격을 둔다. . | ‘style = “discription”은 입력 값을 동일한 범위의 빈으로 나누고 균일한 분포를 가진 변수에 적합하다(결과 맵의 색상 다양성이 거의 없을 수 있으므로 왜곡된 분포를 가진 변수에는 권장하지 않는다). . | ‘style = “quantile”은 동일한 수의 관측치가 각 범주에 속하도록 보장한다(빈 범위가 크게 다를 수 있는 잠재적 단점이 있음). . | ‘style = “sks”는 데이터에서 유사한 값의 그룹을 식별하고 범주 간의 차이를 최대화한다. . | “style = “cont”(및 “order”)는 연속적인 색 필드 위에 많은 색상을 나타내며, 특히 연속 래스터에 적합하다(“order”는 왜곡된 분포를 시각화하는 데 도움이 될 수 있다). . | style = cat은 범주별 값을 나타내며 각 범주가 고유한 색을 받도록 하기 위해 고안되었다. . | . tmap_options(title.size = 0.7, title.position = c(&quot;right&quot;, &quot;bottom&quot;), legend.position = c(&quot;LEFT&quot;, &quot;TOP&quot;)) m_equal = tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, style = &quot;equal&quot;) + tm_layout(title = &#39;style = &quot;equal&quot;&#39;) m_pretty = tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, style = &quot;pretty&quot;) + tm_layout(title = &#39;style = &quot;pretty&quot;&#39;) m_quantile = tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, style = &quot;quantile&quot;) + tm_layout(title = &#39;style = &quot;quantile&quot;&#39;) m_jenks = tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, style = &quot;jenks&quot;) + tm_layout(title = &#39;style = &quot;jenks&quot;&#39;) m_cont = tm_shape(nz_elev) + tm_raster(col = &quot;elevation&quot;, style = &quot;cont&quot;, contrast = c(0.2, 1)) + tm_layout(title = &#39;style = &quot;cont&quot;&#39;) m_cat = tm_shape(nz) + tm_polygons(col = &quot;Island&quot;, style = &quot;cat&quot;) + tm_layout(title = &#39;style = &quot;cat&quot;&#39;) tmap_arrange(m_pretty, m_equal, m_quantile, m_jenks, m_cont, m_cat, ncol=3) tmap_options_reset() . 팔레트는 위에서 설명한 breaks, n 및 style 인수에 의해 결정되고 빈과 관련된 색상 범위를 정의합니다. . 기본 색상 팔레트는 ‘tm_layout()’(자세한 내용은 섹션 @ref(레이아웃) 참조)에 지정되어 있지만 ‘palette’ 인수를 사용하여 빠르게 변경할 수 있습니다. 색상 벡터 또는 새로운 색상 팔레트 이름을 예상하며, ‘tmaptools::palette_explorer()와 대화식으로 선택할 수 있습니다. ‘-‘를 접두사로 추가하여 팔레트 순서를 반전시킬 수 있습니다. . # install.packages(&quot;tmaptools&quot;) library(tmaptools) palette_explorer() get_brewer_pal(&quot;Blues&quot;, n = 15)[1:15] get_brewer_pal(&quot;Blues&quot;,n=20) get_brewer_pal(&quot;Blues&quot;,n=20)[1:20] # 다른 방법 library(RColorBrewer) display.brewer.all() brewer.pal(&quot;Blues&quot;, n=20) # Error 9개까지 제한이 있음 brewer.pal(&quot;Blues&quot;, n=9) m_pretty = tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, style = &quot;pretty&quot;, palette = &quot;Blues&quot;, n = 15) + tm_layout(title = &#39;style = &quot;pretty&quot;&#39;, legend.outside = TRUE) m_pretty m_pretty = tm_shape(nz) + tm_polygons(col = &quot;Median_income&quot;, style = &quot;pretty&quot;, palette = &quot;viridis&quot;, n = 20) + tm_layout(title = &#39;style = &quot;pretty&quot;&#39;, legend.outside = TRUE) m_pretty . 색상 팔레트에는 범주형, 순차형 및 분기형(그림 @ref(fig:colpal))의 세 가지 주요 색상 팔레트 그룹이 있으며 각각 다른 용도로 사용됩니다. . 범주형 팔레트는 쉽게 구별할 수 있는 색상으로 구성되며 주 이름 또는 토지 표지 클래스와 같은 특별한 순서가 없는 범주형 데이터에 가장 적합합니다. 색은 직관적이어야 한다: 예를 들어 강은 파란색이어야 하고, 목초지는 녹색이어야 한다. 범례가 크고 색상이 많은 맵은 해석할 수 없습니다.^[ col = “MAP_COLORS”는 다수의 개별 폴리곤이 있는 지도(예: 개별 국가의 지도)에서 인접 폴리곤에 대한 고유한 색상을 만들 수 있습니다. ] . 두 번째 그룹은 순차 팔레트입니다. 예를 들어 밝은 색에서 어두운 색(밝은 색상은 낮은 값을 나타내는 경향이 있음)으로 기울기를 따르며 연속형(숫자) 변수에 적합합니다. . 순차 팔레트는 아래 코드 청크에서 보여지듯이 단일(예: ‘Blues’에서 연한 노란색에서 진한 파란색으로 이동) 또는 다중 색상/색상(예: ‘YlOrBr’은 연한 노란색에서 주황색을 통해 갈색으로 이동)일 수 있습니다. 출력은 표시되지 않습니다. 결과를 보려면 코드를 직접 실행하십시오! . {r 08-mapping-13, eval=FALSE} . Sequaltial palettes - 적은 색으로 그라데이션을 표현 . tm_shape(nz) + tm_polygons(“Population”, palette = “Blues”) tm_shape(nz) + tm_polygons(“Population”, palette = “YlOrBr”) . tm_shape(nz) + tm_polygons(“Population”, palette = “-viridis”, n = 7) . 마지막 그룹인 분기 팔레트는 일반적으로 세 가지 색상(그림 @ref(그림:colpal)) 사이에 있으며, 일반적으로 두 개의 단일 색상 순차 팔레트를 각 끝에 어두운 색상으로 결합함으로써 생성된다. 주요 목적은 특정 온도, 중위 가구 소득 또는 가뭄 사건의 평균 확률과 같은 중요한 기준점으로부터의 차이를 시각화하는 것이다. 기준점의 값은 &#39;midpoint&#39; 인수를 사용하여 **tmap*에서 조정할 수 있습니다. {r} # 대조를 이루고 싶을 때는 Diverging color를 사용 # 양 끝 색이 분명히 구분되는 다른 색으로 구성 library(RColorBrewer) many_palette_plotter = function(color_names, n, titles){ n_colors = length(color_names) ylim = c(0, n_colors) par(mar = c(0, 5, 0, 0)) plot(1, 1, xlim = c(0, max(n)), ylim = ylim, type = &quot;n&quot;, axes = FALSE, bty = &quot;n&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;) for(i in seq_len(n_colors)){ one_color = brewer.pal(n = n, name = color_names[i]) rect(xleft = 0:(n - 1), ybottom = i - 1, xright = 1:n, ytop = i - 0.2, col = one_color, border = &quot;light gray&quot;) } text(rep(-0.1, n_colors), (1: n_colors) - 0.6, labels = titles, xpd = TRUE, adj = 1) } many_palette_plotter(c(&quot;PRGn&quot;, &quot;YlGn&quot;, &quot;Set2&quot;), 7, titles = c(&quot;Diverging&quot;, &quot;Sequential&quot;, &quot;Categorical&quot;)) .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_6a_mapping.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_6a_mapping.Rmd",
          "date": ""
      }
      
  

  
      ,"page19": {
          "title": "mapping_2",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . ## Prerequisites {-} {r 08-mapping-1, message=FALSE } library(sf) library(tidyverse) library(spData) library(spDataLarge) # library(raster) library(tmap) # for static and interactive maps library(leaflet) # for interactive maps library(ggplot2) # tidyverse data visualization package . 1. 정적 지도 . 레이아웃 . 지도 레이아웃은 모든 지도 요소를 결합하는 맵으로 조합하는 것입니다. 지도 요소에는 매핑할 객체, 제목, 축척 막대, 여백 및 가로 세로 비율이 포함되며, 이전 섹션에서 설명한 색상 설정은 지도의 모양에 영향을 미치는 데 사용된 팔레트 및 중단점과 관련이 있습니다. 두 가지 모두 지도에서 남긴 인상에 똑같이 큰 영향을 미칠 수 있는 미묘한 변화를 초래할 수 있습니다. . map_nz &lt;- tm_shape(nz) + tm_polygons() # tm_compass 나침반 # tm_scale_bar 척도바 map_nz + tm_compass(type = &quot;8star&quot;, position = c(&quot;left&quot;, &quot;top&quot;)) + tm_scale_bar(breaks = c(0, 100, 200), text.size = 1) . tmap은 또한 매우 다양한 레이아웃 설정을 변경할 수 있으며, 그 중 일부는 다음과 같은 코드를 사용하여 생성됩니다(전체 목록은 ‘args(tm_layout)’ 또는 ‘?tm_layout’ 참조). . {r 08-mapping-14, eval=FALSE} map_nz + tm_layout(title = “New Zealand”) map_nz + tm_layout(scale = 5) map_nz + tm_layout(bg.color = “lightblue”) #배경 색 map_nz + tm_layout(frame = FALSE) # 그림 테두리 제거 . &#39;tm_layout()&#39;의 다른 인수는 맵이 위치한 캔버스와 관련하여 맵의 더 많은 측면에 대한 제어를 제공한다. - 프레임 너비(&#39;frame.lwd&#39;) 및 이중 라인을 허용하는 옵션(&#39;frame.double.line&#39;) - &#39;outter.margin&#39; 및 &#39;inner.margin&#39;을 포함한 여백 설정 - &#39;fontface&#39; 및 &#39;fontfamily&#39;에 의해 제어되는 글꼴 설정 - 범례와 같은 이진 옵션을 포함한 범례 설정.show&#39; (범례 표시 여부) &#39;marget.only&#39;(지도 표시) 및 &#39;marget&#39;입니다.outside(범례가 지도 밖으로 나와야 합니까?)는 물론, &#39;model.position&#39;과 같은 다중 선택 설정입니다. - 에스테틱 레이어의 기본 색상(&#39;aes.color&#39;), 프레임과 같은 지도 속성(&#39;attr.color&#39;) - &#39;세피아&#39;를 제어하는 색 설정입니다.강도(지도의 노란색 표시) 및 포화(색-회색 눈금) {r} legend_title = expression(&quot;Area (km&quot;^2*&quot;)&quot;) map_nza = tm_shape(nz) + tm_fill(col = &quot;Land_area&quot;, title = legend_title) + tm_borders() c1 = map_nza + tm_layout(&#39;frame.lwd = 5&#39;, frame.lwd = 5) c2 = map_nza + tm_layout(&#39;inner.margins = 0.2&#39;, inner.margins = 0.2) c3 = map_nza + tm_layout(&#39;legend.show = FALSE&#39;, legend.show = FALSE) c4 = map_nza + tm_layout(&#39;legend.position = n c(&quot;right&quot;, &quot;bottom&quot;)&#39;, legend.position = c(&quot;right&quot;, &quot;bottom&quot;)) tmap_arrange(c1, c2, c3, c4, nrow = 1) . 위에 나열된 색상 설정 변경의 영향은 그림 @ref(그림:layout3)에 나와 있습니다(전체 목록은 ‘?tm_layout’ 참조). . legend_title = expression(&quot;Area (km&quot;^2*&quot;)&quot;) map_nza = tm_shape(nz) + tm_fill(col = &quot;Land_area&quot;, title = legend_title) + tm_borders() aes.color = c(borders = &quot;red&quot;) aes.color.all = tmap_options()$aes.color aes.color.all[names(aes.color)] = aes.color c1 = map_nza + tm_layout(aes.color = aes.color.all) c2 = map_nza + tm_layout(attr.color = &quot;red&quot;) c3 = map_nza + tm_layout(sepia.intensity = 0.9) c4 = map_nza + tm_layout(saturation = 0.1) tmap_arrange(c1, c2, c3, c4, nrow = 1) . *tmap은 레이아웃과 색상에 대한 낮은 수준의 제어 외에도 ‘tm_style() 기능(패키지의 ‘style’의 두 번째 의미)을 사용하여 높은 수준의 스타일을 제공합니다. ‘tm_style’(“코발트”)과 같은 일부 스타일은 맵을 스타일화하지만, ‘tm_style(“회색”)과 같은 다른 스타일은 아래의 코드를 사용하여 생성된 그림 @ref(그림:tmstyles)와 같이 더 미묘한 변화를 일으킨다.R’): . # ggplot의 theme 같은 개념 # 자주 사용함 map_nza + tm_style(&quot;bw&quot;) map_nza + tm_style(&quot;classic&quot;) map_nza + tm_style(&quot;cobalt&quot;) map_nza + tm_style(&quot;col_blind&quot;) # tmap_style_catalogue() . {block2 styles, type=’rmdnote’} 미리 정의된 스타일의 미리보기는 ‘tmap_style_catalogue()’를 실행하여 생성할 수 있습니다. 이렇게 하면 9개의 이미지가 포함된 ‘tmap_style_previews’라는 폴더가 생성됩니다. tm_style_albatross.png에서 tm_style_white.png에 이르는 각 이미지에는 그에 상응하는 스타일로 세계의 단면도가 표시된다. 참고: ‘tmap_style_catalogue()’를 실행하는 데 시간이 걸립니다. . ### 단면도 &#39;작은 배수&#39;라고도 불리는 면상 지도는 많은 지도들이 나란히 배열되어 있으며, 때로는 수직으로 쌓이기도 한다[@meulemans_small_2017]. 측면은 시간과 같은 다른 변수에 대해 공간 관계가 어떻게 변화하는지 시각화할 수 있게 한다. 예를 들어, 정착지의 인구 변화는 특정 시점에 인구를 나타내는 각 패널이 있는 단면도로 나타낼 수 있다. 시간 차원은 색과 같은 다른 *미적*을 통해 표현될 수 있다. 그러나 이것은 여러 겹치는 점들을 포함하기 때문에 지도를 어지럽힐 위험이 있다. 일반적으로 단면 지도의 모든 개별 면은 속성 데이터의 각 열에 대해 한 번씩 여러 번 반복되는 동일한 형상 데이터를 포함한다. 그러나, 패싯은 또한 시간에 따른 점 패턴의 진화와 같은 변화하는 기하학적 구조를 나타낼 수 있다. {r} urb_1970_2030 = urban_agglomerations %&gt;% filter(year %in% c(1970, 1990, 2010, 2030)) urb_1970_2030$geometry # 인구정보를 담고 있는 point 데이터 # tm_shape(world) + tm_polygons() + tm_shape(urb_1970_2030) + tm_symbols(col = &quot;black&quot;, border.col = &quot;white&quot;, size = &quot;population_millions&quot;) + tm_facets(by = &quot;year&quot;, nrow = 2, free.coords = FALSE) . 2. 애니메이션 지도 . 섹션 @ref(faceted-maps)에 설명된 단면 지도는 변수의 공간 분포가 시간에 따라 어떻게 변화하는지 보여줄 수 있지만, 이 접근 방식은 단점이 있다. 얼굴이 많으면 작아진다. 게다가, 각각의 면들이 화면이나 페이지에서 물리적으로 분리되어 있다는 것은 면들 사이의 미묘한 차이를 감지하기 어려울 수 있다는 것을 의미한다. . 애니메이션 맵은 이러한 문제를 해결합니다. 비록 그들이 디지털 출판물에 의존하지만, 점점 더 많은 콘텐츠가 온라인으로 이동함에 따라 이것은 덜 문제가 되고 있다. 애니메이션 맵은 여전히 종이 보고서를 향상시킬 수 있습니다. 인쇄된 맵의 애니메이션(또는 대화형) 버전이 포함된 웹 페이지에 독서자를 항상 연결하여 실제 상태로 만들 수 있습니다. R에서 애니메이션을 생성하는 방법은 ggplot2를 기반으로 하는 gganimate와 같은 애니메이션 패키지를 포함하여 여러 가지가 있습니다(섹션 @ref(기타 매핑 패키지) 참조). 이 섹션에서는 *tmap을 사용하여 애니메이션 맵을 만드는 데 초점을 맞춥니다. 구문은 이전 섹션과 접근 방식의 유연성에 익숙하기 때문입니다. . 그림 @ref(그림: 도시 애니메이션)는 애니메이션 맵의 간단한 예입니다. 평면적인 플롯과 달리 여러 맵을 하나의 화면에 짜넣지 않으며 독자가 시간이 지남에 따라 세계에서 가장 인구가 많은 집합체의 공간 분포가 어떻게 진화하는지를 볼 수 있게 한다(애니메이션 버전은 책 홈페이지 참조). . m_save = world %&gt;% filter(continent != &quot;Antarctica&quot;) %&gt;% tm_shape() + tm_polygons() + tm_shape(urban_agglomerations) + tm_dots(size = &quot;population_millions&quot;, title.size = &quot;Population (m)&quot;, alpha = 0.5, col = &quot;red&quot;) + tm_facets(along = &quot;year&quot;, free.coords = FALSE) tmap_animation(tm = m_save, filename = &quot;data/urban-animated.gif&quot;, width = 1200, height = 800) magick::image_read(&quot;data/urban-animated.gif&quot;) . 그림 @ref(그림: 도시 애니메이션)에 표시된 애니메이션 맵은 섹션 @ref(페이스 맵)에 나와 있는 것과 동일한 *tmap 기법을 사용하여 만들 수 있다. 그러나 tm_facets()의 인수에는 두 가지 차이점이 있다. . “long = “year” 대신 “long = “year”를 사용합니다. | 각 맵 반복에 대한 맵 범위를 유지하는 ‘free.files = FALSE’입니다. | . 이러한 추가 인수는 후속 코드 청크에서 입증된다: . urb_anim = tm_shape(world) + tm_polygons() + tm_shape(urban_agglomerations) + tm_dots(size = &quot;population_millions&quot;) + tm_facets(along = &quot;year&quot;, free.coords = FALSE) . 결과 urb_anim은 각 연도에 대한 별도의 지도 집합을 나타낸다. 마지막 단계는 이 둘을 결합하여 결과를 tmap_animation()이 추출된 .gif 파일로 저장하는 것입니다. 다음 명령은 연습 중에 추가할 몇 가지 요소가 누락된 상태에서 그림 @ref(그림:도시 애니메이션)에 표시된 애니메이션을 생성합니다. . tmap_animation(urb_anim, filename = &quot;data/urb_anim.gif&quot;, delay = 25) . interactive map (대화형 지도) . {r 08-mapping-25, eval=FALSE} tmap_mode(“view”) map_nz . 이제 대화형 모드가 &#39;켜짐&#39;되었으므로 **tmap*으로 생성된 모든 맵이 실행됩니다(&#39;tmap_leaflet&#39; 기능을 사용하여 대화형 맵을 만드는 또 다른 방법). 이 대화형 모드의 주목할 만한 특징으로는 아래와 같이 &#39;tm_basemap()&#39;(또는 &#39;tmap_options()&#39;으로 기본 맵을 지정할 수 있는 기능이 있다. {r 08-mapping-26, eval=FALSE} map_nz + tm_basemap(server = &quot;OpenTopoMap&quot;) . *tmap의 보기 모드의 인상적이고 거의 알려지지 않은 특징은 단면도에서도 작동한다는 것이다. . 이 경우 ‘tm_facets()’의 ‘sync’ 인수를 사용하여 그림 @ref(fig:sync)와 같이 확대/축소 및 이동 설정이 동기화된 여러 맵을 생성할 수 있으며, 이 맵은 다음 코드로 생성됩니다. . {r 08-mapping-27, eval=FALSE} world_coffee = world %&gt;% left_join(coffee_data, by = “name_long”) #name_long이라는 공통된 col로 left_join . tm_shape(world_coffee) + tm_polygons(c(“coffee_production_2016”, “coffee_production_2017”)) + tm_facets(nrow = 1, sync = TRUE) . #sync = FALSE : 왼쪽그래프와 오른쪽 그래프가 독립적으로 움직임 tm_shape(world_coffee) + tm_polygons(c(“coffee_production_2016”, “coffee_production_2017”)) + tm_facets(nrow = 1, sync = FALSE) . **tmap*을 동일한 기능으로 다시 플로팅 모드로 전환: {r 08-mapping-28} tmap_mode(&quot;plot&quot;) . tmap에 익숙하지 않은 경우 대화형 맵을 만드는 가장 빠른 방법은 *mapview index{mapview(package)}일 수 있습니다. . 다음의 ‘원 라이너’는 광범위한 지리적 데이터 형식을 대화식으로 탐색할 수 있는 신뢰할 수 있는 방법입니다. . {r 08-mapping-29, eval=FALSE} library(mapview) . mapview::mapview(nz) . **mapview*는 구문이 간결하지만 강력합니다. 기본적으로 마우스 위치 정보, 속성 쿼리(팝업을 통한), 축척 막대, 줌 투 레이어 버튼과 같은 일부 표준 GIS 기능을 제공한다. 데이터 세트를 여러 레이어로 &#39;버스트&#39;할 수 있는 기능과 &#39;+&#39;로 여러 레이어를 추가하고 지리적 객체 이름을 추가하는 등 고급 제어 기능을 제공한다. 또한 인수 &#39;zcol&#39;을 통해 속성의 자동 색칠을 제공한다. 본질적으로 데이터 중심 **리플릿* API index{로 간주할 수 있다.API}(**leaflet**에 대한 자세한 내용은 아래 참조). **mapview*는 항상 공간 객체(&#39;sf&#39;, &#39;Spacial*the &#39;Raster*&#39;)를 첫 번째 주장으로 기대하기 때문에, 파이프로 연결된 표현식의 끝에서 잘 작동한다. 선과 다각형 교차에 **sf**를 사용한 다음 **mapview*(그림 @ref(그림:mapview2)로 시각화되는 다음 예를 생각해 보십시오. {r} library(mapview) # # plot(franconia[1],reset=FALSE) # plot(franconia[franconia$district == &quot;Oberfranken&quot;, ][1],col=&quot;red&quot;,add=TRUE) # plot(trails[1],col=&quot;black&quot;) # plot(breweries[1]) # # # # trails %&gt;% # st_transform(st_crs(franconia)) %&gt;% # st_intersection(franconia[franconia$district == &quot;Oberfranken&quot;, ]) %&gt;% # st_collection_extract(&quot;LINE&quot;) %&gt;% # mapview(color = &quot;red&quot;, lwd = 3, layer.name = &quot;trails&quot;) + # mapview(franconia, zcol = &quot;district&quot;, burst = TRUE) + # breweries . 한 가지 중요한 점은 mapview* 레이어가 ‘+’ 연산자(ggplot2** 또는 tmap와 유사)를 통해 추가된다는 것입니다. . 이 값은 주 바인딩 연산자가 ‘%&gt;% 인 파이프된 워크플로우에서 자주 사용되는 gotcha)입니다. . R을 사용하여 대화형 맵을 만드는 다른 방법이 있습니다. 예를 들어 googleway 패키지 index{googleway(패키지)}는 유연하고 확장 가능한 대화형 매핑 인터페이스를 제공합니다. (자세한 내용은 [구글웨이-비넷(https://cran.r-project.org/web/packages/googleway/vignettes/googleway-vignette.html))] 참조). 같은 저자의 또 다른 접근법은 mapdeck이다.AU/mapdeck). Uber의 ‘Deck.gl’ 프레임워크 index{mapdeck(mapdeck)}에 액세스할 수 있습니다. WebGL을 사용하여 대규모 데이터 세트(최대 수백만 점)를 대화식으로 시각화할 수 있다. 패키지는 패키지를 사용하기 전에 등록해야 하는 Mapbox access token,)을 사용합니다. . {block2 08-mapping-31, type=’rmdnote’} 다음 블록은 액세스 토큰이 R 환경에 ‘MAPBOX=your_unique_key’로 저장되어 있다고 가정합니다. 이 패키지는 *uses this 패키지의 ‘edit_r_environment()’와 함께 추가할 수 있습니다. . **mapdeck*의 고유한 특징은 그림 @ref(그림:mapdeck)에 나와 있는 대화형 &#39;2.5d&#39; 관점을 제공하는 것이다. 즉, 지도를 이동, 확대/축소 및 회전하고 지도에서 &#39;추출된&#39; 데이터를 볼 수 있습니다. 다음 코드 청크로 생성된 그림 @ref(fig:mapdeck)는 영국의 도로 교통 충돌을 시각화하며, 막대 높이는 영역당 사상자를 나타낸다. https://www.mapbox.com/ 들어가서 회원가입 후 토큰 발급 pk.eyJ1Ijoic3BlYXI1MzA2IiwiYSI6ImNremN5Z2FrOTI0ZGgycm45Mzh3dDV6OWQifQ.kXGWHPRjnVAEHgVgLzXn2g mapbox://styles/spear5306/ckzcyy8fa002014qgwdpqxpyq {r 08-mapping-33, eval=FALSE} library(mapdeck) set_token(Sys.getenv(&quot;pk.eyJ1IjoieWFuZ3JoYTQiLCJhIjoiY2wxb2g5eXhtMTRlYjNjanhkMjJpZ3ZoNSJ9.UEjUYjkYfTztuq0sCcIETg&quot;)) crash_data = read.csv(&quot;https://git.io/geocompr-mapdeck&quot;) crash_data = na.omit(crash_data) ms = mapdeck_style(&quot;dark&quot;) mapdeck(style = ms, pitch = 45, location = c(0, 52), zoom = 4, token = &quot;pk.eyJ1Ijoic3BlYXI1MzA2IiwiYSI6ImNremN5Z2FrOTI0ZGgycm45Mzh3dDV6OWQifQ.kXGWHPRjnVAEHgVgLzXn2g&quot;) %&gt;% add_grid(data = crash_data, lat = &quot;lat&quot;, lon = &quot;lng&quot;, cell_size = 1000, elevation_scale = 50, layer_id = &quot;grid_layer&quot;, colour_range = viridisLite::plasma(6)) mapdeck(pitch = 30, location = c(0, 52), zoom = 1, token = &quot;pk.eyJ1IjoieWFuZ3JoYTQiLCJhIjoiY2wxb2g5eXhtMTRlYjNjanhkMjJpZ3ZoNSJ9.UEjUYjkYfTztuq0sCcIETg&quot;, style = &quot;mapbox://styles/yangrha4/cl1yi0w0w001d15mumm62fhju&quot;) %&gt;% add_scatterplot(data = crash_data, lat = &quot;lat&quot;, lon = &quot;lng&quot;, layer_id = &quot;scatter_layer&quot;, radius = 10) . In the browser you can zoom and drag, in addition to rotating and tilting the map when pressing Cmd/Ctrl. Multiple layers can be added with the %&gt;% operator, as demonstrated in the mapdeck vignette. . Mapdeck also supports sf objects, as can be seen by replacing the add_grid() function call in the preceding code chunk with add_polygon(data = lnd, layer_id = &quot;polygon_layer&quot;), to add polygons representing London to an interactive tilted map. . {r 08-mapping-35, eval=FALSE, echo=FALSE} library(mapdeck) set_token(“pk.eyJ1IjoieWFuZ3JoYTQiLCJhIjoiY2wxb2g5eXhtMTRlYjNjanhkMjJpZ3ZoNSJ9.UEjUYjkYfTztuq0sCcIETg”) . mapdeck_tokens() . df = read.csv(“https://git.io/geocompr-mapdeck”) ms = mapdeck_style(‘dark’) mapdeck(style = ms, pitch = 45, location = c(0, 52), zoom = 4) %&gt;% . add_grid(data = df, lat = “lat”, lon = “lng”, cell_size = 1000, . elevation_scale = 50, layer_id = “grid_layer”, . colour_range = viridisLite::plasma(5)) %&gt;% . add_polygon(data = lnd, layer_id = “polygon_layer”, fill_opacity = 0.3) . {r 08-mapping-36, eval=FALSE, echo=FALSE} str(roads) mapdeck( , style = mapdeck_style(&#39;dark&#39;) , location = c(145, -37.8) , zoom = 10 ) %&gt;% add_path( data = roads , stroke_colour = &quot;RIGHT_LOC&quot; , layer_id = &quot;path_layer&quot; , tooltip = &quot;ROAD_NAME&quot; , auto_highlight = TRUE ) . 마지막으로 leaflet*은 R index{leaflet(package)}에서 가장 성숙하고 널리 사용되는 대화형 매핑 패키지이다. **vp는 리플릿 자바스크립트 라이브러리에 대한 비교적 낮은 수준의 인터페이스를 제공하며, 대부분의 주장은 원본 자바스크립트 라이브러리의 문서를 읽으면 이해할 수 있다. . 리플릿 맵은 ‘리플릿()’으로 생성되며, 그 결과는 다른 *리플릿 함수에 파이프될 수 있는 ‘리플릿’ 맵 개체이다. 이를 통해 그림 @ref(fig:leaflet)를 생성하는 아래 코드에서 보여지듯이 여러 지도 계층과 제어 설정을 대화식으로 추가할 수 있습니다([rstudio] 참조).github.io/leaflet/](http://rstudio.자세한 내용은 github.io/leaflet/)). . pal = colorNumeric(&quot;RdYlBu&quot;, domain = cycle_hire$nbikes) leaflet(data = cycle_hire) %&gt;% addProviderTiles(providers$CartoDB.Positron) %&gt;% addCircles(col = ~pal(nbikes), opacity = 0.9) %&gt;% addPolygons(data = lnd, fill = FALSE) %&gt;% addLegend(pal = pal, values = ~nbikes) %&gt;% setView(lng = -0.1, 51.5, zoom = 12) %&gt;% addMiniMap() . # install.packages(&quot;mapboxapi&quot;) library(mapboxapi) mb_access_token(&quot;pk.eyJ1IjoieWFuZ3JoYTQiLCJhIjoiY2wxeW15a3FhMGR6NTNqbWtxOHl3NGd4aSJ9.DMRmENm8BWnl1VrzpZkMqw&quot;,install = TRUE,overwrite=TRUE) my_token &lt;- Sys.getenv(&quot;MAPBOX_PUBLIC_TOKEN&quot;) leaflet(data = cycle_hire) %&gt;% addCircles(col = ~pal(nbikes), opacity = 0.9) %&gt;% addPolygons(data = lnd, fill = FALSE) %&gt;% addLegend(pal = pal, values = ~nbikes) %&gt;% setView(lng = - 0.1, 51.5, zoom = 12) %&gt;% addMiniMap() %&gt;% addMapboxTiles(style_id = &quot;ckzcz5m8w002814o2coz02sjc&quot;, username = &quot;spear5306&quot;,access_token = my_token) .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_6b_mapping.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_6b_mapping.Rmd",
          "date": ""
      }
      
  

  
      ,"page20": {
          "title": "week_7a_exercise.Rmd",
          "content": "{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) . # 공간 연산 및 시각화 연습해보기 {r} library(sf) library(tidyverse) library(spData) # install.packages(&quot;mapdeck&quot;) library(mapdeck) my_token &lt;- Sys.getenv(&quot;MAPBOX_PUBLIC_TOKEN&quot;) . Import data . crash_data : 영국 전체 사고 데이터 cycle_hire : 런던 자전거 대여소 위치 lnd:The boroughs of London - 런던과 인근지역 행정경계 . crash_data &lt;- read.csv(&quot;https://git.io/geocompr-mapdeck&quot;) # 영국전체사고 crash_data crash_data &lt;- st_as_sf(crash_data %&gt;% drop_na(), coords = c(&quot;lng&quot;,&quot;lat&quot;), crs = st_crs(4326)) . Question 런던 지역에서 사고가 발생한 위치로부터 반경 30m 이내에 있는 모든 자전거 대여소의 위치를 표시하고, 각 자전거 대여소마다 반경 30m 이내에 발생한 사고가 몇 건인지를 매핑하시오. . 런던 지역 데이터만 뽑기 . london_polygon &lt;- lnd %&gt;% filter(ONS_INNER==&quot;T&quot;) crash_london &lt;- crash_data[london_polygon,] crash_london$crash_id &lt;- 1:nrow(crash_london) cycle_hire &lt;- cycle_hire . 1) Crash 주변 버퍼 만들기 . crash_buffer &lt;- crash_london %&gt;% st_transform(27700) %&gt;% st_buffer(dist = 30) %&gt;% st_transform(4326) #mapedeck에서 쓰기위해 다시 위경도 좌표계로 바꿔줌 . 2) Crash 30m 버퍼를 기준으로 자전거대여소와 Join 하기 . crash_buffer_with_cycle &lt;- crash_buffer %&gt;% st_join(cycle_hire, left = FALSE) . 3) 자전거 대여소별로 사고발생건수 Count 하기 . crash_count_by_cycle &lt;- crash_buffer_with_cycle %&gt;% st_drop_geometry() %&gt;% #공간정보제거 #메모리절약 group_by(id) %&gt;% # station_id별로 group_by summarise(crash_n=n()) . 4) 사고 Count 수를 원래 데이터에 붙이기 . cycle_hire_join &lt;- cycle_hire %&gt;% left_join(crash_count_by_cycle, by=&quot;id&quot;) %&gt;% mutate(crash_n = ifelse(is.na(crash_n),0,crash_n)) # 사고가 주변에 안나서 Na 값 발생하므로 0으로 바꿔줌 cycle_hire_join[is.na(cycle_hire_join$crash_n),]$crash_n &lt;- 0 tmp &lt;- cycle_hire %&gt;% left_join(crash_count_by_cycle, by = &quot;id&quot;) . 5) 최종시각화 . ms = mapdeck_style(&quot;dark&quot;) mapdeck(style = ms, pitch = 45, location = c(0,52),zoom = 4) %&gt;% add_polygon(data = lnd, layer_id = &quot;polygon_layer&quot;, fill_opacity = 1) %&gt;% add_scatterplot(data = crash_london, radius = 30, fill_colour = &quot;#DF171C&quot;, layer_id = &quot;Crash&quot;) %&gt;% add_scatterplot(data = cycle_hire, layer_id = &quot;Bike_Station&quot;, radius = 50, fill_colour = &quot;#00FF00FF&quot;,) . Import data . library(tmaptools) library(shinyjs) palette_explorer() . 런던 지역 데이터만 뽑기 . london_polygon &lt;- lnd %&gt;% filter(ONS_INNER==&quot;T&quot;) crash_london &lt;- crash_data[london_polygon,] crash_london$crash_id &lt;- 1:nrow(crash_london) cycle_hire &lt;- cycle_hire . 시각화 . ms = mapdeck_style(&quot;dark&quot;) mapdeck(style = ms, pitch = 45, location = c(0, 52), zoom = 4) %&gt;% add_polygon(data = lnd, layer_id = &quot;polygon_layer&quot;, fill_opacity = 1) %&gt;% add_scatterplot(data=crash_london, radius = 30, fill_colour = &quot;#DF171C&quot;, layer_id=&quot;Crash&quot;) %&gt;% add_scatterplot(data=cycle_hire, layer_id=&quot;Bike_Station&quot;, radius = 50, fill_colour = &quot;#00FF00FF&quot;) .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_7a_exercise.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_7a_exercise.Rmd",
          "date": ""
      }
      
  

  
      ,"page21": {
          "title": "Dashboard_practice",
          "content": "{r setup, include=FALSE} . rm(list=ls()) . library(flexdashboard) library(sf) library(tmap) tmap_mode(“view”) library(tidyverse) library(mapdeck) set_token(“pk.eyJ1Ijoic3BlYXI1MzA2IiwiYSI6ImNremN5Z2FrOTI0ZGgycm45Mzh3dDV6OWQifQ.kXGWHPRjnVAEHgVgLzXn2g”) Sys.setlocale(“LC_ALL”,”Korean”) # 언어 한글로 . ### Task 설명 #### 분석 데이터 1. 인천공항에서 출발/도착하는 1일치(2019년 6월 29일)의 이동데이터 2. 수도권의 행정구역경계 정보 {r} dat &lt;- read_rds(&quot;./data/test.rds&quot;) region &lt;- read_rds(&quot;./data/region.rds&quot;) # region2 &lt;- rmapshaper::ms_simplify(region, keep = 0.1, #keep값을 높여줄수록 simple 해짐 # keep_shapes = TRUE) . 시각화 목록 . 행정구역별 승용차 분담율 및 통행량 1.1 읍면동별 승용차 분담율(%) 1.2 읍면동별 승용차 통행량(명명) 1.3 시군구별 승용차 분담율(%) 1.4 시군구별 승용차 통행량(명) . | 시간대별 교통수단 분담율 및 통행량 변화 2.1 시간별(0~24시) 통행량 시각화 2.2 시간별(0~24시) 교통수단 분담율 시각화 . | 최종적으로 위의 모든 그림들을 Flexdashboard로 나타내보기! . 데이터 전처리 힌트 . 읍면동별 데이터 가공 - 승용차 분담율과 승용차 통행량 변수를 뽑아야함 (원하는 데이터 형태를 연습장에 적어보세요) | car_emd &lt;- dat %&gt;% group_by(admi_cd) %&gt;% summarise(total_cnt = n(), car_cnt = sum(trns_prt == &quot;CAR&quot;)) %&gt;% mutate(car_ratio = car_cnt / total_cnt) . 시군구별 데이터 가공 (위의 작업을 시군구별로 진행) | # sf_use_s2(TRUE) car_sgg &lt;- dat %&gt;% group_by(sgg_cd) %&gt;% summarise( car_trip = sum(trns_prt == &quot;CAR&quot;), subway_trip = sum(trns_prt == &quot;SUBWAY&quot;), bus_trip = sum(trns_prt == &quot;BUS&quot;), total_trip = n() ) %&gt;% mutate(car_trip_ratio = car_trip / total_trip) region_sgg &lt;- region %&gt;% group_by(sgg_cd) %&gt;% summarise(sidonm = sidonm[1], sggnm = sggnm[1]) sf_use_s2(FALSE) . 가공된 데이터에 공간정보 붙이기 | vis_emd &lt;- region %&gt;% left_join(car_emd, by=c(&quot;adm_cd&quot;=&quot;admi_cd&quot;)) vis_sgg &lt;- region_sgg %&gt;% left_join(car_sgg, by=&quot;sgg_cd&quot;) . tmap_mode(&quot;view&quot;) tm_shape(vis_emd)+ tm_fill(col=&quot;car_ratio&quot;)+ tm_borders() . 시간대별 &amp; 교통수단별 통행 데이터 가공 | trip_by_time_and_mode &lt;- dat %&gt;% group_by(st_timezn_cd,trns_prt) %&gt;% summarise(n=n()) # RAIL_B, RAIL_N 변수를 모두 RAIL 변수로 통합 trip_by_time_and_mode &lt;- trip_by_time_and_mode %&gt;% mutate(trns_prt = ifelse(trns_prt %in% c(&quot;RAIL_B&quot;,&quot;RAIL_N&quot;),&quot;RAIL&quot;,trns_prt)) . ggplot(data = trip_by_time_and_mode)+ geom_bar(aes(x=st_timezn_cd, y=n, fill=trns_prt), stat = &quot;identity&quot;, position = &quot;stack&quot;)+ labs(x = &quot;Time&quot;, y = &quot;Travel Volume&quot;, fill = &quot;Transportation Mode&quot;) # ggplotly(vis1) save(list=c(&quot;vis_emd&quot;,&quot;vis_sgg&quot;,&quot;trip_by_time_and_mode&quot;), file = &quot;data/dat_dashboard.RData&quot;) . # 비율로 바꿔줌 ggplot(data = trip_by_time_and_mode)+ geom_bar(aes(x=st_timezn_cd, y=n, fill=trns_prt), stat = &quot;identity&quot;, position = &quot;fill&quot;)+ labs(x = &quot;Time&quot;, y = &quot;Travel Volume&quot;, fill = &quot;Transportation Mode&quot;) # 여러개의 객체를 한번에 저장하고 싶을때 save(list=c(&quot;vis_emd&quot;,&quot;vis_sgg&quot;,&quot;trip_by_time_and_mode&quot;), file = &quot;data/dat_dashboard.RData&quot;) . (위에서 가공한 데이터로 X축에 시간이 y축에 교통수단별 통행량이 찍히면 됨) (아래 링크를 참고) barplot - https://r-graph-gallery.com/48-grouped-barplot-with-ggplot2.html lineplot -https://medium.com/@peteryun/r-ggplot2-multi-line-graph-example-code-9b42c22a609c . 위에서 데이터 가공 및 시각화 구현을 마친 후, flexdashboard로 배치해봅시다 . Column {data-width=650} . library(flexdashboard) library(plotly) library(tmap) library(scales) tmap_mode(&quot;view&quot;) load(&quot;data/dat_dashboard.RData&quot;) . Chart A . . Column {data-width=350} . Chart B . . Chart C . .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_8_dashboard_practice.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_8_dashboard_practice.Rmd",
          "date": ""
      }
      
  

  
      ,"page22": {
          "title": "My first dashboard",
          "content": "{r setup, include=FALSE} . install.packages(“flexdashboard”) . library(flexdashboard) library(sf) library(leaflet) library(RColorBrewer) library(tidyverse) . {r, include=FALSE} # 데이터 읽기 hump &lt;- st_read(&quot;./data/hump_pohang_final.shp&quot;)%&gt;% st_set_precision(1e08) road &lt;- st_read(&quot;./data/road_pohang_split_final.shp&quot;)%&gt;% st_set_precision(1e08) # 위경도 좌표 변환 road_wgs &lt;- road %&gt;% st_set_precision(1e08) %&gt;% st_transform(4326) hump_wgs &lt;- hump %&gt;% st_transform(4326) # 과속방지턱 인근 30m 도로 추출 # hump 주변 30m buffer hump_buff_30m = hump %&gt;% st_buffer(dist = 30) %&gt;% st_set_precision(1e08) %&gt;% st_transform(4326) # st_intersection을 이용하여 buffer된 hump 기준으로 겹치는 road 추출 road_in_hump = st_intersection(road_wgs[1:100,], hump_buff_30m)%&gt;% st_transform(4326) . Column {data-width=350} . 과속방지턱 범위 내의 도로 추출 . leaflet() %&gt;% addTiles(group = &quot;OSM (default)&quot;) %&gt;% addProviderTiles(&#39;CartoDB.Positron&#39;, group = &quot;CartoDB&quot;) %&gt;% addPolylines(data=road_wgs,col=~&quot;grey&quot;, group = &quot;road&quot;, opacity = 1, weight = 1)%&gt;% addPolylines(data=road_in_hump, col=~&quot;orange&quot;, group = &quot;road_hump&quot;, opacity = 1, weight = 3, fillOpacity = 1)%&gt;% addCircleMarkers(data=hump_wgs, radius = 3, color = &quot;red&quot;, stroke = F, fillOpacity = 0.5, group = &quot;hump&quot;) %&gt;% # Layers control addLayersControl( baseGroups = c(&quot;CartoDB&quot;,&quot;OSM (default)&quot;), overlayGroups = c(&quot;road&quot;, &quot;road_hump&quot;, &quot;hump&quot;), options = layersControlOptions(collapsed = FALSE) ) . Column {data-width=250} . 도로 길이 분포 . ggplot(data=road %&gt;% st_drop_geometry()) + geom_histogram(aes(x=length_1)) .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/2022_01/open_data_analysis/week_8_flexdashboard.Rmd",
          "relUrl": "/2022_01/open_data_analysis/week_8_flexdashboard.Rmd",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page26": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yangjunghyun.github.io/yangjunghyun_/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}