{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J2vPGaNNkRz",
        "outputId": "a03ac6dc-023e-4e7c-e964-4dc99d0506ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "my_zip = zipfile.ZipFile('/content/drive/MyDrive/양정현/net/Dataset.zip','r')\n",
        "my_zip.extractall('./AI_Programming')"
      ],
      "metadata": {
        "id": "llgUH0KJNqmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import torch\n",
        "\n",
        "def image_load(img_path, height=128, width=128, bshort=False):\n",
        "    # ---- image total number\n",
        "    img_names = sorted(os.listdir(img_path))\n",
        "\n",
        "    if not bshort:\n",
        "        img_num = len(img_names)\n",
        "    else:\n",
        "        img_num = 10000\n",
        "\n",
        "    print(\"img num : %d \" % img_num)\n",
        "    images = np.zeros((img_num, height, width, 3), dtype=np.uint8)\n",
        "\n",
        "    for it in range(img_num):\n",
        "        images[it, :, :, :] = cv2.imread(img_path + '%s' %(img_names[it]))\n",
        "\n",
        "    return images\n",
        "\n",
        "def gt_load(gt_file):\n",
        "    f = open(gt_file)\n",
        "    lines = f.readlines()\n",
        "    num_gt = len(lines)\n",
        "    print('gt num : %d ' %num_gt)\n",
        "\n",
        "    gts = np.zeros(num_gt, dtype=np.int32)\n",
        "\n",
        "    for it in range(num_gt):\n",
        "        gts[it] = int(lines[it][:-1]) - 1  # saved gt values range : 1 ~ 200 -> 0 ~ 199\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    return gts\n",
        "\n",
        "def mini_batch_image(train_img, train_gts, batch_size, crop_size=96):\n",
        "    batch_img = np.zeros((batch_size, crop_size, crop_size, 3), dtype=np.float32)\n",
        "    batch_gts = np.zeros(batch_size, dtype=np.long)\n",
        "\n",
        "    rand_num = np.random.randint(0, train_img.shape[0], batch_size)\n",
        "\n",
        "    for it in range(batch_size):\n",
        "        img = train_img[rand_num[it], :, :, :]\n",
        "\n",
        "        # --- dataAug 1. Flip\n",
        "        rand_flip = np.random.normal(0.0, scale=1.0)\n",
        "        if rand_flip < 0:\n",
        "            img = cv2.flip(img, 1)\n",
        "\n",
        "        # --- dataAug 2. Crop\n",
        "        crop_y = np.random.randint(0, img.shape[0] - crop_size - 1)\n",
        "        crop_x = np.random.randint(0, img.shape[1] - crop_size - 1)\n",
        "        img = img[crop_y: crop_y + crop_size, crop_x:crop_x + crop_size, :]\n",
        "\n",
        "        batch_img[it, :, :, :] = (img / 255.0 * 2.0) - 1.0\n",
        "        batch_gts[it] = train_gts[rand_num[it]]\n",
        "\n",
        "    return batch_img, batch_gts"
      ],
      "metadata": {
        "id": "zP2dq6XCKSyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class residual_block(nn.Module):\n",
        "    def __init__(self, c_in,c_mid, c_out, bdown=False):\n",
        "        super(residual_block, self).__init__()\n",
        "\n",
        "        self.c_in = c_in\n",
        "        self.c_mid = c_mid\n",
        "        self.c_out = c_out\n",
        "\n",
        "        self.bdown = bdown\n",
        "        if self.bdown:\n",
        "            stride = 2\n",
        "        else:\n",
        "            stride = 1\n",
        "\n",
        "        self.convs = nn.Sequential(nn.BatchNorm2d(c_in),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(c_in, c_mid, kernel_size=(1, 1), padding=0, stride=(1, 1)),\n",
        "                      nn.BatchNorm2d(c_mid),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(c_mid, c_mid, kernel_size=(3, 3), padding=1,stride = (1,1)),\n",
        "                      nn.BatchNorm2d(c_mid),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(c_mid, c_out, kernel_size=(1, 1), padding=0,stride = (1,1)),\n",
        "        )\n",
        "\n",
        "        if self.c_in != c_out:\n",
        "            self.conv_db = nn.Conv2d(c_in, c_out, kernel_size=(1, 1), stride=(1, 1),padding = 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x\n",
        "        x = self.convs(x)\n",
        "\n",
        "        if self.c_in != self.c_out:\n",
        "            y = self.conv_db(y)\n",
        "\n",
        "        return (x + y) / np.sqrt(2.0)\n",
        "\n",
        "class ResNet_50(nn.Module):\n",
        "    def __init__(self, c_in=3, conv_ch=64, output_size=200):\n",
        "        super(ResNet_50, self).__init__()\n",
        "        self.ch = conv_ch\n",
        "\n",
        "        self.conv = nn.Conv2d(c_in, self.ch, kernel_size=(7, 7), padding=3, stride=(2, 2))\n",
        "        self.maxp = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2))\n",
        "        self.resblocks = nn.Sequential(residual_block(self.ch,self.ch, self.ch*4),\n",
        "                                       residual_block(self.ch*4,self.ch, self.ch*4),\n",
        "                                       residual_block(self.ch*4,self.ch, self.ch*4),\n",
        "\n",
        "                                       residual_block(self.ch*4, self.ch*2, self.ch*8, bdown=True),\n",
        "                                       residual_block(self.ch*8, self.ch*2,self.ch*8),\n",
        "                                       residual_block(self.ch*8, self.ch*2,self.ch*8),\n",
        "                                       residual_block(self.ch*8, self.ch*2,self.ch*8),\n",
        "\n",
        "\n",
        "                                       residual_block(self.ch*8, self.ch*4,self.ch*16, bdown=True),\n",
        "                                       residual_block(self.ch*16, self.ch*4,self.ch*16),\n",
        "                                       residual_block(self.ch*16, self.ch*4,self.ch*16),\n",
        "                                       residual_block(self.ch*16, self.ch*4,self.ch*16),\n",
        "                                       residual_block(self.ch*16, self.ch*4,self.ch*16),\n",
        "                                       residual_block(self.ch*16, self.ch*4,self.ch*16),\n",
        "\n",
        "                                       residual_block(self.ch*16, self.ch*8,self.ch*32, bdown=True),\n",
        "                                       residual_block(self.ch*32, self.ch*8, self.ch*32),\n",
        "                                       residual_block(self.ch*32, self.ch*8, self.ch*32),\n",
        "\n",
        "                                       nn.BatchNorm2d(self.ch*32),\n",
        "                                       nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.dr = nn.Dropout(p=0.2)\n",
        "        self.fc = nn.Linear(32* self.ch, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.maxp(x)\n",
        "        x = self.resblocks(x)\n",
        "        x = torch.mean(x, dim=(2, 3))\n",
        "        x = self.dr(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ME9QBUs6W4LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "\n",
        "# 1. Paramters setting\n",
        "num_class = 200\n",
        "batch_size = 128  # or 32\n",
        "initial_lr = 0.01  # 0.01 ---> 0\n",
        "max_iter = 130000  # or 15~20만\n",
        "\n",
        "save_name = 'Resnet50_SGD_b128'\n",
        "\n",
        "model_save_path = './model/%s/' %save_name\n",
        "model_saving_iter = 5000\n",
        "brestore = False\n",
        "restore_iter = 6000\n",
        "if not brestore:\n",
        "    restore_iter = 0\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(DEVICE)\n",
        "\n",
        "# ----- 2. data load\n",
        "print('DATA LOAD')\n",
        "train_images = image_load(\"/content/AI_Programming/train/\")\n",
        "train_gts = gt_load(\"/content/AI_Programming/train_gt.txt\")\n",
        "\n",
        "test_images = image_load(\"/content/AI_Programming/test/\", bshort=True)\n",
        "test_gts = gt_load(\"/content/AI_Programming/test_gt_short.txt\")\n",
        "\n",
        "print('DATA LOAD FINISH')\n",
        "\n",
        "# ---- 3. network build (restore model if necessary)\n",
        "model = ResNet_50().to(DEVICE)\n",
        "\n",
        "if brestore:\n",
        "    model.load_state_dict(torch.load(model_save_path + 'model_%d.pt' % restore_iter))\n",
        "\n",
        "# ---- 4. loss function and optimizer\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr, eps=1.0)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, weight_decay=1e-4, momentum=0.9)\n",
        "\n",
        "start_time = time.time()\n",
        "for it in range(restore_iter, max_iter+1):\n",
        "    optimizer.param_groups[0]['lr'] = initial_lr - (it / max_iter) * initial_lr\n",
        "\n",
        "    batch_img, batch_gts = mini_batch_image(train_images, train_gts, batch_size)\n",
        "    batch_img = np.transpose(batch_img, (0, 3, 1, 2))\n",
        "\n",
        "    # ----- training step\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(torch.from_numpy(batch_img).to(DEVICE))\n",
        "    gt_tensor = torch.tensor(batch_gts, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    train_loss = loss(pred, gt_tensor)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if it % 100 == 0:\n",
        "        consume_time = time.time() - start_time\n",
        "        print(\"iter : %d  lr : %.5f   loss : %.5f  time : %.4f \" % (it, optimizer.param_groups[0]['lr'], train_loss.item(), consume_time))\n",
        "        start_time = time.time()\n",
        "\n",
        "    if it % model_saving_iter == 0 and it != 0:\n",
        "        print('SAVING MODEL')\n",
        "        if not os.path.isdir(model_save_path):\n",
        "            os.makedirs(model_save_path)\n",
        "\n",
        "        torch.save(model.state_dict(), model_save_path + 'model_%d.pt' % it)\n",
        "        print('SAVING MODEL FINISH')\n",
        "\n",
        "        print('START TEST')\n",
        "        model.eval()\n",
        "        t1_count = 0\n",
        "        t5_count = 0\n",
        "        for itest in range(10000):\n",
        "            test_img = test_images[itest:itest+1, :, :, :].astype(np.float32)\n",
        "            test_img = (test_img / 255.0 * 2.0) - 1.0\n",
        "\n",
        "            test_img = np.transpose(test_img, (0, 3, 1, 2))\n",
        "            with torch.no_grad():\n",
        "                pred = model(torch.from_numpy(test_img).to(DEVICE))\n",
        "\n",
        "            gt = test_gts[itest]\n",
        "\n",
        "            pred = pred.cpu().numpy()  # [1, 200]\n",
        "            pred = np.reshape(pred, num_class)\n",
        "\n",
        "            # ----- top5 accuracy\n",
        "            for ik in range(5):\n",
        "                max_index = np.argmax(pred)\n",
        "                if int(gt) == int(max_index):\n",
        "                    t5_count += 1\n",
        "                    # ----- top1 accuracy\n",
        "                    if ik == 0:\n",
        "                        t1_count += 1\n",
        "\n",
        "                pred[max_index] = -9999\n",
        "\n",
        "        print('top1 : %f    top5 : %f \\n' %(t1_count / 10000 * 100, t5_count / 10000 * 100))\n",
        "        f = open('%s.txt' %save_name, 'a+')\n",
        "        f.write('top1 : %f    top5 : %f \\n' %(t1_count / 10000 * 100, t5_count / 10000 * 100))\n",
        "        f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fezNJOZ-VMtt",
        "outputId": "ef6bc587-b8cf-4528-e7bf-c50765ebeb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "DATA LOAD\n",
            "img num : 207005 \n",
            "gt num : 207005 \n",
            "img num : 10000 \n",
            "gt num : 10000 \n",
            "DATA LOAD FINISH\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-df32df591bd6>:41: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  batch_gts = np.zeros(batch_size, dtype=np.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter : 0  lr : 0.01000   loss : 5.31818  time : 7.7340 \n",
            "iter : 100  lr : 0.00999   loss : 5.20614  time : 24.2269 \n",
            "iter : 200  lr : 0.00998   loss : 4.99263  time : 24.1846 \n",
            "iter : 300  lr : 0.00998   loss : 4.73512  time : 24.1318 \n",
            "iter : 400  lr : 0.00997   loss : 4.70748  time : 24.1187 \n",
            "iter : 500  lr : 0.00996   loss : 4.58229  time : 24.1151 \n",
            "iter : 600  lr : 0.00995   loss : 4.48722  time : 24.1100 \n",
            "iter : 700  lr : 0.00995   loss : 4.41911  time : 24.1070 \n",
            "iter : 800  lr : 0.00994   loss : 4.17581  time : 24.1293 \n",
            "iter : 900  lr : 0.00993   loss : 4.17330  time : 24.1246 \n",
            "iter : 1000  lr : 0.00992   loss : 4.04674  time : 24.1095 \n",
            "iter : 1100  lr : 0.00992   loss : 4.29217  time : 24.0865 \n",
            "iter : 1200  lr : 0.00991   loss : 4.17191  time : 24.0921 \n",
            "iter : 1300  lr : 0.00990   loss : 4.17485  time : 24.0842 \n",
            "iter : 1400  lr : 0.00989   loss : 3.97019  time : 24.0886 \n",
            "iter : 1500  lr : 0.00988   loss : 3.81341  time : 24.0853 \n",
            "iter : 1600  lr : 0.00988   loss : 4.21948  time : 24.0920 \n",
            "iter : 1700  lr : 0.00987   loss : 3.98330  time : 24.0857 \n",
            "iter : 1800  lr : 0.00986   loss : 3.72334  time : 24.0888 \n",
            "iter : 1900  lr : 0.00985   loss : 3.62449  time : 24.0934 \n",
            "iter : 2000  lr : 0.00985   loss : 3.59706  time : 24.0764 \n",
            "iter : 2100  lr : 0.00984   loss : 3.86410  time : 24.0974 \n",
            "iter : 2200  lr : 0.00983   loss : 3.83124  time : 24.0778 \n",
            "iter : 2300  lr : 0.00982   loss : 3.42414  time : 24.0834 \n",
            "iter : 2400  lr : 0.00982   loss : 3.50571  time : 24.1050 \n",
            "iter : 2500  lr : 0.00981   loss : 3.32324  time : 24.1106 \n",
            "iter : 2600  lr : 0.00980   loss : 3.71759  time : 24.0767 \n",
            "iter : 2700  lr : 0.00979   loss : 3.29793  time : 24.0801 \n",
            "iter : 2800  lr : 0.00978   loss : 3.50536  time : 24.0935 \n",
            "iter : 2900  lr : 0.00978   loss : 3.28636  time : 24.0829 \n",
            "iter : 3000  lr : 0.00977   loss : 3.38954  time : 24.0954 \n",
            "iter : 3100  lr : 0.00976   loss : 3.40749  time : 24.0919 \n",
            "iter : 3200  lr : 0.00975   loss : 3.51283  time : 24.1297 \n",
            "iter : 3300  lr : 0.00975   loss : 3.24750  time : 24.1182 \n",
            "iter : 3400  lr : 0.00974   loss : 3.03680  time : 24.1047 \n",
            "iter : 3500  lr : 0.00973   loss : 3.21389  time : 24.1456 \n",
            "iter : 3600  lr : 0.00972   loss : 3.01913  time : 24.1180 \n",
            "iter : 3700  lr : 0.00972   loss : 3.33786  time : 24.1237 \n",
            "iter : 3800  lr : 0.00971   loss : 3.10737  time : 24.1004 \n",
            "iter : 3900  lr : 0.00970   loss : 3.13093  time : 24.1061 \n",
            "iter : 4000  lr : 0.00969   loss : 2.94509  time : 24.1203 \n",
            "iter : 4100  lr : 0.00968   loss : 3.18891  time : 24.1024 \n",
            "iter : 4200  lr : 0.00968   loss : 2.57516  time : 24.0632 \n",
            "iter : 4300  lr : 0.00967   loss : 2.96154  time : 24.0747 \n",
            "iter : 4400  lr : 0.00966   loss : 2.77198  time : 24.0897 \n",
            "iter : 4500  lr : 0.00965   loss : 2.64157  time : 24.1050 \n",
            "iter : 4600  lr : 0.00965   loss : 2.81428  time : 24.0583 \n",
            "iter : 4700  lr : 0.00964   loss : 2.52863  time : 24.0649 \n",
            "iter : 4800  lr : 0.00963   loss : 2.90412  time : 24.0717 \n",
            "iter : 4900  lr : 0.00962   loss : 2.91651  time : 24.0544 \n",
            "iter : 5000  lr : 0.00962   loss : 2.63310  time : 24.0901 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 31.630000    top5 : 58.650000 \n",
            "\n",
            "iter : 5100  lr : 0.00961   loss : 2.87928  time : 152.5084 \n",
            "iter : 5200  lr : 0.00960   loss : 2.61185  time : 24.0897 \n",
            "iter : 5300  lr : 0.00959   loss : 2.71783  time : 24.0873 \n",
            "iter : 5400  lr : 0.00958   loss : 2.35852  time : 24.0801 \n",
            "iter : 5500  lr : 0.00958   loss : 2.30368  time : 24.0721 \n",
            "iter : 5600  lr : 0.00957   loss : 2.75036  time : 24.0718 \n",
            "iter : 5700  lr : 0.00956   loss : 2.98502  time : 24.0981 \n",
            "iter : 5800  lr : 0.00955   loss : 2.38857  time : 24.0558 \n",
            "iter : 5900  lr : 0.00955   loss : 2.63567  time : 24.0681 \n",
            "iter : 6000  lr : 0.00954   loss : 2.64066  time : 24.1029 \n",
            "iter : 6100  lr : 0.00953   loss : 2.50166  time : 24.0827 \n",
            "iter : 6200  lr : 0.00952   loss : 2.28507  time : 24.1195 \n",
            "iter : 6300  lr : 0.00952   loss : 2.43444  time : 24.0676 \n",
            "iter : 6400  lr : 0.00951   loss : 2.56380  time : 24.0763 \n",
            "iter : 6500  lr : 0.00950   loss : 2.60132  time : 24.0754 \n",
            "iter : 6600  lr : 0.00949   loss : 2.28865  time : 24.0743 \n",
            "iter : 6700  lr : 0.00948   loss : 2.70756  time : 24.0664 \n",
            "iter : 6800  lr : 0.00948   loss : 2.20306  time : 24.0856 \n",
            "iter : 6900  lr : 0.00947   loss : 2.34004  time : 24.0651 \n",
            "iter : 7000  lr : 0.00946   loss : 2.58485  time : 24.0924 \n",
            "iter : 7100  lr : 0.00945   loss : 2.36561  time : 24.0795 \n",
            "iter : 7200  lr : 0.00945   loss : 2.36979  time : 24.0922 \n",
            "iter : 7300  lr : 0.00944   loss : 2.46183  time : 24.0742 \n",
            "iter : 7400  lr : 0.00943   loss : 2.07444  time : 24.1037 \n",
            "iter : 7500  lr : 0.00942   loss : 2.52035  time : 24.0917 \n",
            "iter : 7600  lr : 0.00942   loss : 2.23624  time : 24.0686 \n",
            "iter : 7700  lr : 0.00941   loss : 2.28520  time : 24.1053 \n",
            "iter : 7800  lr : 0.00940   loss : 2.25644  time : 24.0786 \n",
            "iter : 7900  lr : 0.00939   loss : 2.16837  time : 24.0930 \n",
            "iter : 8000  lr : 0.00938   loss : 2.36248  time : 24.0928 \n",
            "iter : 8100  lr : 0.00938   loss : 2.09910  time : 24.0617 \n",
            "iter : 8200  lr : 0.00937   loss : 2.31238  time : 24.0865 \n",
            "iter : 8300  lr : 0.00936   loss : 2.43587  time : 24.0643 \n",
            "iter : 8400  lr : 0.00935   loss : 2.31805  time : 24.0736 \n",
            "iter : 8500  lr : 0.00935   loss : 2.05362  time : 24.0659 \n",
            "iter : 8600  lr : 0.00934   loss : 2.13861  time : 24.0674 \n",
            "iter : 8700  lr : 0.00933   loss : 1.98696  time : 24.1007 \n",
            "iter : 8800  lr : 0.00932   loss : 2.37979  time : 24.0917 \n",
            "iter : 8900  lr : 0.00932   loss : 2.13026  time : 24.0754 \n",
            "iter : 9000  lr : 0.00931   loss : 2.11931  time : 24.0707 \n",
            "iter : 9100  lr : 0.00930   loss : 2.25928  time : 24.0901 \n",
            "iter : 9200  lr : 0.00929   loss : 2.10194  time : 24.0865 \n",
            "iter : 9300  lr : 0.00928   loss : 1.98926  time : 24.0834 \n",
            "iter : 9400  lr : 0.00928   loss : 2.45283  time : 24.1022 \n",
            "iter : 9500  lr : 0.00927   loss : 2.23822  time : 24.0640 \n",
            "iter : 9600  lr : 0.00926   loss : 2.18644  time : 24.0762 \n",
            "iter : 9700  lr : 0.00925   loss : 1.89863  time : 24.0897 \n",
            "iter : 9800  lr : 0.00925   loss : 2.33977  time : 24.0733 \n",
            "iter : 9900  lr : 0.00924   loss : 1.93479  time : 24.0772 \n",
            "iter : 10000  lr : 0.00923   loss : 2.14104  time : 24.0707 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 44.750000    top5 : 72.740000 \n",
            "\n",
            "iter : 10100  lr : 0.00922   loss : 2.24467  time : 153.2554 \n",
            "iter : 10200  lr : 0.00922   loss : 2.04883  time : 24.0888 \n",
            "iter : 10300  lr : 0.00921   loss : 2.15886  time : 24.0483 \n",
            "iter : 10400  lr : 0.00920   loss : 2.42719  time : 24.0727 \n",
            "iter : 10500  lr : 0.00919   loss : 1.70210  time : 24.0322 \n",
            "iter : 10600  lr : 0.00918   loss : 1.68929  time : 24.0485 \n",
            "iter : 10700  lr : 0.00918   loss : 2.16849  time : 24.0400 \n",
            "iter : 10800  lr : 0.00917   loss : 1.83258  time : 24.0630 \n",
            "iter : 10900  lr : 0.00916   loss : 1.79228  time : 24.1001 \n",
            "iter : 11000  lr : 0.00915   loss : 2.20457  time : 24.1828 \n",
            "iter : 11100  lr : 0.00915   loss : 2.23799  time : 24.1131 \n",
            "iter : 11200  lr : 0.00914   loss : 1.86760  time : 24.1392 \n",
            "iter : 11300  lr : 0.00913   loss : 1.90665  time : 24.1408 \n",
            "iter : 11400  lr : 0.00912   loss : 1.82475  time : 24.1463 \n",
            "iter : 11500  lr : 0.00912   loss : 2.02866  time : 24.1463 \n",
            "iter : 11600  lr : 0.00911   loss : 1.85664  time : 24.1272 \n",
            "iter : 11700  lr : 0.00910   loss : 1.92524  time : 24.1386 \n",
            "iter : 11800  lr : 0.00909   loss : 1.77639  time : 24.1126 \n",
            "iter : 11900  lr : 0.00908   loss : 1.98444  time : 24.0855 \n",
            "iter : 12000  lr : 0.00908   loss : 1.98802  time : 24.1069 \n",
            "iter : 12100  lr : 0.00907   loss : 2.09907  time : 24.1076 \n",
            "iter : 12200  lr : 0.00906   loss : 1.81430  time : 24.1463 \n",
            "iter : 12300  lr : 0.00905   loss : 1.86648  time : 24.1413 \n",
            "iter : 12400  lr : 0.00905   loss : 1.87370  time : 24.1444 \n",
            "iter : 12500  lr : 0.00904   loss : 1.87921  time : 24.1321 \n",
            "iter : 12600  lr : 0.00903   loss : 2.04745  time : 24.1362 \n",
            "iter : 12700  lr : 0.00902   loss : 2.09036  time : 24.1381 \n",
            "iter : 12800  lr : 0.00902   loss : 1.50544  time : 24.1256 \n",
            "iter : 12900  lr : 0.00901   loss : 2.13485  time : 24.1006 \n",
            "iter : 13000  lr : 0.00900   loss : 1.96646  time : 24.1047 \n",
            "iter : 13100  lr : 0.00899   loss : 1.81291  time : 24.1052 \n",
            "iter : 13200  lr : 0.00898   loss : 1.75600  time : 24.1198 \n",
            "iter : 13300  lr : 0.00898   loss : 1.70053  time : 24.1099 \n",
            "iter : 13400  lr : 0.00897   loss : 1.98457  time : 24.1091 \n",
            "iter : 13500  lr : 0.00896   loss : 1.68891  time : 24.1195 \n",
            "iter : 13600  lr : 0.00895   loss : 1.75892  time : 24.0927 \n",
            "iter : 13700  lr : 0.00895   loss : 1.80322  time : 24.0781 \n",
            "iter : 13800  lr : 0.00894   loss : 1.82104  time : 24.0985 \n",
            "iter : 13900  lr : 0.00893   loss : 1.62038  time : 24.0762 \n",
            "iter : 14000  lr : 0.00892   loss : 1.80591  time : 24.0894 \n",
            "iter : 14100  lr : 0.00892   loss : 2.07650  time : 24.0726 \n",
            "iter : 14200  lr : 0.00891   loss : 1.72495  time : 24.0803 \n",
            "iter : 14300  lr : 0.00890   loss : 1.75830  time : 24.1097 \n",
            "iter : 14400  lr : 0.00889   loss : 1.61982  time : 24.1827 \n",
            "iter : 14500  lr : 0.00888   loss : 1.99962  time : 24.1701 \n",
            "iter : 14600  lr : 0.00888   loss : 1.63376  time : 24.1070 \n",
            "iter : 14700  lr : 0.00887   loss : 1.79986  time : 24.1189 \n",
            "iter : 14800  lr : 0.00886   loss : 1.58747  time : 24.0670 \n",
            "iter : 14900  lr : 0.00885   loss : 1.73647  time : 24.0672 \n",
            "iter : 15000  lr : 0.00885   loss : 1.96105  time : 24.0629 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 52.720000    top5 : 78.930000 \n",
            "\n",
            "iter : 15100  lr : 0.00884   loss : 2.16790  time : 151.0441 \n",
            "iter : 15200  lr : 0.00883   loss : 1.56096  time : 24.0789 \n",
            "iter : 15300  lr : 0.00882   loss : 1.41218  time : 24.0557 \n",
            "iter : 15400  lr : 0.00882   loss : 1.72726  time : 24.0749 \n",
            "iter : 15500  lr : 0.00881   loss : 1.77381  time : 24.0558 \n",
            "iter : 15600  lr : 0.00880   loss : 1.58605  time : 24.0709 \n",
            "iter : 15700  lr : 0.00879   loss : 1.70694  time : 24.0551 \n",
            "iter : 15800  lr : 0.00878   loss : 1.47522  time : 24.0760 \n",
            "iter : 15900  lr : 0.00878   loss : 1.55255  time : 24.0541 \n",
            "iter : 16000  lr : 0.00877   loss : 1.46306  time : 24.0717 \n",
            "iter : 16100  lr : 0.00876   loss : 1.63837  time : 24.0680 \n",
            "iter : 16200  lr : 0.00875   loss : 1.56254  time : 24.0699 \n",
            "iter : 16300  lr : 0.00875   loss : 1.31536  time : 24.0560 \n",
            "iter : 16400  lr : 0.00874   loss : 1.56707  time : 24.0787 \n",
            "iter : 16500  lr : 0.00873   loss : 1.65158  time : 24.1140 \n",
            "iter : 16600  lr : 0.00872   loss : 1.74773  time : 24.1163 \n",
            "iter : 16700  lr : 0.00872   loss : 1.75320  time : 24.1005 \n",
            "iter : 16800  lr : 0.00871   loss : 1.82531  time : 24.1296 \n",
            "iter : 16900  lr : 0.00870   loss : 1.47512  time : 24.0960 \n",
            "iter : 17000  lr : 0.00869   loss : 1.68386  time : 24.1106 \n",
            "iter : 17100  lr : 0.00868   loss : 1.79806  time : 24.1063 \n",
            "iter : 17200  lr : 0.00868   loss : 1.89542  time : 24.0879 \n",
            "iter : 17300  lr : 0.00867   loss : 1.81292  time : 24.0999 \n",
            "iter : 17400  lr : 0.00866   loss : 1.54712  time : 24.1116 \n",
            "iter : 17500  lr : 0.00865   loss : 1.25114  time : 24.0900 \n",
            "iter : 17600  lr : 0.00865   loss : 1.39362  time : 24.0863 \n",
            "iter : 17700  lr : 0.00864   loss : 1.70494  time : 24.0807 \n",
            "iter : 17800  lr : 0.00863   loss : 1.41378  time : 24.0510 \n",
            "iter : 17900  lr : 0.00862   loss : 1.47321  time : 24.0742 \n",
            "iter : 18000  lr : 0.00862   loss : 1.54418  time : 24.0615 \n",
            "iter : 18100  lr : 0.00861   loss : 1.50362  time : 24.0642 \n",
            "iter : 18200  lr : 0.00860   loss : 1.62649  time : 24.0702 \n",
            "iter : 18300  lr : 0.00859   loss : 1.73580  time : 24.0750 \n",
            "iter : 18400  lr : 0.00858   loss : 1.47590  time : 24.0495 \n",
            "iter : 18500  lr : 0.00858   loss : 1.73349  time : 24.0775 \n",
            "iter : 18600  lr : 0.00857   loss : 1.36621  time : 24.0518 \n",
            "iter : 18700  lr : 0.00856   loss : 1.49473  time : 24.0515 \n",
            "iter : 18800  lr : 0.00855   loss : 1.54391  time : 24.0655 \n",
            "iter : 18900  lr : 0.00855   loss : 1.57599  time : 24.0734 \n",
            "iter : 19000  lr : 0.00854   loss : 1.60935  time : 24.1232 \n",
            "iter : 19100  lr : 0.00853   loss : 1.42892  time : 24.0921 \n",
            "iter : 19200  lr : 0.00852   loss : 1.55787  time : 24.0989 \n",
            "iter : 19300  lr : 0.00852   loss : 1.50820  time : 24.1220 \n",
            "iter : 19400  lr : 0.00851   loss : 1.16063  time : 24.1017 \n",
            "iter : 19500  lr : 0.00850   loss : 1.72263  time : 24.1033 \n",
            "iter : 19600  lr : 0.00849   loss : 1.31662  time : 24.0958 \n",
            "iter : 19700  lr : 0.00848   loss : 1.42683  time : 24.1012 \n",
            "iter : 19800  lr : 0.00848   loss : 1.78351  time : 24.1018 \n",
            "iter : 19900  lr : 0.00847   loss : 1.49056  time : 24.1153 \n",
            "iter : 20000  lr : 0.00846   loss : 1.58469  time : 24.0818 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 56.410000    top5 : 80.710000 \n",
            "\n",
            "iter : 20100  lr : 0.00845   loss : 1.85741  time : 150.9028 \n",
            "iter : 20200  lr : 0.00845   loss : 1.43152  time : 24.0677 \n",
            "iter : 20300  lr : 0.00844   loss : 1.65337  time : 24.0432 \n",
            "iter : 20400  lr : 0.00843   loss : 1.47539  time : 24.0678 \n",
            "iter : 20500  lr : 0.00842   loss : 1.59269  time : 24.0594 \n",
            "iter : 20600  lr : 0.00842   loss : 1.32382  time : 24.0538 \n",
            "iter : 20700  lr : 0.00841   loss : 1.39256  time : 24.0582 \n",
            "iter : 20800  lr : 0.00840   loss : 1.30482  time : 24.0972 \n",
            "iter : 20900  lr : 0.00839   loss : 1.72499  time : 24.1038 \n",
            "iter : 21000  lr : 0.00838   loss : 1.61579  time : 24.0844 \n",
            "iter : 21100  lr : 0.00838   loss : 1.47078  time : 24.1195 \n",
            "iter : 21200  lr : 0.00837   loss : 1.38917  time : 24.0919 \n",
            "iter : 21300  lr : 0.00836   loss : 1.66695  time : 24.1002 \n",
            "iter : 21400  lr : 0.00835   loss : 1.26644  time : 24.1126 \n",
            "iter : 21500  lr : 0.00835   loss : 1.34785  time : 24.1137 \n",
            "iter : 21600  lr : 0.00834   loss : 1.39048  time : 24.1035 \n",
            "iter : 21700  lr : 0.00833   loss : 1.60475  time : 24.0885 \n",
            "iter : 21800  lr : 0.00832   loss : 1.63269  time : 24.1031 \n",
            "iter : 21900  lr : 0.00832   loss : 1.63952  time : 24.0926 \n",
            "iter : 22000  lr : 0.00831   loss : 1.31511  time : 24.0693 \n",
            "iter : 22100  lr : 0.00830   loss : 1.53107  time : 24.0694 \n",
            "iter : 22200  lr : 0.00829   loss : 1.26164  time : 24.0713 \n",
            "iter : 22300  lr : 0.00828   loss : 1.12794  time : 24.0689 \n",
            "iter : 22400  lr : 0.00828   loss : 1.40803  time : 24.0556 \n",
            "iter : 22500  lr : 0.00827   loss : 1.15689  time : 24.0528 \n",
            "iter : 22600  lr : 0.00826   loss : 1.41802  time : 24.0598 \n",
            "iter : 22700  lr : 0.00825   loss : 1.58191  time : 24.0643 \n",
            "iter : 22800  lr : 0.00825   loss : 1.23578  time : 24.0628 \n",
            "iter : 22900  lr : 0.00824   loss : 1.43843  time : 24.0743 \n",
            "iter : 23000  lr : 0.00823   loss : 1.60010  time : 24.0754 \n",
            "iter : 23100  lr : 0.00822   loss : 1.34357  time : 24.0981 \n",
            "iter : 23200  lr : 0.00822   loss : 1.07397  time : 24.1181 \n",
            "iter : 23300  lr : 0.00821   loss : 1.26695  time : 24.0958 \n",
            "iter : 23400  lr : 0.00820   loss : 1.55709  time : 24.0977 \n",
            "iter : 23500  lr : 0.00819   loss : 1.30662  time : 24.1093 \n",
            "iter : 23600  lr : 0.00818   loss : 1.24742  time : 24.1005 \n",
            "iter : 23700  lr : 0.00818   loss : 1.44179  time : 24.0928 \n",
            "iter : 23800  lr : 0.00817   loss : 1.49171  time : 24.1028 \n",
            "iter : 23900  lr : 0.00816   loss : 1.26873  time : 24.0997 \n",
            "iter : 24000  lr : 0.00815   loss : 1.35019  time : 24.0863 \n",
            "iter : 24100  lr : 0.00815   loss : 1.38998  time : 24.0910 \n",
            "iter : 24200  lr : 0.00814   loss : 1.35700  time : 24.1086 \n",
            "iter : 24300  lr : 0.00813   loss : 1.21926  time : 24.1057 \n",
            "iter : 24400  lr : 0.00812   loss : 1.22414  time : 24.0764 \n",
            "iter : 24500  lr : 0.00812   loss : 1.25527  time : 24.0645 \n",
            "iter : 24600  lr : 0.00811   loss : 1.36488  time : 24.0512 \n",
            "iter : 24700  lr : 0.00810   loss : 1.17011  time : 24.0692 \n",
            "iter : 24800  lr : 0.00809   loss : 1.21478  time : 24.0740 \n",
            "iter : 24900  lr : 0.00808   loss : 1.19630  time : 24.0745 \n",
            "iter : 25000  lr : 0.00808   loss : 1.08455  time : 24.0599 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 60.120000    top5 : 82.880000 \n",
            "\n",
            "iter : 25100  lr : 0.00807   loss : 1.41623  time : 151.3636 \n",
            "iter : 25200  lr : 0.00806   loss : 1.36560  time : 24.1483 \n",
            "iter : 25300  lr : 0.00805   loss : 1.28527  time : 24.0956 \n",
            "iter : 25400  lr : 0.00805   loss : 1.38211  time : 24.1025 \n",
            "iter : 25500  lr : 0.00804   loss : 1.29699  time : 24.0974 \n",
            "iter : 25600  lr : 0.00803   loss : 1.17815  time : 24.0979 \n",
            "iter : 25700  lr : 0.00802   loss : 1.41870  time : 24.0985 \n",
            "iter : 25800  lr : 0.00802   loss : 1.23556  time : 24.1057 \n",
            "iter : 25900  lr : 0.00801   loss : 1.38228  time : 24.1104 \n",
            "iter : 26000  lr : 0.00800   loss : 1.11564  time : 24.1326 \n",
            "iter : 26100  lr : 0.00799   loss : 1.24361  time : 24.1268 \n",
            "iter : 26200  lr : 0.00798   loss : 0.95751  time : 24.1004 \n",
            "iter : 26300  lr : 0.00798   loss : 1.37191  time : 24.1118 \n",
            "iter : 26400  lr : 0.00797   loss : 1.20500  time : 24.0571 \n",
            "iter : 26500  lr : 0.00796   loss : 1.19019  time : 24.0668 \n",
            "iter : 26600  lr : 0.00795   loss : 1.41371  time : 24.0661 \n",
            "iter : 26700  lr : 0.00795   loss : 0.90156  time : 24.0855 \n",
            "iter : 26800  lr : 0.00794   loss : 1.15531  time : 24.0682 \n",
            "iter : 26900  lr : 0.00793   loss : 1.30282  time : 24.0706 \n",
            "iter : 27000  lr : 0.00792   loss : 1.01787  time : 24.0599 \n",
            "iter : 27100  lr : 0.00792   loss : 1.25840  time : 24.0506 \n",
            "iter : 27200  lr : 0.00791   loss : 1.33346  time : 24.1121 \n",
            "iter : 27300  lr : 0.00790   loss : 1.29207  time : 24.1002 \n",
            "iter : 27400  lr : 0.00789   loss : 1.16872  time : 24.0974 \n",
            "iter : 27500  lr : 0.00788   loss : 1.03927  time : 24.1479 \n",
            "iter : 27600  lr : 0.00788   loss : 1.13353  time : 24.1069 \n",
            "iter : 27700  lr : 0.00787   loss : 1.20575  time : 24.0996 \n",
            "iter : 27800  lr : 0.00786   loss : 1.54357  time : 24.1154 \n",
            "iter : 27900  lr : 0.00785   loss : 0.96213  time : 24.0892 \n",
            "iter : 28000  lr : 0.00785   loss : 1.32466  time : 24.0873 \n",
            "iter : 28100  lr : 0.00784   loss : 1.56084  time : 24.1119 \n",
            "iter : 28200  lr : 0.00783   loss : 1.27913  time : 24.1026 \n",
            "iter : 28300  lr : 0.00782   loss : 1.39223  time : 24.0998 \n",
            "iter : 28400  lr : 0.00782   loss : 1.28768  time : 24.1184 \n",
            "iter : 28500  lr : 0.00781   loss : 1.34042  time : 24.0799 \n",
            "iter : 28600  lr : 0.00780   loss : 1.16400  time : 24.0599 \n",
            "iter : 28700  lr : 0.00779   loss : 0.89700  time : 24.0662 \n",
            "iter : 28800  lr : 0.00778   loss : 0.81626  time : 24.0622 \n",
            "iter : 28900  lr : 0.00778   loss : 1.31768  time : 24.0685 \n",
            "iter : 29000  lr : 0.00777   loss : 1.11912  time : 24.0482 \n",
            "iter : 29100  lr : 0.00776   loss : 1.27698  time : 24.0548 \n",
            "iter : 29200  lr : 0.00775   loss : 1.22136  time : 24.0694 \n",
            "iter : 29300  lr : 0.00775   loss : 1.25583  time : 24.0579 \n",
            "iter : 29400  lr : 0.00774   loss : 1.63127  time : 24.0955 \n",
            "iter : 29500  lr : 0.00773   loss : 1.03300  time : 24.1433 \n",
            "iter : 29600  lr : 0.00772   loss : 1.02451  time : 24.1098 \n",
            "iter : 29700  lr : 0.00772   loss : 1.03004  time : 24.0891 \n",
            "iter : 29800  lr : 0.00771   loss : 1.24712  time : 24.1081 \n",
            "iter : 29900  lr : 0.00770   loss : 1.30302  time : 24.1011 \n",
            "iter : 30000  lr : 0.00769   loss : 1.40620  time : 24.0894 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 63.310000    top5 : 86.010000 \n",
            "\n",
            "iter : 30100  lr : 0.00768   loss : 1.03196  time : 151.3288 \n",
            "iter : 30200  lr : 0.00768   loss : 1.29794  time : 24.0943 \n",
            "iter : 30300  lr : 0.00767   loss : 0.95151  time : 24.0711 \n",
            "iter : 30400  lr : 0.00766   loss : 1.14126  time : 24.0530 \n",
            "iter : 30500  lr : 0.00765   loss : 1.08717  time : 24.0617 \n",
            "iter : 30600  lr : 0.00765   loss : 1.13701  time : 24.0621 \n",
            "iter : 30700  lr : 0.00764   loss : 1.22952  time : 24.0641 \n",
            "iter : 30800  lr : 0.00763   loss : 1.15206  time : 24.0618 \n",
            "iter : 30900  lr : 0.00762   loss : 1.06316  time : 24.0638 \n",
            "iter : 31000  lr : 0.00762   loss : 1.07043  time : 24.0539 \n",
            "iter : 31100  lr : 0.00761   loss : 1.29391  time : 24.0465 \n",
            "iter : 31200  lr : 0.00760   loss : 0.94660  time : 24.1078 \n",
            "iter : 31300  lr : 0.00759   loss : 1.03414  time : 24.1104 \n",
            "iter : 31400  lr : 0.00758   loss : 1.11729  time : 24.1056 \n",
            "iter : 31500  lr : 0.00758   loss : 0.97408  time : 24.1136 \n",
            "iter : 31600  lr : 0.00757   loss : 1.41246  time : 24.0937 \n",
            "iter : 31700  lr : 0.00756   loss : 1.19931  time : 24.1109 \n",
            "iter : 31800  lr : 0.00755   loss : 1.11832  time : 24.0923 \n",
            "iter : 31900  lr : 0.00755   loss : 1.13715  time : 24.0846 \n",
            "iter : 32000  lr : 0.00754   loss : 1.04482  time : 24.0936 \n",
            "iter : 32100  lr : 0.00753   loss : 1.29208  time : 24.0838 \n",
            "iter : 32200  lr : 0.00752   loss : 1.20690  time : 24.1181 \n",
            "iter : 32300  lr : 0.00752   loss : 1.46779  time : 24.1032 \n",
            "iter : 32400  lr : 0.00751   loss : 1.03793  time : 24.1065 \n",
            "iter : 32500  lr : 0.00750   loss : 1.24174  time : 24.0784 \n",
            "iter : 32600  lr : 0.00749   loss : 1.10564  time : 24.0781 \n",
            "iter : 32700  lr : 0.00748   loss : 1.19951  time : 24.0638 \n",
            "iter : 32800  lr : 0.00748   loss : 0.97208  time : 24.0713 \n",
            "iter : 32900  lr : 0.00747   loss : 0.86669  time : 24.0623 \n",
            "iter : 33000  lr : 0.00746   loss : 1.25526  time : 24.0697 \n",
            "iter : 33100  lr : 0.00745   loss : 0.88415  time : 24.0661 \n",
            "iter : 33200  lr : 0.00745   loss : 1.18941  time : 24.0651 \n",
            "iter : 33300  lr : 0.00744   loss : 0.90338  time : 24.0422 \n",
            "iter : 33400  lr : 0.00743   loss : 1.18176  time : 24.1040 \n",
            "iter : 33500  lr : 0.00742   loss : 1.48175  time : 24.1130 \n",
            "iter : 33600  lr : 0.00742   loss : 0.92042  time : 24.0920 \n",
            "iter : 33700  lr : 0.00741   loss : 1.27272  time : 24.1065 \n",
            "iter : 33800  lr : 0.00740   loss : 1.08009  time : 24.1199 \n",
            "iter : 33900  lr : 0.00739   loss : 1.24335  time : 24.1039 \n",
            "iter : 34000  lr : 0.00738   loss : 1.22546  time : 24.1168 \n",
            "iter : 34100  lr : 0.00738   loss : 1.38026  time : 24.0986 \n",
            "iter : 34200  lr : 0.00737   loss : 1.13354  time : 24.1012 \n",
            "iter : 34300  lr : 0.00736   loss : 1.00106  time : 24.1033 \n",
            "iter : 34400  lr : 0.00735   loss : 1.28854  time : 24.0930 \n",
            "iter : 34500  lr : 0.00735   loss : 1.04231  time : 24.0731 \n",
            "iter : 34600  lr : 0.00734   loss : 0.98594  time : 24.0798 \n",
            "iter : 34700  lr : 0.00733   loss : 1.07913  time : 24.0913 \n",
            "iter : 34800  lr : 0.00732   loss : 0.99407  time : 24.0719 \n",
            "iter : 34900  lr : 0.00732   loss : 1.01656  time : 24.0660 \n",
            "iter : 35000  lr : 0.00731   loss : 1.02800  time : 24.0611 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 63.460000    top5 : 85.720000 \n",
            "\n",
            "iter : 35100  lr : 0.00730   loss : 1.04322  time : 151.3692 \n",
            "iter : 35200  lr : 0.00729   loss : 1.10243  time : 24.1493 \n",
            "iter : 35300  lr : 0.00728   loss : 1.24498  time : 24.1099 \n",
            "iter : 35400  lr : 0.00728   loss : 0.93319  time : 24.1155 \n",
            "iter : 35500  lr : 0.00727   loss : 1.03551  time : 24.1127 \n",
            "iter : 35600  lr : 0.00726   loss : 0.81053  time : 24.1201 \n",
            "iter : 35700  lr : 0.00725   loss : 0.84698  time : 24.1170 \n",
            "iter : 35800  lr : 0.00725   loss : 1.14848  time : 24.1031 \n",
            "iter : 35900  lr : 0.00724   loss : 0.69585  time : 24.1219 \n",
            "iter : 36000  lr : 0.00723   loss : 0.99725  time : 24.0863 \n",
            "iter : 36100  lr : 0.00722   loss : 1.12759  time : 24.1196 \n",
            "iter : 36200  lr : 0.00722   loss : 1.22552  time : 24.0858 \n",
            "iter : 36300  lr : 0.00721   loss : 0.86076  time : 24.0737 \n",
            "iter : 36400  lr : 0.00720   loss : 0.92169  time : 24.0839 \n",
            "iter : 36500  lr : 0.00719   loss : 0.89511  time : 24.0602 \n",
            "iter : 36600  lr : 0.00718   loss : 0.96014  time : 24.0670 \n",
            "iter : 36700  lr : 0.00718   loss : 1.02939  time : 24.0704 \n",
            "iter : 36800  lr : 0.00717   loss : 0.90064  time : 24.0488 \n",
            "iter : 36900  lr : 0.00716   loss : 0.73990  time : 24.0504 \n",
            "iter : 37000  lr : 0.00715   loss : 1.04147  time : 24.0942 \n",
            "iter : 37100  lr : 0.00715   loss : 1.02753  time : 24.1233 \n",
            "iter : 37200  lr : 0.00714   loss : 1.10483  time : 24.0755 \n",
            "iter : 37300  lr : 0.00713   loss : 1.00538  time : 24.1303 \n",
            "iter : 37400  lr : 0.00712   loss : 0.77972  time : 24.0938 \n",
            "iter : 37500  lr : 0.00712   loss : 0.91277  time : 24.1070 \n",
            "iter : 37600  lr : 0.00711   loss : 0.91748  time : 24.1038 \n",
            "iter : 37700  lr : 0.00710   loss : 1.08142  time : 24.0978 \n",
            "iter : 37800  lr : 0.00709   loss : 0.76359  time : 24.1217 \n",
            "iter : 37900  lr : 0.00708   loss : 1.23858  time : 24.1221 \n",
            "iter : 38000  lr : 0.00708   loss : 0.83931  time : 24.1030 \n",
            "iter : 38100  lr : 0.00707   loss : 1.05257  time : 24.1119 \n",
            "iter : 38200  lr : 0.00706   loss : 0.95938  time : 24.1185 \n",
            "iter : 38300  lr : 0.00705   loss : 0.71962  time : 24.0771 \n",
            "iter : 38400  lr : 0.00705   loss : 0.90911  time : 24.0639 \n",
            "iter : 38500  lr : 0.00704   loss : 1.03523  time : 24.0711 \n",
            "iter : 38600  lr : 0.00703   loss : 1.19762  time : 24.0519 \n",
            "iter : 38700  lr : 0.00702   loss : 0.93029  time : 24.0521 \n",
            "iter : 38800  lr : 0.00702   loss : 0.82990  time : 24.0619 \n",
            "iter : 38900  lr : 0.00701   loss : 0.98048  time : 24.0574 \n",
            "iter : 39000  lr : 0.00700   loss : 0.86339  time : 24.0670 \n",
            "iter : 39100  lr : 0.00699   loss : 0.99531  time : 24.1258 \n",
            "iter : 39200  lr : 0.00698   loss : 0.82492  time : 24.0830 \n",
            "iter : 39300  lr : 0.00698   loss : 0.99772  time : 24.0963 \n",
            "iter : 39400  lr : 0.00697   loss : 0.92987  time : 24.1011 \n",
            "iter : 39500  lr : 0.00696   loss : 0.88675  time : 24.0921 \n",
            "iter : 39600  lr : 0.00695   loss : 1.18188  time : 24.0905 \n",
            "iter : 39700  lr : 0.00695   loss : 0.74460  time : 24.0861 \n",
            "iter : 39800  lr : 0.00694   loss : 0.97019  time : 24.0936 \n",
            "iter : 39900  lr : 0.00693   loss : 0.86735  time : 24.0970 \n",
            "iter : 40000  lr : 0.00692   loss : 0.79252  time : 24.0954 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 64.640000    top5 : 85.980000 \n",
            "\n",
            "iter : 40100  lr : 0.00692   loss : 0.94697  time : 151.2573 \n",
            "iter : 40200  lr : 0.00691   loss : 1.09328  time : 24.0588 \n",
            "iter : 40300  lr : 0.00690   loss : 0.75182  time : 24.0664 \n",
            "iter : 40400  lr : 0.00689   loss : 0.78864  time : 24.0607 \n",
            "iter : 40500  lr : 0.00688   loss : 1.04569  time : 24.1036 \n",
            "iter : 40600  lr : 0.00688   loss : 1.11668  time : 24.1140 \n",
            "iter : 40700  lr : 0.00687   loss : 0.85301  time : 24.1347 \n",
            "iter : 40800  lr : 0.00686   loss : 1.00131  time : 24.1230 \n",
            "iter : 40900  lr : 0.00685   loss : 0.78786  time : 24.0877 \n",
            "iter : 41000  lr : 0.00685   loss : 1.17691  time : 24.1049 \n",
            "iter : 41100  lr : 0.00684   loss : 1.21153  time : 24.1091 \n",
            "iter : 41200  lr : 0.00683   loss : 0.99450  time : 24.1039 \n",
            "iter : 41300  lr : 0.00682   loss : 0.66857  time : 24.0928 \n",
            "iter : 41400  lr : 0.00682   loss : 0.93823  time : 24.0995 \n",
            "iter : 41500  lr : 0.00681   loss : 0.72442  time : 24.0973 \n",
            "iter : 41600  lr : 0.00680   loss : 0.85230  time : 24.1156 \n",
            "iter : 41700  lr : 0.00679   loss : 0.75961  time : 24.1081 \n",
            "iter : 41800  lr : 0.00678   loss : 1.18146  time : 24.0785 \n",
            "iter : 41900  lr : 0.00678   loss : 0.90513  time : 24.0656 \n",
            "iter : 42000  lr : 0.00677   loss : 0.80289  time : 24.0574 \n",
            "iter : 42100  lr : 0.00676   loss : 0.71595  time : 24.0597 \n",
            "iter : 42200  lr : 0.00675   loss : 0.75858  time : 24.0470 \n",
            "iter : 42300  lr : 0.00675   loss : 0.88377  time : 24.0749 \n",
            "iter : 42400  lr : 0.00674   loss : 0.76105  time : 24.0629 \n",
            "iter : 42500  lr : 0.00673   loss : 0.90705  time : 24.0787 \n",
            "iter : 42600  lr : 0.00672   loss : 0.70527  time : 24.1338 \n",
            "iter : 42700  lr : 0.00672   loss : 0.80765  time : 24.0975 \n",
            "iter : 42800  lr : 0.00671   loss : 1.01586  time : 24.1105 \n",
            "iter : 42900  lr : 0.00670   loss : 0.86717  time : 24.0910 \n",
            "iter : 43000  lr : 0.00669   loss : 0.90464  time : 24.1153 \n",
            "iter : 43100  lr : 0.00668   loss : 0.96130  time : 24.1103 \n",
            "iter : 43200  lr : 0.00668   loss : 0.97368  time : 24.0999 \n",
            "iter : 43300  lr : 0.00667   loss : 1.05921  time : 24.0980 \n",
            "iter : 43400  lr : 0.00666   loss : 0.94127  time : 24.1064 \n",
            "iter : 43500  lr : 0.00665   loss : 0.91444  time : 24.0994 \n",
            "iter : 43600  lr : 0.00665   loss : 0.79361  time : 24.1049 \n",
            "iter : 43700  lr : 0.00664   loss : 1.05094  time : 24.1111 \n",
            "iter : 43800  lr : 0.00663   loss : 0.95094  time : 24.0618 \n",
            "iter : 43900  lr : 0.00662   loss : 0.95647  time : 24.0660 \n",
            "iter : 44000  lr : 0.00662   loss : 0.99535  time : 24.0504 \n",
            "iter : 44100  lr : 0.00661   loss : 0.75905  time : 24.0559 \n",
            "iter : 44200  lr : 0.00660   loss : 0.71927  time : 24.0762 \n",
            "iter : 44300  lr : 0.00659   loss : 0.60327  time : 24.0597 \n",
            "iter : 44400  lr : 0.00658   loss : 0.74112  time : 24.0784 \n",
            "iter : 44500  lr : 0.00658   loss : 0.94832  time : 24.0788 \n",
            "iter : 44600  lr : 0.00657   loss : 1.00517  time : 24.1275 \n",
            "iter : 44700  lr : 0.00656   loss : 0.83339  time : 24.1120 \n",
            "iter : 44800  lr : 0.00655   loss : 0.94312  time : 24.0997 \n",
            "iter : 44900  lr : 0.00655   loss : 0.77911  time : 24.1019 \n",
            "iter : 45000  lr : 0.00654   loss : 0.58234  time : 24.1012 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 66.990000    top5 : 87.620000 \n",
            "\n",
            "iter : 45100  lr : 0.00653   loss : 1.00175  time : 151.3167 \n",
            "iter : 45200  lr : 0.00652   loss : 0.90590  time : 24.1136 \n",
            "iter : 45300  lr : 0.00652   loss : 0.76494  time : 24.0753 \n",
            "iter : 45400  lr : 0.00651   loss : 0.79887  time : 24.0639 \n",
            "iter : 45500  lr : 0.00650   loss : 0.77490  time : 24.0809 \n",
            "iter : 45600  lr : 0.00649   loss : 0.81645  time : 24.0583 \n",
            "iter : 45700  lr : 0.00648   loss : 0.89367  time : 24.0951 \n",
            "iter : 45800  lr : 0.00648   loss : 0.90700  time : 24.1389 \n",
            "iter : 45900  lr : 0.00647   loss : 0.99938  time : 24.1143 \n",
            "iter : 46000  lr : 0.00646   loss : 0.77309  time : 24.1122 \n",
            "iter : 46100  lr : 0.00645   loss : 0.74120  time : 24.0967 \n",
            "iter : 46200  lr : 0.00645   loss : 0.90109  time : 24.1240 \n",
            "iter : 46300  lr : 0.00644   loss : 0.86226  time : 24.0983 \n",
            "iter : 46400  lr : 0.00643   loss : 0.84635  time : 24.1097 \n",
            "iter : 46500  lr : 0.00642   loss : 0.71282  time : 24.0990 \n",
            "iter : 46600  lr : 0.00642   loss : 0.90528  time : 24.0953 \n",
            "iter : 46700  lr : 0.00641   loss : 0.66586  time : 24.1035 \n",
            "iter : 46800  lr : 0.00640   loss : 0.78325  time : 24.0958 \n",
            "iter : 46900  lr : 0.00639   loss : 0.90012  time : 24.0895 \n",
            "iter : 47000  lr : 0.00638   loss : 0.83994  time : 24.0697 \n",
            "iter : 47100  lr : 0.00638   loss : 0.81969  time : 24.0734 \n",
            "iter : 47200  lr : 0.00637   loss : 0.92994  time : 24.0550 \n",
            "iter : 47300  lr : 0.00636   loss : 0.78615  time : 24.0724 \n",
            "iter : 47400  lr : 0.00635   loss : 0.56171  time : 24.0628 \n",
            "iter : 47500  lr : 0.00635   loss : 0.70688  time : 24.1164 \n",
            "iter : 47600  lr : 0.00634   loss : 0.91570  time : 24.1164 \n",
            "iter : 47700  lr : 0.00633   loss : 0.64935  time : 24.1160 \n",
            "iter : 47800  lr : 0.00632   loss : 0.88536  time : 24.1056 \n",
            "iter : 47900  lr : 0.00632   loss : 0.58476  time : 24.1117 \n",
            "iter : 48000  lr : 0.00631   loss : 0.60284  time : 24.1051 \n",
            "iter : 48100  lr : 0.00630   loss : 0.68785  time : 24.0910 \n",
            "iter : 48200  lr : 0.00629   loss : 0.79766  time : 24.1113 \n",
            "iter : 48300  lr : 0.00628   loss : 0.77015  time : 24.0910 \n",
            "iter : 48400  lr : 0.00628   loss : 0.84487  time : 24.0999 \n",
            "iter : 48500  lr : 0.00627   loss : 1.00160  time : 24.0901 \n",
            "iter : 48600  lr : 0.00626   loss : 0.79467  time : 24.0996 \n",
            "iter : 48700  lr : 0.00625   loss : 0.69673  time : 24.0878 \n",
            "iter : 48800  lr : 0.00625   loss : 0.66077  time : 24.0997 \n",
            "iter : 48900  lr : 0.00624   loss : 0.53545  time : 24.0650 \n",
            "iter : 49000  lr : 0.00623   loss : 0.68416  time : 24.0571 \n",
            "iter : 49100  lr : 0.00622   loss : 0.82593  time : 24.1062 \n",
            "iter : 49200  lr : 0.00622   loss : 0.65962  time : 24.1029 \n",
            "iter : 49300  lr : 0.00621   loss : 0.92532  time : 24.0998 \n",
            "iter : 49400  lr : 0.00620   loss : 0.67563  time : 24.1138 \n",
            "iter : 49500  lr : 0.00619   loss : 0.82548  time : 24.0941 \n",
            "iter : 49600  lr : 0.00618   loss : 0.84452  time : 24.0990 \n",
            "iter : 49700  lr : 0.00618   loss : 0.85316  time : 24.1115 \n",
            "iter : 49800  lr : 0.00617   loss : 0.75762  time : 24.0901 \n",
            "iter : 49900  lr : 0.00616   loss : 0.76955  time : 24.1046 \n",
            "iter : 50000  lr : 0.00615   loss : 0.79342  time : 24.0917 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 67.330000    top5 : 87.610000 \n",
            "\n",
            "iter : 50100  lr : 0.00615   loss : 0.71002  time : 151.6766 \n",
            "iter : 50200  lr : 0.00614   loss : 0.91789  time : 24.1272 \n",
            "iter : 50300  lr : 0.00613   loss : 0.69999  time : 24.0921 \n",
            "iter : 50400  lr : 0.00612   loss : 0.73337  time : 24.1277 \n",
            "iter : 50500  lr : 0.00612   loss : 0.85909  time : 24.1129 \n",
            "iter : 50600  lr : 0.00611   loss : 0.45030  time : 24.0955 \n",
            "iter : 50700  lr : 0.00610   loss : 0.89242  time : 24.1089 \n",
            "iter : 50800  lr : 0.00609   loss : 0.73075  time : 24.0958 \n",
            "iter : 50900  lr : 0.00608   loss : 0.81751  time : 24.1103 \n",
            "iter : 51000  lr : 0.00608   loss : 0.60023  time : 24.1096 \n",
            "iter : 51100  lr : 0.00607   loss : 0.69047  time : 24.0949 \n",
            "iter : 51200  lr : 0.00606   loss : 0.63913  time : 24.0923 \n",
            "iter : 51300  lr : 0.00605   loss : 0.72367  time : 24.1035 \n",
            "iter : 51400  lr : 0.00605   loss : 0.63134  time : 24.0509 \n",
            "iter : 51500  lr : 0.00604   loss : 0.72672  time : 24.0478 \n",
            "iter : 51600  lr : 0.00603   loss : 0.67089  time : 24.0642 \n",
            "iter : 51700  lr : 0.00602   loss : 0.47164  time : 24.1152 \n",
            "iter : 51800  lr : 0.00602   loss : 0.69157  time : 24.1099 \n",
            "iter : 51900  lr : 0.00601   loss : 0.67061  time : 24.1167 \n",
            "iter : 52000  lr : 0.00600   loss : 0.48180  time : 24.1152 \n",
            "iter : 52100  lr : 0.00599   loss : 0.73456  time : 24.1019 \n",
            "iter : 52200  lr : 0.00598   loss : 0.65502  time : 24.1218 \n",
            "iter : 52300  lr : 0.00598   loss : 0.80153  time : 24.1026 \n",
            "iter : 52400  lr : 0.00597   loss : 0.82114  time : 24.1052 \n",
            "iter : 52500  lr : 0.00596   loss : 0.73355  time : 24.1036 \n",
            "iter : 52600  lr : 0.00595   loss : 0.53521  time : 24.0956 \n",
            "iter : 52700  lr : 0.00595   loss : 0.68445  time : 24.0901 \n",
            "iter : 52800  lr : 0.00594   loss : 0.62145  time : 24.1198 \n",
            "iter : 52900  lr : 0.00593   loss : 0.66593  time : 24.0774 \n",
            "iter : 53000  lr : 0.00592   loss : 0.53602  time : 24.0949 \n",
            "iter : 53100  lr : 0.00592   loss : 0.53097  time : 24.1090 \n",
            "iter : 53200  lr : 0.00591   loss : 0.54688  time : 24.1281 \n",
            "iter : 53300  lr : 0.00590   loss : 0.41573  time : 24.0989 \n",
            "iter : 53400  lr : 0.00589   loss : 0.73113  time : 24.1125 \n",
            "iter : 53500  lr : 0.00588   loss : 0.61793  time : 24.1010 \n",
            "iter : 53600  lr : 0.00588   loss : 0.51677  time : 24.0942 \n",
            "iter : 53700  lr : 0.00587   loss : 0.58918  time : 24.0951 \n",
            "iter : 53800  lr : 0.00586   loss : 0.58674  time : 24.0970 \n",
            "iter : 53900  lr : 0.00585   loss : 0.55068  time : 24.0917 \n",
            "iter : 54000  lr : 0.00585   loss : 0.77969  time : 24.0992 \n",
            "iter : 54100  lr : 0.00584   loss : 0.54046  time : 24.0965 \n",
            "iter : 54200  lr : 0.00583   loss : 0.52020  time : 24.1129 \n",
            "iter : 54300  lr : 0.00582   loss : 0.59920  time : 24.0935 \n",
            "iter : 54400  lr : 0.00582   loss : 0.49074  time : 24.0697 \n",
            "iter : 54500  lr : 0.00581   loss : 0.82056  time : 24.0480 \n",
            "iter : 54600  lr : 0.00580   loss : 0.59478  time : 24.1006 \n",
            "iter : 54700  lr : 0.00579   loss : 0.72092  time : 24.1174 \n",
            "iter : 54800  lr : 0.00578   loss : 0.66832  time : 24.1208 \n",
            "iter : 54900  lr : 0.00578   loss : 0.53052  time : 24.0932 \n",
            "iter : 55000  lr : 0.00577   loss : 0.39268  time : 24.0922 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 68.740000    top5 : 88.170000 \n",
            "\n",
            "iter : 55100  lr : 0.00576   loss : 0.58175  time : 151.1197 \n",
            "iter : 55200  lr : 0.00575   loss : 0.54284  time : 24.1142 \n",
            "iter : 55300  lr : 0.00575   loss : 0.47490  time : 24.0710 \n",
            "iter : 55400  lr : 0.00574   loss : 0.53832  time : 24.0720 \n",
            "iter : 55500  lr : 0.00573   loss : 0.51573  time : 24.0480 \n",
            "iter : 55600  lr : 0.00572   loss : 0.52808  time : 24.1167 \n",
            "iter : 55700  lr : 0.00572   loss : 0.53131  time : 24.1000 \n",
            "iter : 55800  lr : 0.00571   loss : 0.44618  time : 24.1177 \n",
            "iter : 55900  lr : 0.00570   loss : 0.61031  time : 24.0896 \n",
            "iter : 56000  lr : 0.00569   loss : 0.62067  time : 24.0928 \n",
            "iter : 56100  lr : 0.00568   loss : 0.65559  time : 24.0879 \n",
            "iter : 56200  lr : 0.00568   loss : 0.54657  time : 24.0883 \n",
            "iter : 56300  lr : 0.00567   loss : 0.64644  time : 24.0964 \n",
            "iter : 56400  lr : 0.00566   loss : 0.68865  time : 24.0921 \n",
            "iter : 56500  lr : 0.00565   loss : 0.57553  time : 24.0993 \n",
            "iter : 56600  lr : 0.00565   loss : 0.58385  time : 24.1031 \n",
            "iter : 56700  lr : 0.00564   loss : 0.53736  time : 24.1109 \n",
            "iter : 56800  lr : 0.00563   loss : 0.49073  time : 24.0675 \n",
            "iter : 56900  lr : 0.00562   loss : 0.55731  time : 24.0548 \n",
            "iter : 57000  lr : 0.00562   loss : 0.52233  time : 24.0923 \n",
            "iter : 57100  lr : 0.00561   loss : 0.53729  time : 24.1026 \n",
            "iter : 57200  lr : 0.00560   loss : 0.54128  time : 24.1091 \n",
            "iter : 57300  lr : 0.00559   loss : 0.70570  time : 24.1104 \n",
            "iter : 57400  lr : 0.00558   loss : 0.61303  time : 24.1066 \n",
            "iter : 57500  lr : 0.00558   loss : 0.63124  time : 24.1044 \n",
            "iter : 57600  lr : 0.00557   loss : 0.56668  time : 24.1091 \n",
            "iter : 57700  lr : 0.00556   loss : 0.55528  time : 24.1015 \n",
            "iter : 57800  lr : 0.00555   loss : 0.74924  time : 24.1032 \n",
            "iter : 57900  lr : 0.00555   loss : 0.50128  time : 24.1073 \n",
            "iter : 58000  lr : 0.00554   loss : 0.65698  time : 24.0965 \n",
            "iter : 58100  lr : 0.00553   loss : 0.53440  time : 24.1109 \n",
            "iter : 58200  lr : 0.00552   loss : 0.47429  time : 24.0811 \n",
            "iter : 58300  lr : 0.00552   loss : 0.80440  time : 24.0786 \n",
            "iter : 58400  lr : 0.00551   loss : 0.43755  time : 24.0738 \n",
            "iter : 58500  lr : 0.00550   loss : 0.58878  time : 24.1188 \n",
            "iter : 58600  lr : 0.00549   loss : 0.68475  time : 24.1198 \n",
            "iter : 58700  lr : 0.00548   loss : 0.60754  time : 24.1055 \n",
            "iter : 58800  lr : 0.00548   loss : 0.51325  time : 24.1003 \n",
            "iter : 58900  lr : 0.00547   loss : 0.44088  time : 24.0954 \n",
            "iter : 59000  lr : 0.00546   loss : 0.54208  time : 24.0992 \n",
            "iter : 59100  lr : 0.00545   loss : 0.50909  time : 24.1001 \n",
            "iter : 59200  lr : 0.00545   loss : 0.52040  time : 24.0929 \n",
            "iter : 59300  lr : 0.00544   loss : 0.73963  time : 24.0895 \n",
            "iter : 59400  lr : 0.00543   loss : 0.62436  time : 24.1065 \n",
            "iter : 59500  lr : 0.00542   loss : 0.39264  time : 24.0651 \n",
            "iter : 59600  lr : 0.00542   loss : 0.65897  time : 24.0780 \n",
            "iter : 59700  lr : 0.00541   loss : 0.53533  time : 24.0583 \n",
            "iter : 59800  lr : 0.00540   loss : 0.37377  time : 24.0919 \n",
            "iter : 59900  lr : 0.00539   loss : 0.48519  time : 24.0992 \n",
            "iter : 60000  lr : 0.00538   loss : 0.63441  time : 24.1228 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 68.140000    top5 : 87.980000 \n",
            "\n",
            "iter : 60100  lr : 0.00538   loss : 0.56219  time : 151.1551 \n",
            "iter : 60200  lr : 0.00537   loss : 0.48069  time : 24.1014 \n",
            "iter : 60300  lr : 0.00536   loss : 0.47492  time : 24.0866 \n",
            "iter : 60400  lr : 0.00535   loss : 0.56568  time : 24.1218 \n",
            "iter : 60500  lr : 0.00535   loss : 0.66478  time : 24.0563 \n",
            "iter : 60600  lr : 0.00534   loss : 0.49880  time : 24.0747 \n",
            "iter : 60700  lr : 0.00533   loss : 0.56416  time : 24.1248 \n",
            "iter : 60800  lr : 0.00532   loss : 0.63864  time : 24.1199 \n",
            "iter : 60900  lr : 0.00532   loss : 0.42740  time : 24.1130 \n",
            "iter : 61000  lr : 0.00531   loss : 0.41981  time : 24.0923 \n",
            "iter : 61100  lr : 0.00530   loss : 0.42164  time : 24.0842 \n",
            "iter : 61200  lr : 0.00529   loss : 0.51333  time : 24.0926 \n",
            "iter : 61300  lr : 0.00528   loss : 0.44345  time : 24.0842 \n",
            "iter : 61400  lr : 0.00528   loss : 0.52733  time : 24.0925 \n",
            "iter : 61500  lr : 0.00527   loss : 0.58812  time : 24.0990 \n",
            "iter : 61600  lr : 0.00526   loss : 0.34297  time : 24.1245 \n",
            "iter : 61700  lr : 0.00525   loss : 0.50801  time : 24.0512 \n",
            "iter : 61800  lr : 0.00525   loss : 0.43704  time : 24.0652 \n",
            "iter : 61900  lr : 0.00524   loss : 0.50734  time : 24.0654 \n",
            "iter : 62000  lr : 0.00523   loss : 0.42722  time : 24.0856 \n",
            "iter : 62100  lr : 0.00522   loss : 0.52803  time : 24.1142 \n",
            "iter : 62200  lr : 0.00522   loss : 0.55712  time : 24.1146 \n",
            "iter : 62300  lr : 0.00521   loss : 0.48094  time : 24.1321 \n",
            "iter : 62400  lr : 0.00520   loss : 0.45905  time : 24.0907 \n",
            "iter : 62500  lr : 0.00519   loss : 0.62565  time : 24.1161 \n",
            "iter : 62600  lr : 0.00518   loss : 0.38809  time : 24.0979 \n",
            "iter : 62700  lr : 0.00518   loss : 0.36014  time : 24.1236 \n",
            "iter : 62800  lr : 0.00517   loss : 0.38643  time : 24.0987 \n",
            "iter : 62900  lr : 0.00516   loss : 0.56634  time : 24.0971 \n",
            "iter : 63000  lr : 0.00515   loss : 0.56976  time : 24.0839 \n",
            "iter : 63100  lr : 0.00515   loss : 0.79493  time : 24.0915 \n",
            "iter : 63200  lr : 0.00514   loss : 0.37687  time : 24.0981 \n",
            "iter : 63300  lr : 0.00513   loss : 0.53521  time : 24.0976 \n",
            "iter : 63400  lr : 0.00512   loss : 0.38472  time : 24.0987 \n",
            "iter : 63500  lr : 0.00512   loss : 0.48111  time : 24.1117 \n",
            "iter : 63600  lr : 0.00511   loss : 0.36760  time : 24.1093 \n",
            "iter : 63700  lr : 0.00510   loss : 0.30614  time : 24.1044 \n",
            "iter : 63800  lr : 0.00509   loss : 0.25561  time : 24.1132 \n",
            "iter : 63900  lr : 0.00508   loss : 0.37653  time : 24.0990 \n",
            "iter : 64000  lr : 0.00508   loss : 0.45262  time : 24.1003 \n",
            "iter : 64100  lr : 0.00507   loss : 0.54124  time : 24.0952 \n",
            "iter : 64200  lr : 0.00506   loss : 0.62701  time : 24.1000 \n",
            "iter : 64300  lr : 0.00505   loss : 0.38853  time : 24.0912 \n",
            "iter : 64400  lr : 0.00505   loss : 0.51514  time : 24.0962 \n",
            "iter : 64500  lr : 0.00504   loss : 0.45678  time : 24.0873 \n",
            "iter : 64600  lr : 0.00503   loss : 0.43864  time : 24.0763 \n",
            "iter : 64700  lr : 0.00502   loss : 0.36683  time : 24.0569 \n",
            "iter : 64800  lr : 0.00502   loss : 0.38898  time : 24.1269 \n",
            "iter : 64900  lr : 0.00501   loss : 0.44798  time : 24.1160 \n",
            "iter : 65000  lr : 0.00500   loss : 0.42527  time : 24.1093 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 70.050000    top5 : 88.850000 \n",
            "\n",
            "iter : 65100  lr : 0.00499   loss : 0.51215  time : 151.3061 \n",
            "iter : 65200  lr : 0.00498   loss : 0.60732  time : 24.1591 \n",
            "iter : 65300  lr : 0.00498   loss : 0.55647  time : 24.0833 \n",
            "iter : 65400  lr : 0.00497   loss : 0.40912  time : 24.0876 \n",
            "iter : 65500  lr : 0.00496   loss : 0.34519  time : 24.0900 \n",
            "iter : 65600  lr : 0.00495   loss : 0.45350  time : 24.0660 \n",
            "iter : 65700  lr : 0.00495   loss : 0.46369  time : 24.1016 \n",
            "iter : 65800  lr : 0.00494   loss : 0.30602  time : 24.1110 \n",
            "iter : 65900  lr : 0.00493   loss : 0.42070  time : 24.1130 \n",
            "iter : 66000  lr : 0.00492   loss : 0.50866  time : 24.1052 \n",
            "iter : 66100  lr : 0.00492   loss : 0.50412  time : 24.0917 \n",
            "iter : 66200  lr : 0.00491   loss : 0.44816  time : 24.1066 \n",
            "iter : 66300  lr : 0.00490   loss : 0.63054  time : 24.1028 \n",
            "iter : 66400  lr : 0.00489   loss : 0.42615  time : 24.1018 \n",
            "iter : 66500  lr : 0.00488   loss : 0.36356  time : 24.0974 \n",
            "iter : 66600  lr : 0.00488   loss : 0.49085  time : 24.1043 \n",
            "iter : 66700  lr : 0.00487   loss : 0.35049  time : 24.0931 \n",
            "iter : 66800  lr : 0.00486   loss : 0.37156  time : 24.0823 \n",
            "iter : 66900  lr : 0.00485   loss : 0.55015  time : 24.0673 \n",
            "iter : 67000  lr : 0.00485   loss : 0.47389  time : 24.0481 \n",
            "iter : 67100  lr : 0.00484   loss : 0.52480  time : 24.0683 \n",
            "iter : 67200  lr : 0.00483   loss : 0.47770  time : 24.0999 \n",
            "iter : 67300  lr : 0.00482   loss : 0.47192  time : 24.0999 \n",
            "iter : 67400  lr : 0.00482   loss : 0.38041  time : 24.1136 \n",
            "iter : 67500  lr : 0.00481   loss : 0.39275  time : 24.1057 \n",
            "iter : 67600  lr : 0.00480   loss : 0.42592  time : 24.1027 \n",
            "iter : 67700  lr : 0.00479   loss : 0.38822  time : 24.0938 \n",
            "iter : 67800  lr : 0.00478   loss : 0.56520  time : 24.1039 \n",
            "iter : 67900  lr : 0.00478   loss : 0.50737  time : 24.1080 \n",
            "iter : 68000  lr : 0.00477   loss : 0.39768  time : 24.0937 \n",
            "iter : 68100  lr : 0.00476   loss : 0.54129  time : 24.1300 \n",
            "iter : 68200  lr : 0.00475   loss : 0.57803  time : 24.1109 \n",
            "iter : 68300  lr : 0.00475   loss : 0.39525  time : 24.0916 \n",
            "iter : 68400  lr : 0.00474   loss : 0.36866  time : 24.0684 \n",
            "iter : 68500  lr : 0.00473   loss : 0.58283  time : 24.0821 \n",
            "iter : 68600  lr : 0.00472   loss : 0.30787  time : 24.0630 \n",
            "iter : 68700  lr : 0.00472   loss : 0.45601  time : 24.1380 \n",
            "iter : 68800  lr : 0.00471   loss : 0.42956  time : 24.1130 \n",
            "iter : 68900  lr : 0.00470   loss : 0.41290  time : 24.1364 \n",
            "iter : 69000  lr : 0.00469   loss : 0.45004  time : 24.0898 \n",
            "iter : 69100  lr : 0.00468   loss : 0.39079  time : 24.0815 \n",
            "iter : 69200  lr : 0.00468   loss : 0.38946  time : 24.0991 \n",
            "iter : 69300  lr : 0.00467   loss : 0.26962  time : 24.0989 \n",
            "iter : 69400  lr : 0.00466   loss : 0.36926  time : 24.0911 \n",
            "iter : 69500  lr : 0.00465   loss : 0.55984  time : 24.1075 \n",
            "iter : 69600  lr : 0.00465   loss : 0.53046  time : 24.1179 \n",
            "iter : 69700  lr : 0.00464   loss : 0.48123  time : 24.0962 \n",
            "iter : 69800  lr : 0.00463   loss : 0.49389  time : 24.1128 \n",
            "iter : 69900  lr : 0.00462   loss : 0.37495  time : 24.0832 \n",
            "iter : 70000  lr : 0.00462   loss : 0.41533  time : 24.0463 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 69.690000    top5 : 88.360000 \n",
            "\n",
            "iter : 70100  lr : 0.00461   loss : 0.58603  time : 151.5630 \n",
            "iter : 70200  lr : 0.00460   loss : 0.36006  time : 24.1420 \n",
            "iter : 70300  lr : 0.00459   loss : 0.30155  time : 24.1114 \n",
            "iter : 70400  lr : 0.00458   loss : 0.40734  time : 24.0974 \n",
            "iter : 70500  lr : 0.00458   loss : 0.37205  time : 24.1057 \n",
            "iter : 70600  lr : 0.00457   loss : 0.39800  time : 24.0857 \n",
            "iter : 70700  lr : 0.00456   loss : 0.29625  time : 24.0937 \n",
            "iter : 70800  lr : 0.00455   loss : 0.44539  time : 24.0550 \n",
            "iter : 70900  lr : 0.00455   loss : 0.40700  time : 24.0613 \n",
            "iter : 71000  lr : 0.00454   loss : 0.43180  time : 24.0982 \n",
            "iter : 71100  lr : 0.00453   loss : 0.34615  time : 24.1108 \n",
            "iter : 71200  lr : 0.00452   loss : 0.33889  time : 24.1146 \n",
            "iter : 71300  lr : 0.00452   loss : 0.38147  time : 24.1199 \n",
            "iter : 71400  lr : 0.00451   loss : 0.36721  time : 24.0998 \n",
            "iter : 71500  lr : 0.00450   loss : 0.30308  time : 24.0922 \n",
            "iter : 71600  lr : 0.00449   loss : 0.27261  time : 24.0937 \n",
            "iter : 71700  lr : 0.00448   loss : 0.47431  time : 24.1002 \n",
            "iter : 71800  lr : 0.00448   loss : 0.26117  time : 24.0886 \n",
            "iter : 71900  lr : 0.00447   loss : 0.45015  time : 24.0869 \n",
            "iter : 72000  lr : 0.00446   loss : 0.29432  time : 24.1124 \n",
            "iter : 72100  lr : 0.00445   loss : 0.28517  time : 24.1086 \n",
            "iter : 72200  lr : 0.00445   loss : 0.27908  time : 24.0764 \n",
            "iter : 72300  lr : 0.00444   loss : 0.43969  time : 24.0549 \n",
            "iter : 72400  lr : 0.00443   loss : 0.34041  time : 24.0818 \n",
            "iter : 72500  lr : 0.00442   loss : 0.43441  time : 24.1220 \n",
            "iter : 72600  lr : 0.00442   loss : 0.35197  time : 24.1148 \n",
            "iter : 72700  lr : 0.00441   loss : 0.24144  time : 24.1140 \n",
            "iter : 72800  lr : 0.00440   loss : 0.31968  time : 24.0970 \n",
            "iter : 72900  lr : 0.00439   loss : 0.30331  time : 24.1066 \n",
            "iter : 73000  lr : 0.00438   loss : 0.37735  time : 24.1163 \n",
            "iter : 73100  lr : 0.00438   loss : 0.20071  time : 24.0956 \n",
            "iter : 73200  lr : 0.00437   loss : 0.29593  time : 24.1055 \n",
            "iter : 73300  lr : 0.00436   loss : 0.30130  time : 24.1033 \n",
            "iter : 73400  lr : 0.00435   loss : 0.31003  time : 24.1018 \n",
            "iter : 73500  lr : 0.00435   loss : 0.23911  time : 24.1143 \n",
            "iter : 73600  lr : 0.00434   loss : 0.35479  time : 24.1077 \n",
            "iter : 73700  lr : 0.00433   loss : 0.33770  time : 24.0833 \n",
            "iter : 73800  lr : 0.00432   loss : 0.30721  time : 24.0752 \n",
            "iter : 73900  lr : 0.00432   loss : 0.45183  time : 24.1263 \n",
            "iter : 74000  lr : 0.00431   loss : 0.34966  time : 24.1080 \n",
            "iter : 74100  lr : 0.00430   loss : 0.30923  time : 24.1278 \n",
            "iter : 74200  lr : 0.00429   loss : 0.30875  time : 24.0984 \n",
            "iter : 74300  lr : 0.00428   loss : 0.43018  time : 24.0994 \n",
            "iter : 74400  lr : 0.00428   loss : 0.32529  time : 24.1099 \n",
            "iter : 74500  lr : 0.00427   loss : 0.36829  time : 24.1055 \n",
            "iter : 74600  lr : 0.00426   loss : 0.48000  time : 24.0962 \n",
            "iter : 74700  lr : 0.00425   loss : 0.25311  time : 24.0937 \n",
            "iter : 74800  lr : 0.00425   loss : 0.28737  time : 24.0996 \n",
            "iter : 74900  lr : 0.00424   loss : 0.18675  time : 24.0961 \n",
            "iter : 75000  lr : 0.00423   loss : 0.42295  time : 24.1128 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 70.200000    top5 : 88.730000 \n",
            "\n",
            "iter : 75100  lr : 0.00422   loss : 0.31818  time : 151.7525 \n",
            "iter : 75200  lr : 0.00422   loss : 0.39448  time : 24.1386 \n",
            "iter : 75300  lr : 0.00421   loss : 0.30073  time : 24.0943 \n",
            "iter : 75400  lr : 0.00420   loss : 0.30643  time : 24.1010 \n",
            "iter : 75500  lr : 0.00419   loss : 0.18085  time : 24.1098 \n",
            "iter : 75600  lr : 0.00418   loss : 0.42132  time : 24.1099 \n",
            "iter : 75700  lr : 0.00418   loss : 0.34001  time : 24.1111 \n",
            "iter : 75800  lr : 0.00417   loss : 0.29532  time : 24.1020 \n",
            "iter : 75900  lr : 0.00416   loss : 0.37596  time : 24.0981 \n",
            "iter : 76000  lr : 0.00415   loss : 0.21625  time : 24.0890 \n",
            "iter : 76100  lr : 0.00415   loss : 0.28482  time : 24.1107 \n",
            "iter : 76200  lr : 0.00414   loss : 0.30158  time : 24.1324 \n",
            "iter : 76300  lr : 0.00413   loss : 0.32160  time : 24.1017 \n",
            "iter : 76400  lr : 0.00412   loss : 0.31712  time : 24.1088 \n",
            "iter : 76500  lr : 0.00412   loss : 0.31507  time : 24.0873 \n",
            "iter : 76600  lr : 0.00411   loss : 0.27757  time : 24.0949 \n",
            "iter : 76700  lr : 0.00410   loss : 0.27819  time : 24.1044 \n",
            "iter : 76800  lr : 0.00409   loss : 0.36272  time : 24.1106 \n",
            "iter : 76900  lr : 0.00408   loss : 0.22496  time : 24.0973 \n",
            "iter : 77000  lr : 0.00408   loss : 0.24384  time : 24.1049 \n",
            "iter : 77100  lr : 0.00407   loss : 0.40719  time : 24.1085 \n",
            "iter : 77200  lr : 0.00406   loss : 0.26394  time : 24.0894 \n",
            "iter : 77300  lr : 0.00405   loss : 0.31242  time : 24.0845 \n",
            "iter : 77400  lr : 0.00405   loss : 0.25938  time : 24.0839 \n",
            "iter : 77500  lr : 0.00404   loss : 0.26467  time : 24.1190 \n",
            "iter : 77600  lr : 0.00403   loss : 0.27963  time : 24.1116 \n",
            "iter : 77700  lr : 0.00402   loss : 0.38808  time : 24.0990 \n",
            "iter : 77800  lr : 0.00402   loss : 0.21796  time : 24.1125 \n",
            "iter : 77900  lr : 0.00401   loss : 0.24123  time : 24.1053 \n",
            "iter : 78000  lr : 0.00400   loss : 0.32428  time : 24.1064 \n",
            "iter : 78100  lr : 0.00399   loss : 0.22030  time : 24.0992 \n",
            "iter : 78200  lr : 0.00398   loss : 0.25427  time : 24.0963 \n",
            "iter : 78300  lr : 0.00398   loss : 0.30118  time : 24.0972 \n",
            "iter : 78400  lr : 0.00397   loss : 0.20570  time : 24.1076 \n",
            "iter : 78500  lr : 0.00396   loss : 0.22310  time : 24.1117 \n",
            "iter : 78600  lr : 0.00395   loss : 0.17714  time : 24.1134 \n",
            "iter : 78700  lr : 0.00395   loss : 0.36483  time : 24.1046 \n",
            "iter : 78800  lr : 0.00394   loss : 0.16647  time : 24.1268 \n",
            "iter : 78900  lr : 0.00393   loss : 0.24810  time : 24.1105 \n",
            "iter : 79000  lr : 0.00392   loss : 0.19527  time : 24.0903 \n",
            "iter : 79100  lr : 0.00392   loss : 0.27140  time : 24.0977 \n",
            "iter : 79200  lr : 0.00391   loss : 0.32101  time : 24.1163 \n",
            "iter : 79300  lr : 0.00390   loss : 0.40481  time : 24.0906 \n",
            "iter : 79400  lr : 0.00389   loss : 0.28395  time : 24.1044 \n",
            "iter : 79500  lr : 0.00388   loss : 0.30313  time : 24.1015 \n",
            "iter : 79600  lr : 0.00388   loss : 0.32378  time : 24.0906 \n",
            "iter : 79700  lr : 0.00387   loss : 0.18731  time : 24.1083 \n",
            "iter : 79800  lr : 0.00386   loss : 0.23106  time : 24.1031 \n",
            "iter : 79900  lr : 0.00385   loss : 0.27123  time : 24.0901 \n",
            "iter : 80000  lr : 0.00385   loss : 0.30359  time : 24.0685 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 71.000000    top5 : 89.370000 \n",
            "\n",
            "iter : 80100  lr : 0.00384   loss : 0.20523  time : 151.7312 \n",
            "iter : 80200  lr : 0.00383   loss : 0.23699  time : 24.1234 \n",
            "iter : 80300  lr : 0.00382   loss : 0.21664  time : 24.1221 \n",
            "iter : 80400  lr : 0.00382   loss : 0.29393  time : 24.0904 \n",
            "iter : 80500  lr : 0.00381   loss : 0.23184  time : 24.1061 \n",
            "iter : 80600  lr : 0.00380   loss : 0.25780  time : 24.0967 \n",
            "iter : 80700  lr : 0.00379   loss : 0.16262  time : 24.1052 \n",
            "iter : 80800  lr : 0.00378   loss : 0.22260  time : 24.0818 \n",
            "iter : 80900  lr : 0.00378   loss : 0.26316  time : 24.1154 \n",
            "iter : 81000  lr : 0.00377   loss : 0.30178  time : 24.1254 \n",
            "iter : 81100  lr : 0.00376   loss : 0.33092  time : 24.1268 \n",
            "iter : 81200  lr : 0.00375   loss : 0.18069  time : 24.1078 \n",
            "iter : 81300  lr : 0.00375   loss : 0.24234  time : 24.1018 \n",
            "iter : 81400  lr : 0.00374   loss : 0.29522  time : 24.1084 \n",
            "iter : 81500  lr : 0.00373   loss : 0.28593  time : 24.1099 \n",
            "iter : 81600  lr : 0.00372   loss : 0.24287  time : 24.1102 \n",
            "iter : 81700  lr : 0.00372   loss : 0.16913  time : 24.1073 \n",
            "iter : 81800  lr : 0.00371   loss : 0.19828  time : 24.1100 \n",
            "iter : 81900  lr : 0.00370   loss : 0.23147  time : 24.0977 \n",
            "iter : 82000  lr : 0.00369   loss : 0.26729  time : 24.1097 \n",
            "iter : 82100  lr : 0.00368   loss : 0.23587  time : 24.0566 \n",
            "iter : 82200  lr : 0.00368   loss : 0.27457  time : 24.0523 \n",
            "iter : 82300  lr : 0.00367   loss : 0.31731  time : 24.1027 \n",
            "iter : 82400  lr : 0.00366   loss : 0.23196  time : 24.1038 \n",
            "iter : 82500  lr : 0.00365   loss : 0.20827  time : 24.1127 \n",
            "iter : 82600  lr : 0.00365   loss : 0.15005  time : 24.0831 \n",
            "iter : 82700  lr : 0.00364   loss : 0.16259  time : 24.1022 \n",
            "iter : 82800  lr : 0.00363   loss : 0.13468  time : 24.1126 \n",
            "iter : 82900  lr : 0.00362   loss : 0.33887  time : 24.1030 \n",
            "iter : 83000  lr : 0.00362   loss : 0.21290  time : 24.0982 \n",
            "iter : 83100  lr : 0.00361   loss : 0.29616  time : 24.1016 \n",
            "iter : 83200  lr : 0.00360   loss : 0.18861  time : 24.1051 \n",
            "iter : 83300  lr : 0.00359   loss : 0.18503  time : 24.0869 \n",
            "iter : 83400  lr : 0.00358   loss : 0.25246  time : 24.1191 \n",
            "iter : 83500  lr : 0.00358   loss : 0.24379  time : 24.0768 \n",
            "iter : 83600  lr : 0.00357   loss : 0.17100  time : 24.0620 \n",
            "iter : 83700  lr : 0.00356   loss : 0.17507  time : 24.1093 \n",
            "iter : 83800  lr : 0.00355   loss : 0.19694  time : 24.1214 \n",
            "iter : 83900  lr : 0.00355   loss : 0.21494  time : 24.1196 \n",
            "iter : 84000  lr : 0.00354   loss : 0.34805  time : 24.1038 \n",
            "iter : 84100  lr : 0.00353   loss : 0.14829  time : 24.0988 \n",
            "iter : 84200  lr : 0.00352   loss : 0.19720  time : 24.1059 \n",
            "iter : 84300  lr : 0.00352   loss : 0.16318  time : 24.1081 \n",
            "iter : 84400  lr : 0.00351   loss : 0.20675  time : 24.1122 \n",
            "iter : 84500  lr : 0.00350   loss : 0.25669  time : 24.0950 \n",
            "iter : 84600  lr : 0.00349   loss : 0.14039  time : 24.0973 \n",
            "iter : 84700  lr : 0.00348   loss : 0.20399  time : 24.0971 \n",
            "iter : 84800  lr : 0.00348   loss : 0.21987  time : 24.0959 \n",
            "iter : 84900  lr : 0.00347   loss : 0.26469  time : 24.0843 \n",
            "iter : 85000  lr : 0.00346   loss : 0.15218  time : 24.0615 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 71.220000    top5 : 89.400000 \n",
            "\n",
            "iter : 85100  lr : 0.00345   loss : 0.17857  time : 151.6986 \n",
            "iter : 85200  lr : 0.00345   loss : 0.18286  time : 24.0999 \n",
            "iter : 85300  lr : 0.00344   loss : 0.34140  time : 24.1043 \n",
            "iter : 85400  lr : 0.00343   loss : 0.16480  time : 24.1088 \n",
            "iter : 85500  lr : 0.00342   loss : 0.13977  time : 24.0926 \n",
            "iter : 85600  lr : 0.00342   loss : 0.23724  time : 24.1181 \n",
            "iter : 85700  lr : 0.00341   loss : 0.15751  time : 24.1010 \n",
            "iter : 85800  lr : 0.00340   loss : 0.17978  time : 24.0725 \n",
            "iter : 85900  lr : 0.00339   loss : 0.29941  time : 24.0871 \n",
            "iter : 86000  lr : 0.00338   loss : 0.12336  time : 24.1106 \n",
            "iter : 86100  lr : 0.00338   loss : 0.14407  time : 24.1350 \n",
            "iter : 86200  lr : 0.00337   loss : 0.15454  time : 24.1174 \n",
            "iter : 86300  lr : 0.00336   loss : 0.19348  time : 24.1099 \n",
            "iter : 86400  lr : 0.00335   loss : 0.17643  time : 24.0991 \n",
            "iter : 86500  lr : 0.00335   loss : 0.22732  time : 24.1035 \n",
            "iter : 86600  lr : 0.00334   loss : 0.18792  time : 24.1078 \n",
            "iter : 86700  lr : 0.00333   loss : 0.22620  time : 24.0981 \n",
            "iter : 86800  lr : 0.00332   loss : 0.14247  time : 24.1162 \n",
            "iter : 86900  lr : 0.00332   loss : 0.12788  time : 24.1188 \n",
            "iter : 87000  lr : 0.00331   loss : 0.15471  time : 24.0985 \n",
            "iter : 87100  lr : 0.00330   loss : 0.20670  time : 24.0873 \n",
            "iter : 87200  lr : 0.00329   loss : 0.13690  time : 24.0896 \n",
            "iter : 87300  lr : 0.00328   loss : 0.29593  time : 24.1357 \n",
            "iter : 87400  lr : 0.00328   loss : 0.17966  time : 24.1121 \n",
            "iter : 87500  lr : 0.00327   loss : 0.20573  time : 24.1129 \n",
            "iter : 87600  lr : 0.00326   loss : 0.09728  time : 24.0943 \n",
            "iter : 87700  lr : 0.00325   loss : 0.19222  time : 24.1043 \n",
            "iter : 87800  lr : 0.00325   loss : 0.23431  time : 24.1032 \n",
            "iter : 87900  lr : 0.00324   loss : 0.11853  time : 24.1034 \n",
            "iter : 88000  lr : 0.00323   loss : 0.20605  time : 24.0943 \n",
            "iter : 88100  lr : 0.00322   loss : 0.07572  time : 24.0979 \n",
            "iter : 88200  lr : 0.00322   loss : 0.16883  time : 24.1016 \n",
            "iter : 88300  lr : 0.00321   loss : 0.17897  time : 24.0859 \n",
            "iter : 88400  lr : 0.00320   loss : 0.15750  time : 24.0896 \n",
            "iter : 88500  lr : 0.00319   loss : 0.10405  time : 24.1221 \n",
            "iter : 88600  lr : 0.00318   loss : 0.17639  time : 24.1230 \n",
            "iter : 88700  lr : 0.00318   loss : 0.20152  time : 24.1074 \n",
            "iter : 88800  lr : 0.00317   loss : 0.15524  time : 24.0977 \n",
            "iter : 88900  lr : 0.00316   loss : 0.07770  time : 24.1124 \n",
            "iter : 89000  lr : 0.00315   loss : 0.16373  time : 24.1063 \n",
            "iter : 89100  lr : 0.00315   loss : 0.24605  time : 24.0996 \n",
            "iter : 89200  lr : 0.00314   loss : 0.12189  time : 24.1005 \n",
            "iter : 89300  lr : 0.00313   loss : 0.14520  time : 24.1127 \n",
            "iter : 89400  lr : 0.00312   loss : 0.18351  time : 24.1007 \n",
            "iter : 89500  lr : 0.00312   loss : 0.21080  time : 24.1033 \n",
            "iter : 89600  lr : 0.00311   loss : 0.24579  time : 24.1055 \n",
            "iter : 89700  lr : 0.00310   loss : 0.13164  time : 24.0689 \n",
            "iter : 89800  lr : 0.00309   loss : 0.20378  time : 24.1261 \n",
            "iter : 89900  lr : 0.00308   loss : 0.16980  time : 24.1096 \n",
            "iter : 90000  lr : 0.00308   loss : 0.18534  time : 24.0824 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 71.630000    top5 : 89.890000 \n",
            "\n",
            "iter : 90100  lr : 0.00307   loss : 0.13574  time : 151.4159 \n",
            "iter : 90200  lr : 0.00306   loss : 0.22259  time : 24.1473 \n",
            "iter : 90300  lr : 0.00305   loss : 0.17640  time : 24.1056 \n",
            "iter : 90400  lr : 0.00305   loss : 0.13906  time : 24.0978 \n",
            "iter : 90500  lr : 0.00304   loss : 0.10437  time : 24.1131 \n",
            "iter : 90600  lr : 0.00303   loss : 0.13293  time : 24.1276 \n",
            "iter : 90700  lr : 0.00302   loss : 0.07891  time : 24.1052 \n",
            "iter : 90800  lr : 0.00302   loss : 0.15615  time : 24.0995 \n",
            "iter : 90900  lr : 0.00301   loss : 0.14734  time : 24.1031 \n",
            "iter : 91000  lr : 0.00300   loss : 0.18036  time : 24.1071 \n",
            "iter : 91100  lr : 0.00299   loss : 0.10394  time : 24.1057 \n",
            "iter : 91200  lr : 0.00298   loss : 0.13935  time : 24.1054 \n",
            "iter : 91300  lr : 0.00298   loss : 0.18232  time : 24.0939 \n",
            "iter : 91400  lr : 0.00297   loss : 0.13642  time : 24.1166 \n",
            "iter : 91500  lr : 0.00296   loss : 0.12709  time : 24.1022 \n",
            "iter : 91600  lr : 0.00295   loss : 0.13432  time : 24.1048 \n",
            "iter : 91700  lr : 0.00295   loss : 0.16899  time : 24.0968 \n",
            "iter : 91800  lr : 0.00294   loss : 0.13026  time : 24.1196 \n",
            "iter : 91900  lr : 0.00293   loss : 0.08499  time : 24.1187 \n",
            "iter : 92000  lr : 0.00292   loss : 0.12619  time : 24.1097 \n",
            "iter : 92100  lr : 0.00292   loss : 0.12717  time : 24.1061 \n",
            "iter : 92200  lr : 0.00291   loss : 0.07207  time : 24.1001 \n",
            "iter : 92300  lr : 0.00290   loss : 0.16860  time : 24.0954 \n",
            "iter : 92400  lr : 0.00289   loss : 0.12703  time : 24.1073 \n",
            "iter : 92500  lr : 0.00288   loss : 0.18399  time : 24.1038 \n",
            "iter : 92600  lr : 0.00288   loss : 0.11249  time : 24.1231 \n",
            "iter : 92700  lr : 0.00287   loss : 0.13232  time : 24.1171 \n",
            "iter : 92800  lr : 0.00286   loss : 0.17020  time : 24.1009 \n",
            "iter : 92900  lr : 0.00285   loss : 0.13478  time : 24.1189 \n",
            "iter : 93000  lr : 0.00285   loss : 0.09307  time : 24.0965 \n",
            "iter : 93100  lr : 0.00284   loss : 0.05825  time : 24.1127 \n",
            "iter : 93200  lr : 0.00283   loss : 0.09835  time : 24.1097 \n",
            "iter : 93300  lr : 0.00282   loss : 0.07585  time : 24.1296 \n",
            "iter : 93400  lr : 0.00282   loss : 0.10853  time : 24.1145 \n",
            "iter : 93500  lr : 0.00281   loss : 0.09788  time : 24.1106 \n",
            "iter : 93600  lr : 0.00280   loss : 0.10346  time : 24.0996 \n",
            "iter : 93700  lr : 0.00279   loss : 0.10515  time : 24.0917 \n",
            "iter : 93800  lr : 0.00278   loss : 0.10895  time : 24.1095 \n",
            "iter : 93900  lr : 0.00278   loss : 0.13941  time : 24.0968 \n",
            "iter : 94000  lr : 0.00277   loss : 0.09995  time : 24.1078 \n",
            "iter : 94100  lr : 0.00276   loss : 0.13204  time : 24.1103 \n",
            "iter : 94200  lr : 0.00275   loss : 0.10361  time : 24.0975 \n",
            "iter : 94300  lr : 0.00275   loss : 0.08137  time : 24.0855 \n",
            "iter : 94400  lr : 0.00274   loss : 0.12100  time : 24.0914 \n",
            "iter : 94500  lr : 0.00273   loss : 0.14888  time : 24.1224 \n",
            "iter : 94600  lr : 0.00272   loss : 0.10593  time : 24.1192 \n",
            "iter : 94700  lr : 0.00272   loss : 0.09503  time : 24.1068 \n",
            "iter : 94800  lr : 0.00271   loss : 0.20972  time : 24.1016 \n",
            "iter : 94900  lr : 0.00270   loss : 0.08886  time : 24.1188 \n",
            "iter : 95000  lr : 0.00269   loss : 0.10557  time : 24.1103 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 72.670000    top5 : 89.940000 \n",
            "\n",
            "iter : 95100  lr : 0.00268   loss : 0.09743  time : 151.7380 \n",
            "iter : 95200  lr : 0.00268   loss : 0.09115  time : 24.1635 \n",
            "iter : 95300  lr : 0.00267   loss : 0.07863  time : 24.1252 \n",
            "iter : 95400  lr : 0.00266   loss : 0.07989  time : 24.0970 \n",
            "iter : 95500  lr : 0.00265   loss : 0.17940  time : 24.0856 \n",
            "iter : 95600  lr : 0.00265   loss : 0.10471  time : 24.0994 \n",
            "iter : 95700  lr : 0.00264   loss : 0.08395  time : 24.1014 \n",
            "iter : 95800  lr : 0.00263   loss : 0.06799  time : 24.1063 \n",
            "iter : 95900  lr : 0.00262   loss : 0.12087  time : 24.1158 \n",
            "iter : 96000  lr : 0.00262   loss : 0.08585  time : 24.1145 \n",
            "iter : 96100  lr : 0.00261   loss : 0.13657  time : 24.1357 \n",
            "iter : 96200  lr : 0.00260   loss : 0.08871  time : 24.1016 \n",
            "iter : 96300  lr : 0.00259   loss : 0.08702  time : 24.0933 \n",
            "iter : 96400  lr : 0.00258   loss : 0.06957  time : 24.1196 \n",
            "iter : 96500  lr : 0.00258   loss : 0.10342  time : 24.1375 \n",
            "iter : 96600  lr : 0.00257   loss : 0.09906  time : 24.1142 \n",
            "iter : 96700  lr : 0.00256   loss : 0.10388  time : 24.0991 \n",
            "iter : 96800  lr : 0.00255   loss : 0.11744  time : 24.1044 \n",
            "iter : 96900  lr : 0.00255   loss : 0.10877  time : 24.0905 \n",
            "iter : 97000  lr : 0.00254   loss : 0.09903  time : 24.1043 \n",
            "iter : 97100  lr : 0.00253   loss : 0.09041  time : 24.1120 \n",
            "iter : 97200  lr : 0.00252   loss : 0.08725  time : 24.0995 \n",
            "iter : 97300  lr : 0.00252   loss : 0.07238  time : 24.1064 \n",
            "iter : 97400  lr : 0.00251   loss : 0.06965  time : 24.1103 \n",
            "iter : 97500  lr : 0.00250   loss : 0.05990  time : 24.0800 \n",
            "iter : 97600  lr : 0.00249   loss : 0.08437  time : 24.0720 \n",
            "iter : 97700  lr : 0.00248   loss : 0.08951  time : 24.1272 \n",
            "iter : 97800  lr : 0.00248   loss : 0.15295  time : 24.1290 \n",
            "iter : 97900  lr : 0.00247   loss : 0.07741  time : 24.1097 \n",
            "iter : 98000  lr : 0.00246   loss : 0.12597  time : 24.0991 \n",
            "iter : 98100  lr : 0.00245   loss : 0.09717  time : 24.0997 \n",
            "iter : 98200  lr : 0.00245   loss : 0.08942  time : 24.1167 \n",
            "iter : 98300  lr : 0.00244   loss : 0.11845  time : 24.1075 \n",
            "iter : 98400  lr : 0.00243   loss : 0.10796  time : 24.0875 \n",
            "iter : 98500  lr : 0.00242   loss : 0.14393  time : 24.1015 \n",
            "iter : 98600  lr : 0.00242   loss : 0.04924  time : 24.1085 \n",
            "iter : 98700  lr : 0.00241   loss : 0.05632  time : 24.0964 \n",
            "iter : 98800  lr : 0.00240   loss : 0.06029  time : 24.1125 \n",
            "iter : 98900  lr : 0.00239   loss : 0.05807  time : 24.0875 \n",
            "iter : 99000  lr : 0.00238   loss : 0.05755  time : 24.0682 \n",
            "iter : 99100  lr : 0.00238   loss : 0.09514  time : 24.1169 \n",
            "iter : 99200  lr : 0.00237   loss : 0.15868  time : 24.1429 \n",
            "iter : 99300  lr : 0.00236   loss : 0.08955  time : 24.1028 \n",
            "iter : 99400  lr : 0.00235   loss : 0.05570  time : 24.1116 \n",
            "iter : 99500  lr : 0.00235   loss : 0.07228  time : 24.1121 \n",
            "iter : 99600  lr : 0.00234   loss : 0.10319  time : 24.1002 \n",
            "iter : 99700  lr : 0.00233   loss : 0.09264  time : 24.1017 \n",
            "iter : 99800  lr : 0.00232   loss : 0.03871  time : 24.0832 \n",
            "iter : 99900  lr : 0.00232   loss : 0.07977  time : 24.0945 \n",
            "iter : 100000  lr : 0.00231   loss : 0.10135  time : 24.1068 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 73.470000    top5 : 90.300000 \n",
            "\n",
            "iter : 100100  lr : 0.00230   loss : 0.07353  time : 151.9303 \n",
            "iter : 100200  lr : 0.00229   loss : 0.08478  time : 24.1361 \n",
            "iter : 100300  lr : 0.00228   loss : 0.06727  time : 24.1049 \n",
            "iter : 100400  lr : 0.00228   loss : 0.08185  time : 24.0878 \n",
            "iter : 100500  lr : 0.00227   loss : 0.11696  time : 24.0974 \n",
            "iter : 100600  lr : 0.00226   loss : 0.04594  time : 24.1046 \n",
            "iter : 100700  lr : 0.00225   loss : 0.09160  time : 24.1052 \n",
            "iter : 100800  lr : 0.00225   loss : 0.04793  time : 24.0940 \n",
            "iter : 100900  lr : 0.00224   loss : 0.11697  time : 24.1163 \n",
            "iter : 101000  lr : 0.00223   loss : 0.10063  time : 24.0699 \n",
            "iter : 101100  lr : 0.00222   loss : 0.10808  time : 24.0769 \n",
            "iter : 101200  lr : 0.00222   loss : 0.16256  time : 24.1265 \n",
            "iter : 101300  lr : 0.00221   loss : 0.07043  time : 24.1153 \n",
            "iter : 101400  lr : 0.00220   loss : 0.05489  time : 24.1237 \n",
            "iter : 101500  lr : 0.00219   loss : 0.06395  time : 24.1058 \n",
            "iter : 101600  lr : 0.00218   loss : 0.05602  time : 24.1261 \n",
            "iter : 101700  lr : 0.00218   loss : 0.09314  time : 24.1134 \n",
            "iter : 101800  lr : 0.00217   loss : 0.05095  time : 24.1002 \n",
            "iter : 101900  lr : 0.00216   loss : 0.03765  time : 24.1018 \n",
            "iter : 102000  lr : 0.00215   loss : 0.05353  time : 24.1031 \n",
            "iter : 102100  lr : 0.00215   loss : 0.07282  time : 24.1015 \n",
            "iter : 102200  lr : 0.00214   loss : 0.06771  time : 24.1165 \n",
            "iter : 102300  lr : 0.00213   loss : 0.02290  time : 24.1215 \n",
            "iter : 102400  lr : 0.00212   loss : 0.10222  time : 24.0849 \n",
            "iter : 102500  lr : 0.00212   loss : 0.04068  time : 24.0926 \n",
            "iter : 102600  lr : 0.00211   loss : 0.06811  time : 24.1429 \n",
            "iter : 102700  lr : 0.00210   loss : 0.11382  time : 24.1068 \n",
            "iter : 102800  lr : 0.00209   loss : 0.05346  time : 24.1083 \n",
            "iter : 102900  lr : 0.00208   loss : 0.08100  time : 24.1026 \n",
            "iter : 103000  lr : 0.00208   loss : 0.09301  time : 24.0951 \n",
            "iter : 103100  lr : 0.00207   loss : 0.05437  time : 24.0999 \n",
            "iter : 103200  lr : 0.00206   loss : 0.06122  time : 24.0903 \n",
            "iter : 103300  lr : 0.00205   loss : 0.05268  time : 24.1052 \n",
            "iter : 103400  lr : 0.00205   loss : 0.04698  time : 24.0933 \n",
            "iter : 103500  lr : 0.00204   loss : 0.07287  time : 24.1187 \n",
            "iter : 103600  lr : 0.00203   loss : 0.06032  time : 24.0830 \n",
            "iter : 103700  lr : 0.00202   loss : 0.03748  time : 24.0674 \n",
            "iter : 103800  lr : 0.00202   loss : 0.08316  time : 24.1164 \n",
            "iter : 103900  lr : 0.00201   loss : 0.02083  time : 24.1178 \n",
            "iter : 104000  lr : 0.00200   loss : 0.07142  time : 24.0929 \n",
            "iter : 104100  lr : 0.00199   loss : 0.07102  time : 24.0986 \n",
            "iter : 104200  lr : 0.00198   loss : 0.04691  time : 24.1019 \n",
            "iter : 104300  lr : 0.00198   loss : 0.04311  time : 24.0856 \n",
            "iter : 104400  lr : 0.00197   loss : 0.05571  time : 24.0966 \n",
            "iter : 104500  lr : 0.00196   loss : 0.11869  time : 24.0856 \n",
            "iter : 104600  lr : 0.00195   loss : 0.08470  time : 24.0973 \n",
            "iter : 104700  lr : 0.00195   loss : 0.03894  time : 24.1036 \n",
            "iter : 104800  lr : 0.00194   loss : 0.04591  time : 24.0832 \n",
            "iter : 104900  lr : 0.00193   loss : 0.04177  time : 24.1349 \n",
            "iter : 105000  lr : 0.00192   loss : 0.05895  time : 24.0954 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 73.490000    top5 : 90.320000 \n",
            "\n",
            "iter : 105100  lr : 0.00192   loss : 0.02609  time : 151.5391 \n",
            "iter : 105200  lr : 0.00191   loss : 0.02062  time : 24.1001 \n",
            "iter : 105300  lr : 0.00190   loss : 0.07781  time : 24.1086 \n",
            "iter : 105400  lr : 0.00189   loss : 0.03352  time : 24.0948 \n",
            "iter : 105500  lr : 0.00188   loss : 0.01667  time : 24.0870 \n",
            "iter : 105600  lr : 0.00188   loss : 0.03491  time : 24.1193 \n",
            "iter : 105700  lr : 0.00187   loss : 0.02741  time : 24.0853 \n",
            "iter : 105800  lr : 0.00186   loss : 0.04114  time : 24.0737 \n",
            "iter : 105900  lr : 0.00185   loss : 0.04076  time : 24.1228 \n",
            "iter : 106000  lr : 0.00185   loss : 0.04006  time : 24.1120 \n",
            "iter : 106100  lr : 0.00184   loss : 0.04540  time : 24.1073 \n",
            "iter : 106200  lr : 0.00183   loss : 0.06220  time : 24.1067 \n",
            "iter : 106300  lr : 0.00182   loss : 0.03374  time : 24.0869 \n",
            "iter : 106400  lr : 0.00182   loss : 0.04505  time : 24.0840 \n",
            "iter : 106500  lr : 0.00181   loss : 0.05226  time : 24.0786 \n",
            "iter : 106600  lr : 0.00180   loss : 0.03194  time : 24.0893 \n",
            "iter : 106700  lr : 0.00179   loss : 0.04533  time : 24.0996 \n",
            "iter : 106800  lr : 0.00178   loss : 0.03978  time : 24.0807 \n",
            "iter : 106900  lr : 0.00178   loss : 0.07574  time : 24.1183 \n",
            "iter : 107000  lr : 0.00177   loss : 0.04728  time : 24.1050 \n",
            "iter : 107100  lr : 0.00176   loss : 0.03434  time : 24.0787 \n",
            "iter : 107200  lr : 0.00175   loss : 0.03261  time : 24.0959 \n",
            "iter : 107300  lr : 0.00175   loss : 0.03979  time : 24.1165 \n",
            "iter : 107400  lr : 0.00174   loss : 0.07094  time : 24.0969 \n",
            "iter : 107500  lr : 0.00173   loss : 0.07883  time : 24.0945 \n",
            "iter : 107600  lr : 0.00172   loss : 0.04962  time : 24.0870 \n",
            "iter : 107700  lr : 0.00172   loss : 0.05744  time : 24.0940 \n",
            "iter : 107800  lr : 0.00171   loss : 0.03664  time : 24.1019 \n",
            "iter : 107900  lr : 0.00170   loss : 0.08942  time : 24.0990 \n",
            "iter : 108000  lr : 0.00169   loss : 0.09090  time : 24.0972 \n",
            "iter : 108100  lr : 0.00168   loss : 0.05445  time : 24.0809 \n",
            "iter : 108200  lr : 0.00168   loss : 0.03819  time : 24.1119 \n",
            "iter : 108300  lr : 0.00167   loss : 0.03186  time : 24.0995 \n",
            "iter : 108400  lr : 0.00166   loss : 0.03616  time : 24.1337 \n",
            "iter : 108500  lr : 0.00165   loss : 0.03657  time : 24.0873 \n",
            "iter : 108600  lr : 0.00165   loss : 0.04492  time : 24.1083 \n",
            "iter : 108700  lr : 0.00164   loss : 0.03317  time : 24.0902 \n",
            "iter : 108800  lr : 0.00163   loss : 0.08134  time : 24.1008 \n",
            "iter : 108900  lr : 0.00162   loss : 0.02493  time : 24.0932 \n",
            "iter : 109000  lr : 0.00162   loss : 0.04900  time : 24.0961 \n",
            "iter : 109100  lr : 0.00161   loss : 0.06901  time : 24.0984 \n",
            "iter : 109200  lr : 0.00160   loss : 0.04701  time : 24.1001 \n",
            "iter : 109300  lr : 0.00159   loss : 0.02981  time : 24.0956 \n",
            "iter : 109400  lr : 0.00158   loss : 0.05652  time : 24.1020 \n",
            "iter : 109500  lr : 0.00158   loss : 0.03531  time : 24.1019 \n",
            "iter : 109600  lr : 0.00157   loss : 0.04242  time : 24.0931 \n",
            "iter : 109700  lr : 0.00156   loss : 0.07354  time : 24.1237 \n",
            "iter : 109800  lr : 0.00155   loss : 0.04292  time : 24.1206 \n",
            "iter : 109900  lr : 0.00155   loss : 0.04746  time : 24.1043 \n",
            "iter : 110000  lr : 0.00154   loss : 0.04374  time : 24.1139 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 73.830000    top5 : 90.210000 \n",
            "\n",
            "iter : 110100  lr : 0.00153   loss : 0.04147  time : 151.4710 \n",
            "iter : 110200  lr : 0.00152   loss : 0.02298  time : 24.0894 \n",
            "iter : 110300  lr : 0.00152   loss : 0.02873  time : 24.0924 \n",
            "iter : 110400  lr : 0.00151   loss : 0.03498  time : 24.1183 \n",
            "iter : 110500  lr : 0.00150   loss : 0.04110  time : 24.0967 \n",
            "iter : 110600  lr : 0.00149   loss : 0.02846  time : 24.0867 \n",
            "iter : 110700  lr : 0.00148   loss : 0.02628  time : 24.0968 \n",
            "iter : 110800  lr : 0.00148   loss : 0.08094  time : 24.1069 \n",
            "iter : 110900  lr : 0.00147   loss : 0.02338  time : 24.0996 \n",
            "iter : 111000  lr : 0.00146   loss : 0.02429  time : 24.0936 \n",
            "iter : 111100  lr : 0.00145   loss : 0.03872  time : 24.1037 \n",
            "iter : 111200  lr : 0.00145   loss : 0.04561  time : 24.0961 \n",
            "iter : 111300  lr : 0.00144   loss : 0.02313  time : 24.0989 \n",
            "iter : 111400  lr : 0.00143   loss : 0.04092  time : 24.1025 \n",
            "iter : 111500  lr : 0.00142   loss : 0.03124  time : 24.1098 \n",
            "iter : 111600  lr : 0.00142   loss : 0.03375  time : 24.1019 \n",
            "iter : 111700  lr : 0.00141   loss : 0.08801  time : 24.1013 \n",
            "iter : 111800  lr : 0.00140   loss : 0.03622  time : 24.0994 \n",
            "iter : 111900  lr : 0.00139   loss : 0.04682  time : 24.1005 \n",
            "iter : 112000  lr : 0.00138   loss : 0.01156  time : 24.1195 \n",
            "iter : 112100  lr : 0.00138   loss : 0.02232  time : 24.1218 \n",
            "iter : 112200  lr : 0.00137   loss : 0.06147  time : 24.1050 \n",
            "iter : 112300  lr : 0.00136   loss : 0.03758  time : 24.0999 \n",
            "iter : 112400  lr : 0.00135   loss : 0.01928  time : 24.1124 \n",
            "iter : 112500  lr : 0.00135   loss : 0.03831  time : 24.1195 \n",
            "iter : 112600  lr : 0.00134   loss : 0.03661  time : 24.0885 \n",
            "iter : 112700  lr : 0.00133   loss : 0.02059  time : 24.0994 \n",
            "iter : 112800  lr : 0.00132   loss : 0.03991  time : 24.1135 \n",
            "iter : 112900  lr : 0.00132   loss : 0.08472  time : 24.1209 \n",
            "iter : 113000  lr : 0.00131   loss : 0.06609  time : 24.1027 \n",
            "iter : 113100  lr : 0.00130   loss : 0.03563  time : 24.1032 \n",
            "iter : 113200  lr : 0.00129   loss : 0.04815  time : 24.0983 \n",
            "iter : 113300  lr : 0.00128   loss : 0.03735  time : 24.0916 \n",
            "iter : 113400  lr : 0.00128   loss : 0.03895  time : 24.1097 \n",
            "iter : 113500  lr : 0.00127   loss : 0.04023  time : 24.0958 \n",
            "iter : 113600  lr : 0.00126   loss : 0.03327  time : 24.0925 \n",
            "iter : 113700  lr : 0.00125   loss : 0.02570  time : 24.0881 \n",
            "iter : 113800  lr : 0.00125   loss : 0.02649  time : 24.1048 \n",
            "iter : 113900  lr : 0.00124   loss : 0.02190  time : 24.1147 \n",
            "iter : 114000  lr : 0.00123   loss : 0.03671  time : 24.1186 \n",
            "iter : 114100  lr : 0.00122   loss : 0.04010  time : 24.1111 \n",
            "iter : 114200  lr : 0.00122   loss : 0.03047  time : 24.1290 \n",
            "iter : 114300  lr : 0.00121   loss : 0.02897  time : 24.0996 \n",
            "iter : 114400  lr : 0.00120   loss : 0.04349  time : 24.1112 \n",
            "iter : 114500  lr : 0.00119   loss : 0.03064  time : 24.0950 \n",
            "iter : 114600  lr : 0.00118   loss : 0.04228  time : 24.0914 \n",
            "iter : 114700  lr : 0.00118   loss : 0.02442  time : 24.0980 \n",
            "iter : 114800  lr : 0.00117   loss : 0.04421  time : 24.1004 \n",
            "iter : 114900  lr : 0.00116   loss : 0.02605  time : 24.1113 \n",
            "iter : 115000  lr : 0.00115   loss : 0.03993  time : 24.0978 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 74.200000    top5 : 90.380000 \n",
            "\n",
            "iter : 115100  lr : 0.00115   loss : 0.02439  time : 151.8622 \n",
            "iter : 115200  lr : 0.00114   loss : 0.02011  time : 24.1307 \n",
            "iter : 115300  lr : 0.00113   loss : 0.01894  time : 24.0929 \n",
            "iter : 115400  lr : 0.00112   loss : 0.03325  time : 24.0980 \n",
            "iter : 115500  lr : 0.00112   loss : 0.04405  time : 24.0966 \n",
            "iter : 115600  lr : 0.00111   loss : 0.05274  time : 24.0932 \n",
            "iter : 115700  lr : 0.00110   loss : 0.02961  time : 24.1163 \n",
            "iter : 115800  lr : 0.00109   loss : 0.04047  time : 24.1012 \n",
            "iter : 115900  lr : 0.00108   loss : 0.06015  time : 24.0833 \n",
            "iter : 116000  lr : 0.00108   loss : 0.01676  time : 24.1100 \n",
            "iter : 116100  lr : 0.00107   loss : 0.05518  time : 24.1325 \n",
            "iter : 116200  lr : 0.00106   loss : 0.01862  time : 24.0980 \n",
            "iter : 116300  lr : 0.00105   loss : 0.05005  time : 24.1011 \n",
            "iter : 116400  lr : 0.00105   loss : 0.01437  time : 24.1051 \n",
            "iter : 116500  lr : 0.00104   loss : 0.02474  time : 24.0787 \n",
            "iter : 116600  lr : 0.00103   loss : 0.02671  time : 24.0970 \n",
            "iter : 116700  lr : 0.00102   loss : 0.01849  time : 24.0858 \n",
            "iter : 116800  lr : 0.00102   loss : 0.07125  time : 24.1358 \n",
            "iter : 116900  lr : 0.00101   loss : 0.04726  time : 24.0963 \n",
            "iter : 117000  lr : 0.00100   loss : 0.03050  time : 24.1093 \n",
            "iter : 117100  lr : 0.00099   loss : 0.03835  time : 24.1122 \n",
            "iter : 117200  lr : 0.00098   loss : 0.02073  time : 24.1081 \n",
            "iter : 117300  lr : 0.00098   loss : 0.01900  time : 24.1124 \n",
            "iter : 117400  lr : 0.00097   loss : 0.03334  time : 24.0958 \n",
            "iter : 117500  lr : 0.00096   loss : 0.01779  time : 24.1014 \n",
            "iter : 117600  lr : 0.00095   loss : 0.01187  time : 24.0900 \n",
            "iter : 117700  lr : 0.00095   loss : 0.02319  time : 24.0977 \n",
            "iter : 117800  lr : 0.00094   loss : 0.01840  time : 24.0866 \n",
            "iter : 117900  lr : 0.00093   loss : 0.03143  time : 24.0957 \n",
            "iter : 118000  lr : 0.00092   loss : 0.03280  time : 24.1042 \n",
            "iter : 118100  lr : 0.00092   loss : 0.03588  time : 24.0964 \n",
            "iter : 118200  lr : 0.00091   loss : 0.01769  time : 24.1020 \n",
            "iter : 118300  lr : 0.00090   loss : 0.02434  time : 24.0765 \n",
            "iter : 118400  lr : 0.00089   loss : 0.03319  time : 24.1077 \n",
            "iter : 118500  lr : 0.00088   loss : 0.01265  time : 24.1080 \n",
            "iter : 118600  lr : 0.00088   loss : 0.02678  time : 24.0887 \n",
            "iter : 118700  lr : 0.00087   loss : 0.02158  time : 24.1137 \n",
            "iter : 118800  lr : 0.00086   loss : 0.01894  time : 24.0845 \n",
            "iter : 118900  lr : 0.00085   loss : 0.01961  time : 24.1022 \n",
            "iter : 119000  lr : 0.00085   loss : 0.03537  time : 24.0894 \n",
            "iter : 119100  lr : 0.00084   loss : 0.00981  time : 24.0959 \n",
            "iter : 119200  lr : 0.00083   loss : 0.02709  time : 24.0913 \n",
            "iter : 119300  lr : 0.00082   loss : 0.04284  time : 24.1174 \n",
            "iter : 119400  lr : 0.00082   loss : 0.02799  time : 24.1209 \n",
            "iter : 119500  lr : 0.00081   loss : 0.03614  time : 24.0949 \n",
            "iter : 119600  lr : 0.00080   loss : 0.01278  time : 24.1013 \n",
            "iter : 119700  lr : 0.00079   loss : 0.02022  time : 24.1225 \n",
            "iter : 119800  lr : 0.00078   loss : 0.02237  time : 24.1447 \n",
            "iter : 119900  lr : 0.00078   loss : 0.02450  time : 24.1022 \n",
            "iter : 120000  lr : 0.00077   loss : 0.03032  time : 24.0990 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 74.400000    top5 : 90.430000 \n",
            "\n",
            "iter : 120100  lr : 0.00076   loss : 0.02612  time : 151.6810 \n",
            "iter : 120200  lr : 0.00075   loss : 0.05745  time : 24.1178 \n",
            "iter : 120300  lr : 0.00075   loss : 0.03336  time : 24.1286 \n",
            "iter : 120400  lr : 0.00074   loss : 0.02600  time : 24.0955 \n",
            "iter : 120500  lr : 0.00073   loss : 0.03120  time : 24.1061 \n",
            "iter : 120600  lr : 0.00072   loss : 0.02850  time : 24.1143 \n",
            "iter : 120700  lr : 0.00072   loss : 0.01866  time : 24.1174 \n",
            "iter : 120800  lr : 0.00071   loss : 0.03424  time : 24.0925 \n",
            "iter : 120900  lr : 0.00070   loss : 0.05432  time : 24.1042 \n",
            "iter : 121000  lr : 0.00069   loss : 0.01673  time : 24.1121 \n",
            "iter : 121100  lr : 0.00068   loss : 0.02236  time : 24.0926 \n",
            "iter : 121200  lr : 0.00068   loss : 0.01524  time : 24.1049 \n",
            "iter : 121300  lr : 0.00067   loss : 0.04037  time : 24.1112 \n",
            "iter : 121400  lr : 0.00066   loss : 0.01825  time : 24.1028 \n",
            "iter : 121500  lr : 0.00065   loss : 0.01790  time : 24.0903 \n",
            "iter : 121600  lr : 0.00065   loss : 0.01365  time : 24.0818 \n",
            "iter : 121700  lr : 0.00064   loss : 0.01086  time : 24.1147 \n",
            "iter : 121800  lr : 0.00063   loss : 0.02437  time : 24.0880 \n",
            "iter : 121900  lr : 0.00062   loss : 0.01932  time : 24.0777 \n",
            "iter : 122000  lr : 0.00062   loss : 0.01834  time : 24.1149 \n",
            "iter : 122100  lr : 0.00061   loss : 0.02784  time : 24.0907 \n",
            "iter : 122200  lr : 0.00060   loss : 0.01822  time : 24.1013 \n",
            "iter : 122300  lr : 0.00059   loss : 0.04714  time : 24.1003 \n",
            "iter : 122400  lr : 0.00058   loss : 0.02424  time : 24.0856 \n",
            "iter : 122500  lr : 0.00058   loss : 0.01787  time : 24.0893 \n",
            "iter : 122600  lr : 0.00057   loss : 0.01963  time : 24.0908 \n",
            "iter : 122700  lr : 0.00056   loss : 0.02108  time : 24.1211 \n",
            "iter : 122800  lr : 0.00055   loss : 0.00964  time : 24.0929 \n",
            "iter : 122900  lr : 0.00055   loss : 0.01972  time : 24.0870 \n",
            "iter : 123000  lr : 0.00054   loss : 0.05268  time : 24.0990 \n",
            "iter : 123100  lr : 0.00053   loss : 0.04225  time : 24.1332 \n",
            "iter : 123200  lr : 0.00052   loss : 0.03291  time : 24.0834 \n",
            "iter : 123300  lr : 0.00052   loss : 0.01657  time : 24.0970 \n",
            "iter : 123400  lr : 0.00051   loss : 0.01262  time : 24.1259 \n",
            "iter : 123500  lr : 0.00050   loss : 0.02451  time : 24.1035 \n",
            "iter : 123600  lr : 0.00049   loss : 0.01857  time : 24.1017 \n",
            "iter : 123700  lr : 0.00048   loss : 0.01516  time : 24.1139 \n",
            "iter : 123800  lr : 0.00048   loss : 0.04177  time : 24.1065 \n",
            "iter : 123900  lr : 0.00047   loss : 0.01497  time : 24.0872 \n",
            "iter : 124000  lr : 0.00046   loss : 0.02816  time : 24.1130 \n",
            "iter : 124100  lr : 0.00045   loss : 0.01120  time : 24.1050 \n",
            "iter : 124200  lr : 0.00045   loss : 0.01715  time : 24.0917 \n",
            "iter : 124300  lr : 0.00044   loss : 0.03380  time : 24.1204 \n",
            "iter : 124400  lr : 0.00043   loss : 0.02934  time : 24.0890 \n",
            "iter : 124500  lr : 0.00042   loss : 0.02232  time : 24.1053 \n",
            "iter : 124600  lr : 0.00042   loss : 0.01234  time : 24.1161 \n",
            "iter : 124700  lr : 0.00041   loss : 0.03150  time : 24.1047 \n",
            "iter : 124800  lr : 0.00040   loss : 0.02254  time : 24.0960 \n",
            "iter : 124900  lr : 0.00039   loss : 0.02351  time : 24.0869 \n",
            "iter : 125000  lr : 0.00038   loss : 0.01725  time : 24.0881 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 74.480000    top5 : 90.570000 \n",
            "\n",
            "iter : 125100  lr : 0.00038   loss : 0.01543  time : 151.6444 \n",
            "iter : 125200  lr : 0.00037   loss : 0.01295  time : 24.0908 \n",
            "iter : 125300  lr : 0.00036   loss : 0.01738  time : 24.1324 \n",
            "iter : 125400  lr : 0.00035   loss : 0.02408  time : 24.1034 \n",
            "iter : 125500  lr : 0.00035   loss : 0.02989  time : 24.0878 \n",
            "iter : 125600  lr : 0.00034   loss : 0.01438  time : 24.1044 \n",
            "iter : 125700  lr : 0.00033   loss : 0.01270  time : 24.1016 \n",
            "iter : 125800  lr : 0.00032   loss : 0.01281  time : 24.0895 \n",
            "iter : 125900  lr : 0.00032   loss : 0.02628  time : 24.0864 \n",
            "iter : 126000  lr : 0.00031   loss : 0.02928  time : 24.1017 \n",
            "iter : 126100  lr : 0.00030   loss : 0.01877  time : 24.1118 \n",
            "iter : 126200  lr : 0.00029   loss : 0.01697  time : 24.0949 \n",
            "iter : 126300  lr : 0.00028   loss : 0.01639  time : 24.0749 \n",
            "iter : 126400  lr : 0.00028   loss : 0.01799  time : 24.0776 \n",
            "iter : 126500  lr : 0.00027   loss : 0.02649  time : 24.1242 \n",
            "iter : 126600  lr : 0.00026   loss : 0.02816  time : 24.1137 \n",
            "iter : 126700  lr : 0.00025   loss : 0.02207  time : 24.0974 \n",
            "iter : 126800  lr : 0.00025   loss : 0.02921  time : 24.1000 \n",
            "iter : 126900  lr : 0.00024   loss : 0.02418  time : 24.1028 \n",
            "iter : 127000  lr : 0.00023   loss : 0.04510  time : 24.0920 \n",
            "iter : 127100  lr : 0.00022   loss : 0.02830  time : 24.0715 \n",
            "iter : 127200  lr : 0.00022   loss : 0.01081  time : 24.0936 \n",
            "iter : 127300  lr : 0.00021   loss : 0.01948  time : 24.1137 \n",
            "iter : 127400  lr : 0.00020   loss : 0.01284  time : 24.1044 \n",
            "iter : 127500  lr : 0.00019   loss : 0.01361  time : 24.0904 \n",
            "iter : 127600  lr : 0.00018   loss : 0.02301  time : 24.1052 \n",
            "iter : 127700  lr : 0.00018   loss : 0.02240  time : 24.0867 \n",
            "iter : 127800  lr : 0.00017   loss : 0.02772  time : 24.0613 \n",
            "iter : 127900  lr : 0.00016   loss : 0.02398  time : 24.1205 \n",
            "iter : 128000  lr : 0.00015   loss : 0.02218  time : 24.1035 \n",
            "iter : 128100  lr : 0.00015   loss : 0.03264  time : 24.0948 \n",
            "iter : 128200  lr : 0.00014   loss : 0.01748  time : 24.1060 \n",
            "iter : 128300  lr : 0.00013   loss : 0.01708  time : 24.0939 \n",
            "iter : 128400  lr : 0.00012   loss : 0.02481  time : 24.0863 \n",
            "iter : 128500  lr : 0.00012   loss : 0.02243  time : 24.0775 \n",
            "iter : 128600  lr : 0.00011   loss : 0.01434  time : 24.0890 \n",
            "iter : 128700  lr : 0.00010   loss : 0.01492  time : 24.0976 \n",
            "iter : 128800  lr : 0.00009   loss : 0.03016  time : 24.0942 \n",
            "iter : 128900  lr : 0.00008   loss : 0.02033  time : 24.1055 \n",
            "iter : 129000  lr : 0.00008   loss : 0.02570  time : 24.1069 \n",
            "iter : 129100  lr : 0.00007   loss : 0.01454  time : 24.0782 \n",
            "iter : 129200  lr : 0.00006   loss : 0.02395  time : 24.0755 \n",
            "iter : 129300  lr : 0.00005   loss : 0.01231  time : 24.1218 \n",
            "iter : 129400  lr : 0.00005   loss : 0.01065  time : 24.1253 \n",
            "iter : 129500  lr : 0.00004   loss : 0.02884  time : 24.0922 \n",
            "iter : 129600  lr : 0.00003   loss : 0.00994  time : 24.0908 \n",
            "iter : 129700  lr : 0.00002   loss : 0.01179  time : 24.1097 \n",
            "iter : 129800  lr : 0.00002   loss : 0.03331  time : 24.0934 \n",
            "iter : 129900  lr : 0.00001   loss : 0.01375  time : 24.0932 \n",
            "iter : 130000  lr : 0.00000   loss : 0.01114  time : 24.0757 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 74.610000    top5 : 90.540000 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "\n",
        "# 1. Paramters setting\n",
        "num_class = 200\n",
        "model_save_path = './model/Resnet50_SGD_b128/'\n",
        "restore_iter = 130000\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(DEVICE)\n",
        "\n",
        "# ----- 2. data load\n",
        "print('DATA LOAD')\n",
        "test_images = image_load(\"/content/AI_Programming/test/\")\n",
        "print('DATA LOAD FINISH')\n",
        "\n",
        "# ---- 3. network build (restore model if necessary)\n",
        "model = ResNet_50().to(DEVICE)\n",
        "model.load_state_dict(torch.load(model_save_path + 'model_%d.pt' % restore_iter))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print('START TEST')\n",
        "count = 0\n",
        "for itest in range(test_images.shape[0]):\n",
        "    if itest % 100 == 0:\n",
        "        print('%d / %d ' %(itest, test_images.shape[0]))\n",
        "\n",
        "    test_img = test_images[itest:itest+1, :, :, :].astype(np.float32)\n",
        "    test_img = (test_img / 255.0 * 2.0) - 1.0\n",
        "\n",
        "    test_img = np.transpose(test_img, (0, 3, 1, 2))\n",
        "    with torch.no_grad():\n",
        "        pred = model(torch.from_numpy(test_img).to(DEVICE))\n",
        "\n",
        "    pred = pred.cpu().numpy()  # [1, 200]\n",
        "    pred = np.reshape(pred, num_class)\n",
        "\n",
        "    f = open('딥머닝_Resnet50_SGD_b128.txt', 'a+')\n",
        "\n",
        "    for ik in range(5):\n",
        "        max_index = np.argmax(pred)\n",
        "        f.write('%d ' % (int(max_index)))\n",
        "        pred[max_index] = -9999\n",
        "\n",
        "    f.write('\\n')\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "UFEzzVTjVZyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4deef4d3-ce5f-4470-a401-eda7c04ddfba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "DATA LOAD\n",
            "img num : 51752 \n",
            "DATA LOAD FINISH\n",
            "START TEST\n",
            "0 / 51752 \n",
            "100 / 51752 \n",
            "200 / 51752 \n",
            "300 / 51752 \n",
            "400 / 51752 \n",
            "500 / 51752 \n",
            "600 / 51752 \n",
            "700 / 51752 \n",
            "800 / 51752 \n",
            "900 / 51752 \n",
            "1000 / 51752 \n",
            "1100 / 51752 \n",
            "1200 / 51752 \n",
            "1300 / 51752 \n",
            "1400 / 51752 \n",
            "1500 / 51752 \n",
            "1600 / 51752 \n",
            "1700 / 51752 \n",
            "1800 / 51752 \n",
            "1900 / 51752 \n",
            "2000 / 51752 \n",
            "2100 / 51752 \n",
            "2200 / 51752 \n",
            "2300 / 51752 \n",
            "2400 / 51752 \n",
            "2500 / 51752 \n",
            "2600 / 51752 \n",
            "2700 / 51752 \n",
            "2800 / 51752 \n",
            "2900 / 51752 \n",
            "3000 / 51752 \n",
            "3100 / 51752 \n",
            "3200 / 51752 \n",
            "3300 / 51752 \n",
            "3400 / 51752 \n",
            "3500 / 51752 \n",
            "3600 / 51752 \n",
            "3700 / 51752 \n",
            "3800 / 51752 \n",
            "3900 / 51752 \n",
            "4000 / 51752 \n",
            "4100 / 51752 \n",
            "4200 / 51752 \n",
            "4300 / 51752 \n",
            "4400 / 51752 \n",
            "4500 / 51752 \n",
            "4600 / 51752 \n",
            "4700 / 51752 \n",
            "4800 / 51752 \n",
            "4900 / 51752 \n",
            "5000 / 51752 \n",
            "5100 / 51752 \n",
            "5200 / 51752 \n",
            "5300 / 51752 \n",
            "5400 / 51752 \n",
            "5500 / 51752 \n",
            "5600 / 51752 \n",
            "5700 / 51752 \n",
            "5800 / 51752 \n",
            "5900 / 51752 \n",
            "6000 / 51752 \n",
            "6100 / 51752 \n",
            "6200 / 51752 \n",
            "6300 / 51752 \n",
            "6400 / 51752 \n",
            "6500 / 51752 \n",
            "6600 / 51752 \n",
            "6700 / 51752 \n",
            "6800 / 51752 \n",
            "6900 / 51752 \n",
            "7000 / 51752 \n",
            "7100 / 51752 \n",
            "7200 / 51752 \n",
            "7300 / 51752 \n",
            "7400 / 51752 \n",
            "7500 / 51752 \n",
            "7600 / 51752 \n",
            "7700 / 51752 \n",
            "7800 / 51752 \n",
            "7900 / 51752 \n",
            "8000 / 51752 \n",
            "8100 / 51752 \n",
            "8200 / 51752 \n",
            "8300 / 51752 \n",
            "8400 / 51752 \n",
            "8500 / 51752 \n",
            "8600 / 51752 \n",
            "8700 / 51752 \n",
            "8800 / 51752 \n",
            "8900 / 51752 \n",
            "9000 / 51752 \n",
            "9100 / 51752 \n",
            "9200 / 51752 \n",
            "9300 / 51752 \n",
            "9400 / 51752 \n",
            "9500 / 51752 \n",
            "9600 / 51752 \n",
            "9700 / 51752 \n",
            "9800 / 51752 \n",
            "9900 / 51752 \n",
            "10000 / 51752 \n",
            "10100 / 51752 \n",
            "10200 / 51752 \n",
            "10300 / 51752 \n",
            "10400 / 51752 \n",
            "10500 / 51752 \n",
            "10600 / 51752 \n",
            "10700 / 51752 \n",
            "10800 / 51752 \n",
            "10900 / 51752 \n",
            "11000 / 51752 \n",
            "11100 / 51752 \n",
            "11200 / 51752 \n",
            "11300 / 51752 \n",
            "11400 / 51752 \n",
            "11500 / 51752 \n",
            "11600 / 51752 \n",
            "11700 / 51752 \n",
            "11800 / 51752 \n",
            "11900 / 51752 \n",
            "12000 / 51752 \n",
            "12100 / 51752 \n",
            "12200 / 51752 \n",
            "12300 / 51752 \n",
            "12400 / 51752 \n",
            "12500 / 51752 \n",
            "12600 / 51752 \n",
            "12700 / 51752 \n",
            "12800 / 51752 \n",
            "12900 / 51752 \n",
            "13000 / 51752 \n",
            "13100 / 51752 \n",
            "13200 / 51752 \n",
            "13300 / 51752 \n",
            "13400 / 51752 \n",
            "13500 / 51752 \n",
            "13600 / 51752 \n",
            "13700 / 51752 \n",
            "13800 / 51752 \n",
            "13900 / 51752 \n",
            "14000 / 51752 \n",
            "14100 / 51752 \n",
            "14200 / 51752 \n",
            "14300 / 51752 \n",
            "14400 / 51752 \n",
            "14500 / 51752 \n",
            "14600 / 51752 \n",
            "14700 / 51752 \n",
            "14800 / 51752 \n",
            "14900 / 51752 \n",
            "15000 / 51752 \n",
            "15100 / 51752 \n",
            "15200 / 51752 \n",
            "15300 / 51752 \n",
            "15400 / 51752 \n",
            "15500 / 51752 \n",
            "15600 / 51752 \n",
            "15700 / 51752 \n",
            "15800 / 51752 \n",
            "15900 / 51752 \n",
            "16000 / 51752 \n",
            "16100 / 51752 \n",
            "16200 / 51752 \n",
            "16300 / 51752 \n",
            "16400 / 51752 \n",
            "16500 / 51752 \n",
            "16600 / 51752 \n",
            "16700 / 51752 \n",
            "16800 / 51752 \n",
            "16900 / 51752 \n",
            "17000 / 51752 \n",
            "17100 / 51752 \n",
            "17200 / 51752 \n",
            "17300 / 51752 \n",
            "17400 / 51752 \n",
            "17500 / 51752 \n",
            "17600 / 51752 \n",
            "17700 / 51752 \n",
            "17800 / 51752 \n",
            "17900 / 51752 \n",
            "18000 / 51752 \n",
            "18100 / 51752 \n",
            "18200 / 51752 \n",
            "18300 / 51752 \n",
            "18400 / 51752 \n",
            "18500 / 51752 \n",
            "18600 / 51752 \n",
            "18700 / 51752 \n",
            "18800 / 51752 \n",
            "18900 / 51752 \n",
            "19000 / 51752 \n",
            "19100 / 51752 \n",
            "19200 / 51752 \n",
            "19300 / 51752 \n",
            "19400 / 51752 \n",
            "19500 / 51752 \n",
            "19600 / 51752 \n",
            "19700 / 51752 \n",
            "19800 / 51752 \n",
            "19900 / 51752 \n",
            "20000 / 51752 \n",
            "20100 / 51752 \n",
            "20200 / 51752 \n",
            "20300 / 51752 \n",
            "20400 / 51752 \n",
            "20500 / 51752 \n",
            "20600 / 51752 \n",
            "20700 / 51752 \n",
            "20800 / 51752 \n",
            "20900 / 51752 \n",
            "21000 / 51752 \n",
            "21100 / 51752 \n",
            "21200 / 51752 \n",
            "21300 / 51752 \n",
            "21400 / 51752 \n",
            "21500 / 51752 \n",
            "21600 / 51752 \n",
            "21700 / 51752 \n",
            "21800 / 51752 \n",
            "21900 / 51752 \n",
            "22000 / 51752 \n",
            "22100 / 51752 \n",
            "22200 / 51752 \n",
            "22300 / 51752 \n",
            "22400 / 51752 \n",
            "22500 / 51752 \n",
            "22600 / 51752 \n",
            "22700 / 51752 \n",
            "22800 / 51752 \n",
            "22900 / 51752 \n",
            "23000 / 51752 \n",
            "23100 / 51752 \n",
            "23200 / 51752 \n",
            "23300 / 51752 \n",
            "23400 / 51752 \n",
            "23500 / 51752 \n",
            "23600 / 51752 \n",
            "23700 / 51752 \n",
            "23800 / 51752 \n",
            "23900 / 51752 \n",
            "24000 / 51752 \n",
            "24100 / 51752 \n",
            "24200 / 51752 \n",
            "24300 / 51752 \n",
            "24400 / 51752 \n",
            "24500 / 51752 \n",
            "24600 / 51752 \n",
            "24700 / 51752 \n",
            "24800 / 51752 \n",
            "24900 / 51752 \n",
            "25000 / 51752 \n",
            "25100 / 51752 \n",
            "25200 / 51752 \n",
            "25300 / 51752 \n",
            "25400 / 51752 \n",
            "25500 / 51752 \n",
            "25600 / 51752 \n",
            "25700 / 51752 \n",
            "25800 / 51752 \n",
            "25900 / 51752 \n",
            "26000 / 51752 \n",
            "26100 / 51752 \n",
            "26200 / 51752 \n",
            "26300 / 51752 \n",
            "26400 / 51752 \n",
            "26500 / 51752 \n",
            "26600 / 51752 \n",
            "26700 / 51752 \n",
            "26800 / 51752 \n",
            "26900 / 51752 \n",
            "27000 / 51752 \n",
            "27100 / 51752 \n",
            "27200 / 51752 \n",
            "27300 / 51752 \n",
            "27400 / 51752 \n",
            "27500 / 51752 \n",
            "27600 / 51752 \n",
            "27700 / 51752 \n",
            "27800 / 51752 \n",
            "27900 / 51752 \n",
            "28000 / 51752 \n",
            "28100 / 51752 \n",
            "28200 / 51752 \n",
            "28300 / 51752 \n",
            "28400 / 51752 \n",
            "28500 / 51752 \n",
            "28600 / 51752 \n",
            "28700 / 51752 \n",
            "28800 / 51752 \n",
            "28900 / 51752 \n",
            "29000 / 51752 \n",
            "29100 / 51752 \n",
            "29200 / 51752 \n",
            "29300 / 51752 \n",
            "29400 / 51752 \n",
            "29500 / 51752 \n",
            "29600 / 51752 \n",
            "29700 / 51752 \n",
            "29800 / 51752 \n",
            "29900 / 51752 \n",
            "30000 / 51752 \n",
            "30100 / 51752 \n",
            "30200 / 51752 \n",
            "30300 / 51752 \n",
            "30400 / 51752 \n",
            "30500 / 51752 \n",
            "30600 / 51752 \n",
            "30700 / 51752 \n",
            "30800 / 51752 \n",
            "30900 / 51752 \n",
            "31000 / 51752 \n",
            "31100 / 51752 \n",
            "31200 / 51752 \n",
            "31300 / 51752 \n",
            "31400 / 51752 \n",
            "31500 / 51752 \n",
            "31600 / 51752 \n",
            "31700 / 51752 \n",
            "31800 / 51752 \n",
            "31900 / 51752 \n",
            "32000 / 51752 \n",
            "32100 / 51752 \n",
            "32200 / 51752 \n",
            "32300 / 51752 \n",
            "32400 / 51752 \n",
            "32500 / 51752 \n",
            "32600 / 51752 \n",
            "32700 / 51752 \n",
            "32800 / 51752 \n",
            "32900 / 51752 \n",
            "33000 / 51752 \n",
            "33100 / 51752 \n",
            "33200 / 51752 \n",
            "33300 / 51752 \n",
            "33400 / 51752 \n",
            "33500 / 51752 \n",
            "33600 / 51752 \n",
            "33700 / 51752 \n",
            "33800 / 51752 \n",
            "33900 / 51752 \n",
            "34000 / 51752 \n",
            "34100 / 51752 \n",
            "34200 / 51752 \n",
            "34300 / 51752 \n",
            "34400 / 51752 \n",
            "34500 / 51752 \n",
            "34600 / 51752 \n",
            "34700 / 51752 \n",
            "34800 / 51752 \n",
            "34900 / 51752 \n",
            "35000 / 51752 \n",
            "35100 / 51752 \n",
            "35200 / 51752 \n",
            "35300 / 51752 \n",
            "35400 / 51752 \n",
            "35500 / 51752 \n",
            "35600 / 51752 \n",
            "35700 / 51752 \n",
            "35800 / 51752 \n",
            "35900 / 51752 \n",
            "36000 / 51752 \n",
            "36100 / 51752 \n",
            "36200 / 51752 \n",
            "36300 / 51752 \n",
            "36400 / 51752 \n",
            "36500 / 51752 \n",
            "36600 / 51752 \n",
            "36700 / 51752 \n",
            "36800 / 51752 \n",
            "36900 / 51752 \n",
            "37000 / 51752 \n",
            "37100 / 51752 \n",
            "37200 / 51752 \n",
            "37300 / 51752 \n",
            "37400 / 51752 \n",
            "37500 / 51752 \n",
            "37600 / 51752 \n",
            "37700 / 51752 \n",
            "37800 / 51752 \n",
            "37900 / 51752 \n",
            "38000 / 51752 \n",
            "38100 / 51752 \n",
            "38200 / 51752 \n",
            "38300 / 51752 \n",
            "38400 / 51752 \n",
            "38500 / 51752 \n",
            "38600 / 51752 \n",
            "38700 / 51752 \n",
            "38800 / 51752 \n",
            "38900 / 51752 \n",
            "39000 / 51752 \n",
            "39100 / 51752 \n",
            "39200 / 51752 \n",
            "39300 / 51752 \n",
            "39400 / 51752 \n",
            "39500 / 51752 \n",
            "39600 / 51752 \n",
            "39700 / 51752 \n",
            "39800 / 51752 \n",
            "39900 / 51752 \n",
            "40000 / 51752 \n",
            "40100 / 51752 \n",
            "40200 / 51752 \n",
            "40300 / 51752 \n",
            "40400 / 51752 \n",
            "40500 / 51752 \n",
            "40600 / 51752 \n",
            "40700 / 51752 \n",
            "40800 / 51752 \n",
            "40900 / 51752 \n",
            "41000 / 51752 \n",
            "41100 / 51752 \n",
            "41200 / 51752 \n",
            "41300 / 51752 \n",
            "41400 / 51752 \n",
            "41500 / 51752 \n",
            "41600 / 51752 \n",
            "41700 / 51752 \n",
            "41800 / 51752 \n",
            "41900 / 51752 \n",
            "42000 / 51752 \n",
            "42100 / 51752 \n",
            "42200 / 51752 \n",
            "42300 / 51752 \n",
            "42400 / 51752 \n",
            "42500 / 51752 \n",
            "42600 / 51752 \n",
            "42700 / 51752 \n",
            "42800 / 51752 \n",
            "42900 / 51752 \n",
            "43000 / 51752 \n",
            "43100 / 51752 \n",
            "43200 / 51752 \n",
            "43300 / 51752 \n",
            "43400 / 51752 \n",
            "43500 / 51752 \n",
            "43600 / 51752 \n",
            "43700 / 51752 \n",
            "43800 / 51752 \n",
            "43900 / 51752 \n",
            "44000 / 51752 \n",
            "44100 / 51752 \n",
            "44200 / 51752 \n",
            "44300 / 51752 \n",
            "44400 / 51752 \n",
            "44500 / 51752 \n",
            "44600 / 51752 \n",
            "44700 / 51752 \n",
            "44800 / 51752 \n",
            "44900 / 51752 \n",
            "45000 / 51752 \n",
            "45100 / 51752 \n",
            "45200 / 51752 \n",
            "45300 / 51752 \n",
            "45400 / 51752 \n",
            "45500 / 51752 \n",
            "45600 / 51752 \n",
            "45700 / 51752 \n",
            "45800 / 51752 \n",
            "45900 / 51752 \n",
            "46000 / 51752 \n",
            "46100 / 51752 \n",
            "46200 / 51752 \n",
            "46300 / 51752 \n",
            "46400 / 51752 \n",
            "46500 / 51752 \n",
            "46600 / 51752 \n",
            "46700 / 51752 \n",
            "46800 / 51752 \n",
            "46900 / 51752 \n",
            "47000 / 51752 \n",
            "47100 / 51752 \n",
            "47200 / 51752 \n",
            "47300 / 51752 \n",
            "47400 / 51752 \n",
            "47500 / 51752 \n",
            "47600 / 51752 \n",
            "47700 / 51752 \n",
            "47800 / 51752 \n",
            "47900 / 51752 \n",
            "48000 / 51752 \n",
            "48100 / 51752 \n",
            "48200 / 51752 \n",
            "48300 / 51752 \n",
            "48400 / 51752 \n",
            "48500 / 51752 \n",
            "48600 / 51752 \n",
            "48700 / 51752 \n",
            "48800 / 51752 \n",
            "48900 / 51752 \n",
            "49000 / 51752 \n",
            "49100 / 51752 \n",
            "49200 / 51752 \n",
            "49300 / 51752 \n",
            "49400 / 51752 \n",
            "49500 / 51752 \n",
            "49600 / 51752 \n",
            "49700 / 51752 \n",
            "49800 / 51752 \n",
            "49900 / 51752 \n",
            "50000 / 51752 \n",
            "50100 / 51752 \n",
            "50200 / 51752 \n",
            "50300 / 51752 \n",
            "50400 / 51752 \n",
            "50500 / 51752 \n",
            "50600 / 51752 \n",
            "50700 / 51752 \n",
            "50800 / 51752 \n",
            "50900 / 51752 \n",
            "51000 / 51752 \n",
            "51100 / 51752 \n",
            "51200 / 51752 \n",
            "51300 / 51752 \n",
            "51400 / 51752 \n",
            "51500 / 51752 \n",
            "51600 / 51752 \n",
            "51700 / 51752 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uCq0BfKCe5nT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}