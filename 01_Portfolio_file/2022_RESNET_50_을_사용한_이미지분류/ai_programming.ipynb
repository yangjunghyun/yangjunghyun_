{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J2vPGaNNkRz",
        "outputId": "66e00872-5e5f-4762-d90b-a79b30d51422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "my_zip = zipfile.ZipFile('/content/drive/MyDrive/양정현/net/Dataset.zip','r')\n",
        "my_zip.extractall('./AI_Programming')"
      ],
      "metadata": {
        "id": "llgUH0KJNqmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import torch\n",
        "\n",
        "def image_load(img_path, height=128, width=128, bshort=False):\n",
        "    # ---- image total number\n",
        "    img_names = sorted(os.listdir(img_path))\n",
        "\n",
        "    if not bshort:\n",
        "        img_num = len(img_names)\n",
        "    else:\n",
        "        img_num = 10000\n",
        "\n",
        "    print(\"img num : %d \" % img_num)\n",
        "    images = np.zeros((img_num, height, width, 3), dtype=np.uint8)\n",
        "\n",
        "    for it in range(img_num):\n",
        "        images[it, :, :, :] = cv2.imread(img_path + '%s' %(img_names[it]))\n",
        "\n",
        "    return images\n",
        "\n",
        "def gt_load(gt_file):\n",
        "    f = open(gt_file)\n",
        "    lines = f.readlines()\n",
        "    num_gt = len(lines)\n",
        "    print('gt num : %d ' %num_gt)\n",
        "\n",
        "    gts = np.zeros(num_gt, dtype=np.int32)\n",
        "\n",
        "    for it in range(num_gt):\n",
        "        gts[it] = int(lines[it][:-1]) - 1  # saved gt values range : 1 ~ 200 -> 0 ~ 199\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    return gts\n",
        "\n",
        "def mini_batch_image(train_img, train_gts, batch_size, crop_size=96):\n",
        "    batch_img = np.zeros((batch_size, crop_size, crop_size, 3), dtype=np.float32)\n",
        "    batch_gts = np.zeros(batch_size, dtype=np.long)\n",
        "\n",
        "    rand_num = np.random.randint(0, train_img.shape[0], batch_size)\n",
        "\n",
        "    for it in range(batch_size):\n",
        "        img = train_img[rand_num[it], :, :, :]\n",
        "\n",
        "        # --- dataAug 1. Flip\n",
        "        rand_flip = np.random.normal(0.0, scale=1.0)\n",
        "        if rand_flip < 0:\n",
        "            img = cv2.flip(img, 1)\n",
        "\n",
        "        # --- dataAug 2. Crop\n",
        "        crop_y = np.random.randint(0, img.shape[0] - crop_size - 1)\n",
        "        crop_x = np.random.randint(0, img.shape[1] - crop_size - 1)\n",
        "        img = img[crop_y: crop_y + crop_size, crop_x:crop_x + crop_size, :]\n",
        "\n",
        "        batch_img[it, :, :, :] = (img / 255.0 * 2.0) - 1.0\n",
        "        batch_gts[it] = train_gts[rand_num[it]]\n",
        "\n",
        "    return batch_img, batch_gts"
      ],
      "metadata": {
        "id": "zP2dq6XCKSyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class residual_block(nn.Module):\n",
        "    def __init__(self, c_in,c_mid, c_out, bdown=False):\n",
        "        super(residual_block, self).__init__()\n",
        "\n",
        "        self.c_in = c_in\n",
        "        self.c_mid = c_mid\n",
        "        self.c_out = c_out\n",
        "\n",
        "        self.bdown = bdown\n",
        "        if self.bdown:\n",
        "            stride = 2\n",
        "        else:\n",
        "            stride = 1\n",
        "\n",
        "        self.convs = nn.Sequential(nn.BatchNorm2d(c_in),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(c_in, c_mid, kernel_size=(1, 1), padding=0, stride=(1, 1)),\n",
        "                      nn.BatchNorm2d(c_mid),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(c_mid, c_mid, kernel_size=(5, 5), padding=2,stride = (1,1)),\n",
        "                      nn.BatchNorm2d(c_mid),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Conv2d(c_mid, c_out, kernel_size=(1, 1), padding=0,stride = (1,1)),\n",
        "        )\n",
        "\n",
        "        if self.c_in != c_out:\n",
        "            self.conv_db = nn.Conv2d(c_in, c_out, kernel_size=(1, 1), stride=(1, 1),padding = 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x\n",
        "        x = self.convs(x)\n",
        "\n",
        "        if self.c_in != self.c_out:\n",
        "            y = self.conv_db(y)\n",
        "\n",
        "        return (x + y) / np.sqrt(2.0)\n",
        "\n",
        "class ResNet_50(nn.Module):\n",
        "    def __init__(self, c_in=3, conv_ch=64, output_size=200):\n",
        "        super(ResNet_50, self).__init__()\n",
        "        self.ch = conv_ch\n",
        "\n",
        "        self.conv = nn.Conv2d(c_in, self.ch, kernel_size=(7, 7), padding=3, stride=(2, 2))\n",
        "        self.maxp = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2))\n",
        "        self.resblocks = nn.Sequential(residual_block(self.ch,self.ch, self.ch*4),\n",
        "                                       residual_block(self.ch*4,self.ch, self.ch*4),\n",
        "                                       residual_block(self.ch*4,self.ch, self.ch*4),\n",
        "\n",
        "                                       residual_block(self.ch*4, self.ch*2, self.ch*8, bdown=True),\n",
        "                                       residual_block(self.ch*8, self.ch*2,self.ch*8),\n",
        "                                       residual_block(self.ch*8, self.ch*2,self.ch*8),\n",
        "                                       residual_block(self.ch*8, self.ch*2,self.ch*8),\n",
        "\n",
        "\n",
        "                                       residual_block(self.ch*8, self.ch*4,self.ch*16, bdown=True),\n",
        "                                       residual_block(self.ch*16, self.ch*4,self.ch*16),\n",
        "                                       residual_block(self.ch*16, self.ch*4,self.ch*16),\n",
        "                                       residual_block(self.ch*16, self.ch*4,self.ch*16),\n",
        "                                       residual_block(self.ch*16, self.ch*4,self.ch*16),\n",
        "                                       residual_block(self.ch*16, self.ch*4,self.ch*16),\n",
        "\n",
        "                                       residual_block(self.ch*16, self.ch*8,self.ch*32, bdown=True),\n",
        "                                       residual_block(self.ch*32, self.ch*8, self.ch*32),\n",
        "                                       residual_block(self.ch*32, self.ch*8, self.ch*32),\n",
        "\n",
        "                                       nn.BatchNorm2d(self.ch*32),\n",
        "                                       nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.dr = nn.Dropout(p=0.2)\n",
        "        self.fc = nn.Linear(32* self.ch, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.maxp(x)\n",
        "        x = self.resblocks(x)\n",
        "        x = torch.mean(x, dim=(2, 3))\n",
        "        x = self.dr(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ME9QBUs6W4LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "\n",
        "# 1. Paramters setting\n",
        "num_class = 200\n",
        "batch_size = 64  # or 32\n",
        "initial_lr = 0.01  # 0.01 ---> 0\n",
        "max_iter = 130000  # or 15~20만\n",
        "\n",
        "save_name = 'Resnet50_SGD_b64_v2'\n",
        "\n",
        "model_save_path = './model/%s/' %save_name\n",
        "model_saving_iter = 5000\n",
        "brestore = False\n",
        "restore_iter = 6000\n",
        "if not brestore:\n",
        "    restore_iter = 0\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(DEVICE)\n",
        "\n",
        "# ----- 2. data load\n",
        "print('DATA LOAD')\n",
        "train_images = image_load(\"/content/AI_Programming/train/\")\n",
        "train_gts = gt_load(\"/content/AI_Programming/train_gt.txt\")\n",
        "\n",
        "test_images = image_load(\"/content/AI_Programming/test/\", bshort=True)\n",
        "test_gts = gt_load(\"/content/AI_Programming/test_gt_short.txt\")\n",
        "\n",
        "print('DATA LOAD FINISH')\n",
        "\n",
        "# ---- 3. network build (restore model if necessary)\n",
        "model = ResNet_50().to(DEVICE)\n",
        "\n",
        "if brestore:\n",
        "    model.load_state_dict(torch.load(model_save_path + 'model_%d.pt' % restore_iter))\n",
        "\n",
        "# ---- 4. loss function and optimizer\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr, eps=1.0)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, weight_decay=1e-4, momentum=0.9)\n",
        "\n",
        "start_time = time.time()\n",
        "for it in range(restore_iter, max_iter+1):\n",
        "    optimizer.param_groups[0]['lr'] = initial_lr - (it / max_iter) * initial_lr\n",
        "\n",
        "    batch_img, batch_gts = mini_batch_image(train_images, train_gts, batch_size)\n",
        "    batch_img = np.transpose(batch_img, (0, 3, 1, 2))\n",
        "\n",
        "    # ----- training step\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(torch.from_numpy(batch_img).to(DEVICE))\n",
        "    gt_tensor = torch.tensor(batch_gts, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    train_loss = loss(pred, gt_tensor)\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if it % 100 == 0:\n",
        "        consume_time = time.time() - start_time\n",
        "        print(\"iter : %d  lr : %.5f   loss : %.5f  time : %.4f \" % (it, optimizer.param_groups[0]['lr'], train_loss.item(), consume_time))\n",
        "        start_time = time.time()\n",
        "\n",
        "    if it % model_saving_iter == 0 and it != 0:\n",
        "        print('SAVING MODEL')\n",
        "        if not os.path.isdir(model_save_path):\n",
        "            os.makedirs(model_save_path)\n",
        "\n",
        "        torch.save(model.state_dict(), model_save_path + 'model_%d.pt' % it)\n",
        "        print('SAVING MODEL FINISH')\n",
        "\n",
        "        print('START TEST')\n",
        "        model.eval()\n",
        "        t1_count = 0\n",
        "        t5_count = 0\n",
        "        for itest in range(10000):\n",
        "            test_img = test_images[itest:itest+1, :, :, :].astype(np.float32)\n",
        "            test_img = (test_img / 255.0 * 2.0) - 1.0\n",
        "\n",
        "            test_img = np.transpose(test_img, (0, 3, 1, 2))\n",
        "            with torch.no_grad():\n",
        "                pred = model(torch.from_numpy(test_img).to(DEVICE))\n",
        "\n",
        "            gt = test_gts[itest]\n",
        "\n",
        "            pred = pred.cpu().numpy()  # [1, 200]\n",
        "            pred = np.reshape(pred, num_class)\n",
        "\n",
        "            # ----- top5 accuracy\n",
        "            for ik in range(5):\n",
        "                max_index = np.argmax(pred)\n",
        "                if int(gt) == int(max_index):\n",
        "                    t5_count += 1\n",
        "                    # ----- top1 accuracy\n",
        "                    if ik == 0:\n",
        "                        t1_count += 1\n",
        "\n",
        "                pred[max_index] = -9999\n",
        "\n",
        "        print('top1 : %f    top5 : %f \\n' %(t1_count / 10000 * 100, t5_count / 10000 * 100))\n",
        "        f = open('%s.txt' %save_name, 'a+')\n",
        "        f.write('top1 : %f    top5 : %f \\n' %(t1_count / 10000 * 100, t5_count / 10000 * 100))\n",
        "        f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fezNJOZ-VMtt",
        "outputId": "8e8249d0-d2f1-41c4-c8fd-2468c1015cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "DATA LOAD\n",
            "img num : 207005 \n",
            "gt num : 207005 \n",
            "img num : 10000 \n",
            "gt num : 10000 \n",
            "DATA LOAD FINISH\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-df32df591bd6>:41: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  batch_gts = np.zeros(batch_size, dtype=np.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter : 0  lr : 0.01000   loss : 5.33023  time : 0.8644 \n",
            "iter : 100  lr : 0.00999   loss : 5.17091  time : 16.2492 \n",
            "iter : 200  lr : 0.00998   loss : 5.17531  time : 16.2338 \n",
            "iter : 300  lr : 0.00998   loss : 5.01193  time : 16.2458 \n",
            "iter : 400  lr : 0.00997   loss : 4.94319  time : 16.2318 \n",
            "iter : 500  lr : 0.00996   loss : 4.61053  time : 16.2536 \n",
            "iter : 600  lr : 0.00995   loss : 4.80545  time : 16.2358 \n",
            "iter : 700  lr : 0.00995   loss : 4.81506  time : 16.2468 \n",
            "iter : 800  lr : 0.00994   loss : 4.71118  time : 16.2448 \n",
            "iter : 900  lr : 0.00993   loss : 4.59723  time : 16.2327 \n",
            "iter : 1000  lr : 0.00992   loss : 4.53103  time : 16.2352 \n",
            "iter : 1100  lr : 0.00992   loss : 4.49592  time : 16.2409 \n",
            "iter : 1200  lr : 0.00991   loss : 4.51274  time : 16.2301 \n",
            "iter : 1300  lr : 0.00990   loss : 4.40963  time : 16.2516 \n",
            "iter : 1400  lr : 0.00989   loss : 4.28420  time : 16.2385 \n",
            "iter : 1500  lr : 0.00988   loss : 4.41777  time : 16.2231 \n",
            "iter : 1600  lr : 0.00988   loss : 4.09897  time : 16.2375 \n",
            "iter : 1700  lr : 0.00987   loss : 4.49723  time : 16.2243 \n",
            "iter : 1800  lr : 0.00986   loss : 4.44730  time : 16.2299 \n",
            "iter : 1900  lr : 0.00985   loss : 4.21516  time : 16.2272 \n",
            "iter : 2000  lr : 0.00985   loss : 4.25484  time : 16.2385 \n",
            "iter : 2100  lr : 0.00984   loss : 4.29504  time : 16.2418 \n",
            "iter : 2200  lr : 0.00983   loss : 4.29019  time : 16.2416 \n",
            "iter : 2300  lr : 0.00982   loss : 4.46545  time : 16.2383 \n",
            "iter : 2400  lr : 0.00982   loss : 3.76137  time : 16.2292 \n",
            "iter : 2500  lr : 0.00981   loss : 4.26212  time : 16.2326 \n",
            "iter : 2600  lr : 0.00980   loss : 4.08160  time : 16.2178 \n",
            "iter : 2700  lr : 0.00979   loss : 3.70962  time : 16.2293 \n",
            "iter : 2800  lr : 0.00978   loss : 3.99359  time : 16.2239 \n",
            "iter : 2900  lr : 0.00978   loss : 4.24112  time : 16.2352 \n",
            "iter : 3000  lr : 0.00977   loss : 3.97925  time : 16.2331 \n",
            "iter : 3100  lr : 0.00976   loss : 3.96233  time : 16.2377 \n",
            "iter : 3200  lr : 0.00975   loss : 3.88347  time : 16.2525 \n",
            "iter : 3300  lr : 0.00975   loss : 3.99433  time : 16.2313 \n",
            "iter : 3400  lr : 0.00974   loss : 3.63732  time : 16.2568 \n",
            "iter : 3500  lr : 0.00973   loss : 3.77439  time : 16.2312 \n",
            "iter : 3600  lr : 0.00972   loss : 3.74013  time : 16.2446 \n",
            "iter : 3700  lr : 0.00972   loss : 3.95570  time : 16.2516 \n",
            "iter : 3800  lr : 0.00971   loss : 3.56259  time : 16.2419 \n",
            "iter : 3900  lr : 0.00970   loss : 3.87291  time : 16.2425 \n",
            "iter : 4000  lr : 0.00969   loss : 3.60085  time : 16.2341 \n",
            "iter : 4100  lr : 0.00968   loss : 3.40006  time : 16.2606 \n",
            "iter : 4200  lr : 0.00968   loss : 3.77768  time : 16.2381 \n",
            "iter : 4300  lr : 0.00967   loss : 3.64680  time : 16.2401 \n",
            "iter : 4400  lr : 0.00966   loss : 3.62919  time : 16.2361 \n",
            "iter : 4500  lr : 0.00965   loss : 3.55139  time : 16.2479 \n",
            "iter : 4600  lr : 0.00965   loss : 3.47372  time : 16.2652 \n",
            "iter : 4700  lr : 0.00964   loss : 3.67000  time : 16.2546 \n",
            "iter : 4800  lr : 0.00963   loss : 3.52748  time : 16.2423 \n",
            "iter : 4900  lr : 0.00962   loss : 3.66321  time : 16.2465 \n",
            "iter : 5000  lr : 0.00962   loss : 3.03922  time : 16.2485 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 19.510000    top5 : 42.900000 \n",
            "\n",
            "iter : 5100  lr : 0.00961   loss : 3.21868  time : 145.1654 \n",
            "iter : 5200  lr : 0.00960   loss : 3.66188  time : 16.2255 \n",
            "iter : 5300  lr : 0.00959   loss : 3.30236  time : 16.2434 \n",
            "iter : 5400  lr : 0.00958   loss : 3.30504  time : 16.2323 \n",
            "iter : 5500  lr : 0.00958   loss : 3.44131  time : 16.2377 \n",
            "iter : 5600  lr : 0.00957   loss : 3.24298  time : 16.2384 \n",
            "iter : 5700  lr : 0.00956   loss : 3.06154  time : 16.2408 \n",
            "iter : 5800  lr : 0.00955   loss : 3.53383  time : 16.2305 \n",
            "iter : 5900  lr : 0.00955   loss : 3.66681  time : 16.2413 \n",
            "iter : 6000  lr : 0.00954   loss : 3.64407  time : 16.2299 \n",
            "iter : 6100  lr : 0.00953   loss : 3.43216  time : 16.2261 \n",
            "iter : 6200  lr : 0.00952   loss : 3.12277  time : 16.2327 \n",
            "iter : 6300  lr : 0.00952   loss : 3.50577  time : 16.2519 \n",
            "iter : 6400  lr : 0.00951   loss : 2.73278  time : 16.2409 \n",
            "iter : 6500  lr : 0.00950   loss : 3.22396  time : 16.2386 \n",
            "iter : 6600  lr : 0.00949   loss : 2.90867  time : 16.2536 \n",
            "iter : 6700  lr : 0.00948   loss : 3.15990  time : 16.2424 \n",
            "iter : 6800  lr : 0.00948   loss : 3.61187  time : 16.2627 \n",
            "iter : 6900  lr : 0.00947   loss : 2.82105  time : 16.2306 \n",
            "iter : 7000  lr : 0.00946   loss : 2.60722  time : 16.2971 \n",
            "iter : 7100  lr : 0.00945   loss : 2.65178  time : 16.3014 \n",
            "iter : 7200  lr : 0.00945   loss : 3.60588  time : 16.3136 \n",
            "iter : 7300  lr : 0.00944   loss : 3.05388  time : 16.3314 \n",
            "iter : 7400  lr : 0.00943   loss : 2.99271  time : 16.3019 \n",
            "iter : 7500  lr : 0.00942   loss : 3.04165  time : 16.3077 \n",
            "iter : 7600  lr : 0.00942   loss : 2.72321  time : 16.3090 \n",
            "iter : 7700  lr : 0.00941   loss : 3.01791  time : 16.2853 \n",
            "iter : 7800  lr : 0.00940   loss : 3.07528  time : 16.2989 \n",
            "iter : 7900  lr : 0.00939   loss : 2.75879  time : 16.3139 \n",
            "iter : 8000  lr : 0.00938   loss : 2.71739  time : 16.2915 \n",
            "iter : 8100  lr : 0.00938   loss : 3.35949  time : 16.3186 \n",
            "iter : 8200  lr : 0.00937   loss : 2.80795  time : 16.3351 \n",
            "iter : 8300  lr : 0.00936   loss : 2.80561  time : 16.3424 \n",
            "iter : 8400  lr : 0.00935   loss : 2.66644  time : 16.3113 \n",
            "iter : 8500  lr : 0.00935   loss : 3.06185  time : 16.2368 \n",
            "iter : 8600  lr : 0.00934   loss : 3.15960  time : 16.2971 \n",
            "iter : 8700  lr : 0.00933   loss : 2.84452  time : 16.2598 \n",
            "iter : 8800  lr : 0.00932   loss : 3.07345  time : 16.3229 \n",
            "iter : 8900  lr : 0.00932   loss : 3.04066  time : 16.3287 \n",
            "iter : 9000  lr : 0.00931   loss : 2.97657  time : 16.3694 \n",
            "iter : 9100  lr : 0.00930   loss : 3.05007  time : 16.4428 \n",
            "iter : 9200  lr : 0.00929   loss : 2.63323  time : 16.4379 \n",
            "iter : 9300  lr : 0.00928   loss : 2.93960  time : 16.4423 \n",
            "iter : 9400  lr : 0.00928   loss : 2.97883  time : 16.4343 \n",
            "iter : 9500  lr : 0.00927   loss : 2.37464  time : 16.4394 \n",
            "iter : 9600  lr : 0.00926   loss : 2.96553  time : 16.4158 \n",
            "iter : 9700  lr : 0.00925   loss : 2.91283  time : 16.3822 \n",
            "iter : 9800  lr : 0.00925   loss : 3.07950  time : 16.3589 \n",
            "iter : 9900  lr : 0.00924   loss : 2.96396  time : 16.3722 \n",
            "iter : 10000  lr : 0.00923   loss : 2.66144  time : 16.3684 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 32.670000    top5 : 60.100000 \n",
            "\n",
            "iter : 10100  lr : 0.00922   loss : 2.82293  time : 150.2954 \n",
            "iter : 10200  lr : 0.00922   loss : 3.09408  time : 16.4290 \n",
            "iter : 10300  lr : 0.00921   loss : 2.69553  time : 16.3939 \n",
            "iter : 10400  lr : 0.00920   loss : 2.43196  time : 16.3808 \n",
            "iter : 10500  lr : 0.00919   loss : 2.25036  time : 16.3426 \n",
            "iter : 10600  lr : 0.00918   loss : 2.63223  time : 16.4348 \n",
            "iter : 10700  lr : 0.00918   loss : 2.66335  time : 16.4255 \n",
            "iter : 10800  lr : 0.00917   loss : 2.52161  time : 16.3390 \n",
            "iter : 10900  lr : 0.00916   loss : 2.52619  time : 16.2984 \n",
            "iter : 11000  lr : 0.00915   loss : 2.83428  time : 16.2903 \n",
            "iter : 11100  lr : 0.00915   loss : 2.46166  time : 16.2784 \n",
            "iter : 11200  lr : 0.00914   loss : 2.27903  time : 16.2921 \n",
            "iter : 11300  lr : 0.00913   loss : 2.56886  time : 16.2968 \n",
            "iter : 11400  lr : 0.00912   loss : 2.84766  time : 16.3072 \n",
            "iter : 11500  lr : 0.00912   loss : 2.33684  time : 16.3123 \n",
            "iter : 11600  lr : 0.00911   loss : 2.94369  time : 16.3002 \n",
            "iter : 11700  lr : 0.00910   loss : 2.86945  time : 16.2855 \n",
            "iter : 11800  lr : 0.00909   loss : 2.47020  time : 16.2955 \n",
            "iter : 11900  lr : 0.00908   loss : 2.48299  time : 16.3278 \n",
            "iter : 12000  lr : 0.00908   loss : 2.48969  time : 16.3214 \n",
            "iter : 12100  lr : 0.00907   loss : 2.52142  time : 16.3197 \n",
            "iter : 12200  lr : 0.00906   loss : 2.33674  time : 16.2568 \n",
            "iter : 12300  lr : 0.00905   loss : 2.41491  time : 16.2663 \n",
            "iter : 12400  lr : 0.00905   loss : 2.59586  time : 16.2673 \n",
            "iter : 12500  lr : 0.00904   loss : 2.70325  time : 16.2583 \n",
            "iter : 12600  lr : 0.00903   loss : 3.05551  time : 16.2860 \n",
            "iter : 12700  lr : 0.00902   loss : 2.19231  time : 16.2602 \n",
            "iter : 12800  lr : 0.00902   loss : 2.76824  time : 16.2697 \n",
            "iter : 12900  lr : 0.00901   loss : 2.86691  time : 16.2638 \n",
            "iter : 13000  lr : 0.00900   loss : 2.35523  time : 16.2657 \n",
            "iter : 13100  lr : 0.00899   loss : 2.34612  time : 16.2821 \n",
            "iter : 13200  lr : 0.00898   loss : 2.85493  time : 16.2835 \n",
            "iter : 13300  lr : 0.00898   loss : 2.84834  time : 16.3022 \n",
            "iter : 13400  lr : 0.00897   loss : 2.72234  time : 16.2823 \n",
            "iter : 13500  lr : 0.00896   loss : 2.27256  time : 16.2800 \n",
            "iter : 13600  lr : 0.00895   loss : 2.40901  time : 16.2966 \n",
            "iter : 13700  lr : 0.00895   loss : 1.76964  time : 16.2710 \n",
            "iter : 13800  lr : 0.00894   loss : 2.73567  time : 16.2663 \n",
            "iter : 13900  lr : 0.00893   loss : 2.29603  time : 16.2600 \n",
            "iter : 14000  lr : 0.00892   loss : 2.49672  time : 16.2650 \n",
            "iter : 14100  lr : 0.00892   loss : 2.45683  time : 16.2735 \n",
            "iter : 14200  lr : 0.00891   loss : 2.51801  time : 16.2558 \n",
            "iter : 14300  lr : 0.00890   loss : 2.38253  time : 16.2786 \n",
            "iter : 14400  lr : 0.00889   loss : 2.26324  time : 16.2663 \n",
            "iter : 14500  lr : 0.00888   loss : 2.28611  time : 16.2995 \n",
            "iter : 14600  lr : 0.00888   loss : 2.42223  time : 16.3002 \n",
            "iter : 14700  lr : 0.00887   loss : 2.04118  time : 16.2920 \n",
            "iter : 14800  lr : 0.00886   loss : 2.52767  time : 16.2879 \n",
            "iter : 14900  lr : 0.00885   loss : 2.39453  time : 16.3046 \n",
            "iter : 15000  lr : 0.00885   loss : 2.26546  time : 16.3059 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 43.270000    top5 : 70.630000 \n",
            "\n",
            "iter : 15100  lr : 0.00884   loss : 2.35752  time : 145.1492 \n",
            "iter : 15200  lr : 0.00883   loss : 2.03070  time : 16.2597 \n",
            "iter : 15300  lr : 0.00882   loss : 2.47395  time : 16.2806 \n",
            "iter : 15400  lr : 0.00882   loss : 2.54609  time : 16.2764 \n",
            "iter : 15500  lr : 0.00881   loss : 2.43524  time : 16.2909 \n",
            "iter : 15600  lr : 0.00880   loss : 2.39278  time : 16.2832 \n",
            "iter : 15700  lr : 0.00879   loss : 2.30995  time : 16.2906 \n",
            "iter : 15800  lr : 0.00878   loss : 2.14073  time : 16.2605 \n",
            "iter : 15900  lr : 0.00878   loss : 2.19120  time : 16.2643 \n",
            "iter : 16000  lr : 0.00877   loss : 2.30040  time : 16.2727 \n",
            "iter : 16100  lr : 0.00876   loss : 1.99768  time : 16.2642 \n",
            "iter : 16200  lr : 0.00875   loss : 2.01612  time : 16.2640 \n",
            "iter : 16300  lr : 0.00875   loss : 1.81628  time : 16.2741 \n",
            "iter : 16400  lr : 0.00874   loss : 2.07561  time : 16.2650 \n",
            "iter : 16500  lr : 0.00873   loss : 2.30369  time : 16.2811 \n",
            "iter : 16600  lr : 0.00872   loss : 2.31735  time : 16.2853 \n",
            "iter : 16700  lr : 0.00872   loss : 2.07506  time : 16.2636 \n",
            "iter : 16800  lr : 0.00871   loss : 2.14184  time : 16.2743 \n",
            "iter : 16900  lr : 0.00870   loss : 2.31754  time : 16.2766 \n",
            "iter : 17000  lr : 0.00869   loss : 2.12033  time : 16.2765 \n",
            "iter : 17100  lr : 0.00868   loss : 2.32071  time : 16.2700 \n",
            "iter : 17200  lr : 0.00868   loss : 2.07847  time : 16.2818 \n",
            "iter : 17300  lr : 0.00867   loss : 2.20142  time : 16.2686 \n",
            "iter : 17400  lr : 0.00866   loss : 2.39158  time : 16.2823 \n",
            "iter : 17500  lr : 0.00865   loss : 2.36683  time : 16.2712 \n",
            "iter : 17600  lr : 0.00865   loss : 2.35382  time : 16.2741 \n",
            "iter : 17700  lr : 0.00864   loss : 2.09791  time : 16.2839 \n",
            "iter : 17800  lr : 0.00863   loss : 1.92897  time : 16.2661 \n",
            "iter : 17900  lr : 0.00862   loss : 1.71741  time : 16.2941 \n",
            "iter : 18000  lr : 0.00862   loss : 1.87505  time : 16.2615 \n",
            "iter : 18100  lr : 0.00861   loss : 1.67295  time : 16.2738 \n",
            "iter : 18200  lr : 0.00860   loss : 1.95461  time : 16.2838 \n",
            "iter : 18300  lr : 0.00859   loss : 1.92021  time : 16.2735 \n",
            "iter : 18400  lr : 0.00858   loss : 2.15977  time : 16.2811 \n",
            "iter : 18500  lr : 0.00858   loss : 2.16373  time : 16.2812 \n",
            "iter : 18600  lr : 0.00857   loss : 1.93481  time : 16.2806 \n",
            "iter : 18700  lr : 0.00856   loss : 2.62218  time : 16.2715 \n",
            "iter : 18800  lr : 0.00855   loss : 2.15456  time : 16.2785 \n",
            "iter : 18900  lr : 0.00855   loss : 1.95276  time : 16.2768 \n",
            "iter : 19000  lr : 0.00854   loss : 2.19985  time : 16.2632 \n",
            "iter : 19100  lr : 0.00853   loss : 2.14637  time : 16.2695 \n",
            "iter : 19200  lr : 0.00852   loss : 2.08199  time : 16.2711 \n",
            "iter : 19300  lr : 0.00852   loss : 1.95609  time : 16.2655 \n",
            "iter : 19400  lr : 0.00851   loss : 1.80787  time : 16.2794 \n",
            "iter : 19500  lr : 0.00850   loss : 1.70741  time : 16.2785 \n",
            "iter : 19600  lr : 0.00849   loss : 2.05762  time : 16.2949 \n",
            "iter : 19700  lr : 0.00848   loss : 2.03904  time : 16.2836 \n",
            "iter : 19800  lr : 0.00848   loss : 2.01453  time : 16.2774 \n",
            "iter : 19900  lr : 0.00847   loss : 2.06390  time : 16.2863 \n",
            "iter : 20000  lr : 0.00846   loss : 1.72279  time : 16.2720 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 47.520000    top5 : 74.510000 \n",
            "\n",
            "iter : 20100  lr : 0.00845   loss : 1.80011  time : 145.2456 \n",
            "iter : 20200  lr : 0.00845   loss : 1.82172  time : 16.2877 \n",
            "iter : 20300  lr : 0.00844   loss : 2.21083  time : 16.2727 \n",
            "iter : 20400  lr : 0.00843   loss : 2.34209  time : 16.2715 \n",
            "iter : 20500  lr : 0.00842   loss : 2.08499  time : 16.2698 \n",
            "iter : 20600  lr : 0.00842   loss : 1.95068  time : 16.2773 \n",
            "iter : 20700  lr : 0.00841   loss : 1.84813  time : 16.2771 \n",
            "iter : 20800  lr : 0.00840   loss : 1.95497  time : 16.2872 \n",
            "iter : 20900  lr : 0.00839   loss : 2.19416  time : 16.2809 \n",
            "iter : 21000  lr : 0.00838   loss : 1.64483  time : 16.2818 \n",
            "iter : 21100  lr : 0.00838   loss : 1.85205  time : 16.2938 \n",
            "iter : 21200  lr : 0.00837   loss : 1.77103  time : 16.2976 \n",
            "iter : 21300  lr : 0.00836   loss : 2.08803  time : 16.2902 \n",
            "iter : 21400  lr : 0.00835   loss : 1.80840  time : 16.2975 \n",
            "iter : 21500  lr : 0.00835   loss : 1.50885  time : 16.2850 \n",
            "iter : 21600  lr : 0.00834   loss : 1.85802  time : 16.2806 \n",
            "iter : 21700  lr : 0.00833   loss : 2.20976  time : 16.2788 \n",
            "iter : 21800  lr : 0.00832   loss : 1.92420  time : 16.2794 \n",
            "iter : 21900  lr : 0.00832   loss : 2.01249  time : 16.2818 \n",
            "iter : 22000  lr : 0.00831   loss : 2.00147  time : 16.2848 \n",
            "iter : 22100  lr : 0.00830   loss : 1.64484  time : 16.2768 \n",
            "iter : 22200  lr : 0.00829   loss : 2.21845  time : 16.2867 \n",
            "iter : 22300  lr : 0.00828   loss : 1.65118  time : 16.2850 \n",
            "iter : 22400  lr : 0.00828   loss : 1.97578  time : 16.2841 \n",
            "iter : 22500  lr : 0.00827   loss : 1.77890  time : 16.2867 \n",
            "iter : 22600  lr : 0.00826   loss : 1.90623  time : 16.2854 \n",
            "iter : 22700  lr : 0.00825   loss : 1.60984  time : 16.3000 \n",
            "iter : 22800  lr : 0.00825   loss : 1.94858  time : 16.2846 \n",
            "iter : 22900  lr : 0.00824   loss : 2.10030  time : 16.2839 \n",
            "iter : 23000  lr : 0.00823   loss : 1.37486  time : 16.2972 \n",
            "iter : 23100  lr : 0.00822   loss : 2.02503  time : 16.2875 \n",
            "iter : 23200  lr : 0.00822   loss : 2.00820  time : 16.2929 \n",
            "iter : 23300  lr : 0.00821   loss : 2.49995  time : 16.2970 \n",
            "iter : 23400  lr : 0.00820   loss : 1.62334  time : 16.2994 \n",
            "iter : 23500  lr : 0.00819   loss : 1.45507  time : 16.2960 \n",
            "iter : 23600  lr : 0.00818   loss : 1.97299  time : 16.3198 \n",
            "iter : 23700  lr : 0.00818   loss : 1.83723  time : 16.3089 \n",
            "iter : 23800  lr : 0.00817   loss : 2.26072  time : 16.3083 \n",
            "iter : 23900  lr : 0.00816   loss : 1.54167  time : 16.2875 \n",
            "iter : 24000  lr : 0.00815   loss : 1.37045  time : 16.2888 \n",
            "iter : 24100  lr : 0.00815   loss : 1.73243  time : 16.2733 \n",
            "iter : 24200  lr : 0.00814   loss : 1.66252  time : 16.2888 \n",
            "iter : 24300  lr : 0.00813   loss : 1.60215  time : 16.2897 \n",
            "iter : 24400  lr : 0.00812   loss : 1.82135  time : 16.2765 \n",
            "iter : 24500  lr : 0.00812   loss : 1.95089  time : 16.2721 \n",
            "iter : 24600  lr : 0.00811   loss : 1.79644  time : 16.2967 \n",
            "iter : 24700  lr : 0.00810   loss : 1.82826  time : 16.2959 \n",
            "iter : 24800  lr : 0.00809   loss : 1.79349  time : 16.3068 \n",
            "iter : 24900  lr : 0.00808   loss : 1.99461  time : 16.3589 \n",
            "iter : 25000  lr : 0.00808   loss : 1.71012  time : 16.3494 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 55.490000    top5 : 79.710000 \n",
            "\n",
            "iter : 25100  lr : 0.00807   loss : 1.73466  time : 145.7411 \n",
            "iter : 25200  lr : 0.00806   loss : 1.56406  time : 16.2214 \n",
            "iter : 25300  lr : 0.00805   loss : 1.83799  time : 16.2206 \n",
            "iter : 25400  lr : 0.00805   loss : 1.94996  time : 16.2205 \n",
            "iter : 25500  lr : 0.00804   loss : 2.36126  time : 16.2423 \n",
            "iter : 25600  lr : 0.00803   loss : 1.45580  time : 16.2311 \n",
            "iter : 25700  lr : 0.00802   loss : 1.92658  time : 16.2297 \n",
            "iter : 25800  lr : 0.00802   loss : 2.27628  time : 16.2332 \n",
            "iter : 25900  lr : 0.00801   loss : 1.88162  time : 16.2500 \n",
            "iter : 26000  lr : 0.00800   loss : 1.96002  time : 16.2269 \n",
            "iter : 26100  lr : 0.00799   loss : 1.72239  time : 16.2329 \n",
            "iter : 26200  lr : 0.00798   loss : 1.75901  time : 16.2183 \n",
            "iter : 26300  lr : 0.00798   loss : 1.64404  time : 16.2170 \n",
            "iter : 26400  lr : 0.00797   loss : 1.77582  time : 16.2258 \n",
            "iter : 26500  lr : 0.00796   loss : 1.80829  time : 16.2459 \n",
            "iter : 26600  lr : 0.00795   loss : 1.71300  time : 16.2361 \n",
            "iter : 26700  lr : 0.00795   loss : 1.46320  time : 16.2331 \n",
            "iter : 26800  lr : 0.00794   loss : 1.42156  time : 16.2300 \n",
            "iter : 26900  lr : 0.00793   loss : 1.83345  time : 16.2291 \n",
            "iter : 27000  lr : 0.00792   loss : 1.62932  time : 16.2363 \n",
            "iter : 27100  lr : 0.00792   loss : 2.12219  time : 16.2328 \n",
            "iter : 27200  lr : 0.00791   loss : 1.50488  time : 16.2335 \n",
            "iter : 27300  lr : 0.00790   loss : 1.92939  time : 16.2331 \n",
            "iter : 27400  lr : 0.00789   loss : 2.18740  time : 16.2350 \n",
            "iter : 27500  lr : 0.00788   loss : 1.43829  time : 16.2437 \n",
            "iter : 27600  lr : 0.00788   loss : 1.67611  time : 16.2274 \n",
            "iter : 27700  lr : 0.00787   loss : 1.70448  time : 16.2304 \n",
            "iter : 27800  lr : 0.00786   loss : 1.77252  time : 16.2219 \n",
            "iter : 27900  lr : 0.00785   loss : 1.46022  time : 16.2301 \n",
            "iter : 28000  lr : 0.00785   loss : 1.54972  time : 16.2324 \n",
            "iter : 28100  lr : 0.00784   loss : 1.94574  time : 16.2437 \n",
            "iter : 28200  lr : 0.00783   loss : 1.55486  time : 16.2823 \n",
            "iter : 28300  lr : 0.00782   loss : 1.83311  time : 16.3535 \n",
            "iter : 28400  lr : 0.00782   loss : 1.75696  time : 16.3321 \n",
            "iter : 28500  lr : 0.00781   loss : 1.69532  time : 16.3195 \n",
            "iter : 28600  lr : 0.00780   loss : 1.49337  time : 16.2707 \n",
            "iter : 28700  lr : 0.00779   loss : 1.87224  time : 16.2592 \n",
            "iter : 28800  lr : 0.00778   loss : 1.63779  time : 16.2414 \n",
            "iter : 28900  lr : 0.00778   loss : 1.53889  time : 16.2467 \n",
            "iter : 29000  lr : 0.00777   loss : 1.39492  time : 16.2432 \n",
            "iter : 29100  lr : 0.00776   loss : 2.06225  time : 16.2324 \n",
            "iter : 29200  lr : 0.00775   loss : 1.64255  time : 16.2196 \n",
            "iter : 29300  lr : 0.00775   loss : 1.75761  time : 16.2249 \n",
            "iter : 29400  lr : 0.00774   loss : 1.49792  time : 16.2497 \n",
            "iter : 29500  lr : 0.00773   loss : 1.56527  time : 16.2938 \n",
            "iter : 29600  lr : 0.00772   loss : 1.74939  time : 16.3241 \n",
            "iter : 29700  lr : 0.00772   loss : 1.52100  time : 16.3450 \n",
            "iter : 29800  lr : 0.00771   loss : 2.03822  time : 16.3564 \n",
            "iter : 29900  lr : 0.00770   loss : 1.67082  time : 16.3502 \n",
            "iter : 30000  lr : 0.00769   loss : 1.85571  time : 16.3591 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 56.740000    top5 : 81.240000 \n",
            "\n",
            "iter : 30100  lr : 0.00768   loss : 1.72016  time : 146.5442 \n",
            "iter : 30200  lr : 0.00768   loss : 2.04030  time : 16.3441 \n",
            "iter : 30300  lr : 0.00767   loss : 1.94128  time : 16.3496 \n",
            "iter : 30400  lr : 0.00766   loss : 2.08157  time : 16.3434 \n",
            "iter : 30500  lr : 0.00765   loss : 1.50159  time : 16.3515 \n",
            "iter : 30600  lr : 0.00765   loss : 1.32656  time : 16.3472 \n",
            "iter : 30700  lr : 0.00764   loss : 1.86228  time : 16.3322 \n",
            "iter : 30800  lr : 0.00763   loss : 1.41008  time : 16.3369 \n",
            "iter : 30900  lr : 0.00762   loss : 1.54679  time : 16.2910 \n",
            "iter : 31000  lr : 0.00762   loss : 1.80811  time : 16.2832 \n",
            "iter : 31100  lr : 0.00761   loss : 1.40990  time : 16.3279 \n",
            "iter : 31200  lr : 0.00760   loss : 1.10910  time : 16.3436 \n",
            "iter : 31300  lr : 0.00759   loss : 1.39175  time : 16.3190 \n",
            "iter : 31400  lr : 0.00758   loss : 1.12115  time : 16.3161 \n",
            "iter : 31500  lr : 0.00758   loss : 1.38974  time : 16.3131 \n",
            "iter : 31600  lr : 0.00757   loss : 1.74466  time : 16.3412 \n",
            "iter : 31700  lr : 0.00756   loss : 1.81124  time : 16.3280 \n",
            "iter : 31800  lr : 0.00755   loss : 1.48483  time : 16.3383 \n",
            "iter : 31900  lr : 0.00755   loss : 1.55233  time : 16.3060 \n",
            "iter : 32000  lr : 0.00754   loss : 1.26673  time : 16.3081 \n",
            "iter : 32100  lr : 0.00753   loss : 1.49408  time : 16.2676 \n",
            "iter : 32200  lr : 0.00752   loss : 1.44183  time : 16.2714 \n",
            "iter : 32300  lr : 0.00752   loss : 1.31916  time : 16.3056 \n",
            "iter : 32400  lr : 0.00751   loss : 1.62855  time : 16.3208 \n",
            "iter : 32500  lr : 0.00750   loss : 1.58254  time : 16.3314 \n",
            "iter : 32600  lr : 0.00749   loss : 1.38417  time : 16.3624 \n",
            "iter : 32700  lr : 0.00748   loss : 1.61389  time : 16.2802 \n",
            "iter : 32800  lr : 0.00748   loss : 1.43096  time : 16.2703 \n",
            "iter : 32900  lr : 0.00747   loss : 1.80767  time : 16.3491 \n",
            "iter : 33000  lr : 0.00746   loss : 1.49012  time : 16.3330 \n",
            "iter : 33100  lr : 0.00745   loss : 1.27863  time : 16.3276 \n",
            "iter : 33200  lr : 0.00745   loss : 1.63951  time : 16.3084 \n",
            "iter : 33300  lr : 0.00744   loss : 1.75189  time : 16.3054 \n",
            "iter : 33400  lr : 0.00743   loss : 1.40923  time : 16.2784 \n",
            "iter : 33500  lr : 0.00742   loss : 1.72567  time : 16.2692 \n",
            "iter : 33600  lr : 0.00742   loss : 1.78036  time : 16.2845 \n",
            "iter : 33700  lr : 0.00741   loss : 1.42185  time : 16.2729 \n",
            "iter : 33800  lr : 0.00740   loss : 1.54851  time : 16.2675 \n",
            "iter : 33900  lr : 0.00739   loss : 1.07444  time : 16.2828 \n",
            "iter : 34000  lr : 0.00738   loss : 1.44373  time : 16.2749 \n",
            "iter : 34100  lr : 0.00738   loss : 1.09820  time : 16.2936 \n",
            "iter : 34200  lr : 0.00737   loss : 1.22867  time : 16.2679 \n",
            "iter : 34300  lr : 0.00736   loss : 1.37263  time : 16.2805 \n",
            "iter : 34400  lr : 0.00735   loss : 1.61418  time : 16.2676 \n",
            "iter : 34500  lr : 0.00735   loss : 1.18320  time : 16.2576 \n",
            "iter : 34600  lr : 0.00734   loss : 1.36281  time : 16.2733 \n",
            "iter : 34700  lr : 0.00733   loss : 1.23567  time : 16.2774 \n",
            "iter : 34800  lr : 0.00732   loss : 1.35528  time : 16.3128 \n",
            "iter : 34900  lr : 0.00732   loss : 1.26284  time : 16.2913 \n",
            "iter : 35000  lr : 0.00731   loss : 1.36809  time : 16.2634 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 60.430000    top5 : 83.570000 \n",
            "\n",
            "iter : 35100  lr : 0.00730   loss : 1.52171  time : 145.1456 \n",
            "iter : 35200  lr : 0.00729   loss : 1.62588  time : 16.2500 \n",
            "iter : 35300  lr : 0.00728   loss : 1.61021  time : 16.2671 \n",
            "iter : 35400  lr : 0.00728   loss : 1.91762  time : 16.2840 \n",
            "iter : 35500  lr : 0.00727   loss : 1.47156  time : 16.2758 \n",
            "iter : 35600  lr : 0.00726   loss : 1.69109  time : 16.2860 \n",
            "iter : 35700  lr : 0.00725   loss : 1.09347  time : 16.2808 \n",
            "iter : 35800  lr : 0.00725   loss : 1.36974  time : 16.2603 \n",
            "iter : 35900  lr : 0.00724   loss : 1.97727  time : 16.2797 \n",
            "iter : 36000  lr : 0.00723   loss : 1.12288  time : 16.2700 \n",
            "iter : 36100  lr : 0.00722   loss : 1.56655  time : 16.2690 \n",
            "iter : 36200  lr : 0.00722   loss : 1.43999  time : 16.2671 \n",
            "iter : 36300  lr : 0.00721   loss : 1.58228  time : 16.2802 \n",
            "iter : 36400  lr : 0.00720   loss : 1.18089  time : 16.2837 \n",
            "iter : 36500  lr : 0.00719   loss : 1.62384  time : 16.2974 \n",
            "iter : 36600  lr : 0.00718   loss : 1.56710  time : 16.2759 \n",
            "iter : 36700  lr : 0.00718   loss : 1.21686  time : 16.2781 \n",
            "iter : 36800  lr : 0.00717   loss : 1.45575  time : 16.2704 \n",
            "iter : 36900  lr : 0.00716   loss : 1.65752  time : 16.2777 \n",
            "iter : 37000  lr : 0.00715   loss : 1.39863  time : 16.2776 \n",
            "iter : 37100  lr : 0.00715   loss : 1.34868  time : 16.3083 \n",
            "iter : 37200  lr : 0.00714   loss : 1.21900  time : 16.2967 \n",
            "iter : 37300  lr : 0.00713   loss : 1.43770  time : 16.2971 \n",
            "iter : 37400  lr : 0.00712   loss : 1.36922  time : 16.2996 \n",
            "iter : 37500  lr : 0.00712   loss : 1.29066  time : 16.3161 \n",
            "iter : 37600  lr : 0.00711   loss : 1.67620  time : 16.2971 \n",
            "iter : 37700  lr : 0.00710   loss : 1.25983  time : 16.2774 \n",
            "iter : 37800  lr : 0.00709   loss : 1.34926  time : 16.2964 \n",
            "iter : 37900  lr : 0.00708   loss : 1.47698  time : 16.2872 \n",
            "iter : 38000  lr : 0.00708   loss : 1.18185  time : 16.2935 \n",
            "iter : 38100  lr : 0.00707   loss : 1.27734  time : 16.2960 \n",
            "iter : 38200  lr : 0.00706   loss : 1.15665  time : 16.2890 \n",
            "iter : 38300  lr : 0.00705   loss : 1.27036  time : 16.2912 \n",
            "iter : 38400  lr : 0.00705   loss : 1.30966  time : 16.3183 \n",
            "iter : 38500  lr : 0.00704   loss : 1.24822  time : 16.2954 \n",
            "iter : 38600  lr : 0.00703   loss : 1.24056  time : 16.3241 \n",
            "iter : 38700  lr : 0.00702   loss : 1.44966  time : 16.2873 \n",
            "iter : 38800  lr : 0.00702   loss : 1.80293  time : 16.3055 \n",
            "iter : 38900  lr : 0.00701   loss : 1.48595  time : 16.2770 \n",
            "iter : 39000  lr : 0.00700   loss : 1.51227  time : 16.2813 \n",
            "iter : 39100  lr : 0.00699   loss : 1.26966  time : 16.3089 \n",
            "iter : 39200  lr : 0.00698   loss : 1.35143  time : 16.3051 \n",
            "iter : 39300  lr : 0.00698   loss : 1.49427  time : 16.2848 \n",
            "iter : 39400  lr : 0.00697   loss : 1.37539  time : 16.2824 \n",
            "iter : 39500  lr : 0.00696   loss : 1.56281  time : 16.2908 \n",
            "iter : 39600  lr : 0.00695   loss : 1.29103  time : 16.2846 \n",
            "iter : 39700  lr : 0.00695   loss : 1.81874  time : 16.3188 \n",
            "iter : 39800  lr : 0.00694   loss : 1.76895  time : 16.2762 \n",
            "iter : 39900  lr : 0.00693   loss : 1.43425  time : 16.2993 \n",
            "iter : 40000  lr : 0.00692   loss : 1.06332  time : 16.2940 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 59.890000    top5 : 83.070000 \n",
            "\n",
            "iter : 40100  lr : 0.00692   loss : 1.33827  time : 145.7795 \n",
            "iter : 40200  lr : 0.00691   loss : 0.98765  time : 16.2968 \n",
            "iter : 40300  lr : 0.00690   loss : 1.25042  time : 16.3083 \n",
            "iter : 40400  lr : 0.00689   loss : 1.53614  time : 16.2867 \n",
            "iter : 40500  lr : 0.00688   loss : 1.16797  time : 16.2947 \n",
            "iter : 40600  lr : 0.00688   loss : 1.57680  time : 16.2859 \n",
            "iter : 40700  lr : 0.00687   loss : 1.06344  time : 16.3005 \n",
            "iter : 40800  lr : 0.00686   loss : 1.32548  time : 16.2938 \n",
            "iter : 40900  lr : 0.00685   loss : 1.53128  time : 16.2929 \n",
            "iter : 41000  lr : 0.00685   loss : 1.57728  time : 16.3278 \n",
            "iter : 41100  lr : 0.00684   loss : 1.37179  time : 16.3125 \n",
            "iter : 41200  lr : 0.00683   loss : 1.11863  time : 16.3077 \n",
            "iter : 41300  lr : 0.00682   loss : 1.05663  time : 16.2985 \n",
            "iter : 41400  lr : 0.00682   loss : 0.99213  time : 16.3119 \n",
            "iter : 41500  lr : 0.00681   loss : 1.05239  time : 16.2967 \n",
            "iter : 41600  lr : 0.00680   loss : 1.32689  time : 16.3010 \n",
            "iter : 41700  lr : 0.00679   loss : 1.27083  time : 16.2956 \n",
            "iter : 41800  lr : 0.00678   loss : 1.30354  time : 16.3064 \n",
            "iter : 41900  lr : 0.00678   loss : 1.35571  time : 16.3101 \n",
            "iter : 42000  lr : 0.00677   loss : 1.26203  time : 16.2983 \n",
            "iter : 42100  lr : 0.00676   loss : 1.54350  time : 16.3066 \n",
            "iter : 42200  lr : 0.00675   loss : 1.32189  time : 16.3125 \n",
            "iter : 42300  lr : 0.00675   loss : 1.21985  time : 16.2996 \n",
            "iter : 42400  lr : 0.00674   loss : 0.99487  time : 16.3000 \n",
            "iter : 42500  lr : 0.00673   loss : 1.31730  time : 16.2936 \n",
            "iter : 42600  lr : 0.00672   loss : 1.14042  time : 16.2990 \n",
            "iter : 42700  lr : 0.00672   loss : 1.27998  time : 16.3041 \n",
            "iter : 42800  lr : 0.00671   loss : 1.51531  time : 16.2887 \n",
            "iter : 42900  lr : 0.00670   loss : 1.16780  time : 16.2926 \n",
            "iter : 43000  lr : 0.00669   loss : 1.33361  time : 16.2990 \n",
            "iter : 43100  lr : 0.00668   loss : 1.59282  time : 16.3053 \n",
            "iter : 43200  lr : 0.00668   loss : 1.21386  time : 16.3076 \n",
            "iter : 43300  lr : 0.00667   loss : 1.34277  time : 16.3041 \n",
            "iter : 43400  lr : 0.00666   loss : 1.10142  time : 16.2972 \n",
            "iter : 43500  lr : 0.00665   loss : 1.09448  time : 16.2895 \n",
            "iter : 43600  lr : 0.00665   loss : 1.18174  time : 16.2926 \n",
            "iter : 43700  lr : 0.00664   loss : 1.28546  time : 16.2897 \n",
            "iter : 43800  lr : 0.00663   loss : 1.40014  time : 16.2862 \n",
            "iter : 43900  lr : 0.00662   loss : 1.25157  time : 16.2874 \n",
            "iter : 44000  lr : 0.00662   loss : 1.17329  time : 16.2933 \n",
            "iter : 44100  lr : 0.00661   loss : 1.19378  time : 16.2910 \n",
            "iter : 44200  lr : 0.00660   loss : 0.86691  time : 16.2937 \n",
            "iter : 44300  lr : 0.00659   loss : 1.33395  time : 16.3052 \n",
            "iter : 44400  lr : 0.00658   loss : 1.25701  time : 16.3003 \n",
            "iter : 44500  lr : 0.00658   loss : 1.38701  time : 16.2913 \n",
            "iter : 44600  lr : 0.00657   loss : 1.28649  time : 16.3039 \n",
            "iter : 44700  lr : 0.00656   loss : 0.92733  time : 16.2870 \n",
            "iter : 44800  lr : 0.00655   loss : 1.14716  time : 16.2863 \n",
            "iter : 44900  lr : 0.00655   loss : 1.09986  time : 16.3043 \n",
            "iter : 45000  lr : 0.00654   loss : 1.31777  time : 16.2994 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 62.270000    top5 : 85.420000 \n",
            "\n",
            "iter : 45100  lr : 0.00653   loss : 1.34806  time : 145.6332 \n",
            "iter : 45200  lr : 0.00652   loss : 1.17086  time : 16.2842 \n",
            "iter : 45300  lr : 0.00652   loss : 1.39377  time : 16.3007 \n",
            "iter : 45400  lr : 0.00651   loss : 1.19307  time : 16.2979 \n",
            "iter : 45500  lr : 0.00650   loss : 0.97408  time : 16.3006 \n",
            "iter : 45600  lr : 0.00649   loss : 1.09370  time : 16.3053 \n",
            "iter : 45700  lr : 0.00648   loss : 1.31344  time : 16.3110 \n",
            "iter : 45800  lr : 0.00648   loss : 1.45103  time : 16.3011 \n",
            "iter : 45900  lr : 0.00647   loss : 1.28389  time : 16.2941 \n",
            "iter : 46000  lr : 0.00646   loss : 1.35254  time : 16.2973 \n",
            "iter : 46100  lr : 0.00645   loss : 1.20940  time : 16.2925 \n",
            "iter : 46200  lr : 0.00645   loss : 1.01570  time : 16.2964 \n",
            "iter : 46300  lr : 0.00644   loss : 0.98724  time : 16.2888 \n",
            "iter : 46400  lr : 0.00643   loss : 1.16638  time : 16.2977 \n",
            "iter : 46500  lr : 0.00642   loss : 1.13334  time : 16.2895 \n",
            "iter : 46600  lr : 0.00642   loss : 1.07120  time : 16.2918 \n",
            "iter : 46700  lr : 0.00641   loss : 1.01954  time : 16.2949 \n",
            "iter : 46800  lr : 0.00640   loss : 1.61862  time : 16.3040 \n",
            "iter : 46900  lr : 0.00639   loss : 1.02186  time : 16.2975 \n",
            "iter : 47000  lr : 0.00638   loss : 0.96842  time : 16.2910 \n",
            "iter : 47100  lr : 0.00638   loss : 1.59009  time : 16.3023 \n",
            "iter : 47200  lr : 0.00637   loss : 1.34136  time : 16.2976 \n",
            "iter : 47300  lr : 0.00636   loss : 1.12285  time : 16.3066 \n",
            "iter : 47400  lr : 0.00635   loss : 1.08386  time : 16.3019 \n",
            "iter : 47500  lr : 0.00635   loss : 1.14685  time : 16.3086 \n",
            "iter : 47600  lr : 0.00634   loss : 1.22905  time : 16.3018 \n",
            "iter : 47700  lr : 0.00633   loss : 1.34287  time : 16.3015 \n",
            "iter : 47800  lr : 0.00632   loss : 1.04868  time : 16.2910 \n",
            "iter : 47900  lr : 0.00632   loss : 1.23955  time : 16.3194 \n",
            "iter : 48000  lr : 0.00631   loss : 1.34634  time : 16.3044 \n",
            "iter : 48100  lr : 0.00630   loss : 1.22134  time : 16.3114 \n",
            "iter : 48200  lr : 0.00629   loss : 1.16113  time : 16.3124 \n",
            "iter : 48300  lr : 0.00628   loss : 0.97977  time : 16.2952 \n",
            "iter : 48400  lr : 0.00628   loss : 0.98809  time : 16.3065 \n",
            "iter : 48500  lr : 0.00627   loss : 0.83548  time : 16.2973 \n",
            "iter : 48600  lr : 0.00626   loss : 1.49561  time : 16.3050 \n",
            "iter : 48700  lr : 0.00625   loss : 1.56407  time : 16.3068 \n",
            "iter : 48800  lr : 0.00625   loss : 1.03624  time : 16.2884 \n",
            "iter : 48900  lr : 0.00624   loss : 1.15149  time : 16.3080 \n",
            "iter : 49000  lr : 0.00623   loss : 1.45844  time : 16.3046 \n",
            "iter : 49100  lr : 0.00622   loss : 1.03420  time : 16.3145 \n",
            "iter : 49200  lr : 0.00622   loss : 0.90533  time : 16.2980 \n",
            "iter : 49300  lr : 0.00621   loss : 1.34618  time : 16.3036 \n",
            "iter : 49400  lr : 0.00620   loss : 1.13179  time : 16.3176 \n",
            "iter : 49500  lr : 0.00619   loss : 1.40784  time : 16.3013 \n",
            "iter : 49600  lr : 0.00618   loss : 1.24836  time : 16.2891 \n",
            "iter : 49700  lr : 0.00618   loss : 1.26891  time : 16.2994 \n",
            "iter : 49800  lr : 0.00617   loss : 1.46865  time : 16.2988 \n",
            "iter : 49900  lr : 0.00616   loss : 0.90278  time : 16.2825 \n",
            "iter : 50000  lr : 0.00615   loss : 0.96691  time : 16.2954 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 63.420000    top5 : 85.510000 \n",
            "\n",
            "iter : 50100  lr : 0.00615   loss : 1.48807  time : 145.8300 \n",
            "iter : 50200  lr : 0.00614   loss : 1.13443  time : 16.3027 \n",
            "iter : 50300  lr : 0.00613   loss : 1.09173  time : 16.2982 \n",
            "iter : 50400  lr : 0.00612   loss : 1.40505  time : 16.2887 \n",
            "iter : 50500  lr : 0.00612   loss : 1.29725  time : 16.3126 \n",
            "iter : 50600  lr : 0.00611   loss : 0.93416  time : 16.3036 \n",
            "iter : 50700  lr : 0.00610   loss : 1.07037  time : 16.3051 \n",
            "iter : 50800  lr : 0.00609   loss : 0.95266  time : 16.3007 \n",
            "iter : 50900  lr : 0.00608   loss : 1.26551  time : 16.3136 \n",
            "iter : 51000  lr : 0.00608   loss : 1.59327  time : 16.3080 \n",
            "iter : 51100  lr : 0.00607   loss : 1.17299  time : 16.3020 \n",
            "iter : 51200  lr : 0.00606   loss : 1.14726  time : 16.3016 \n",
            "iter : 51300  lr : 0.00605   loss : 0.99153  time : 16.3061 \n",
            "iter : 51400  lr : 0.00605   loss : 0.80969  time : 16.2961 \n",
            "iter : 51500  lr : 0.00604   loss : 1.36579  time : 16.3083 \n",
            "iter : 51600  lr : 0.00603   loss : 0.99846  time : 16.3233 \n",
            "iter : 51700  lr : 0.00602   loss : 0.84527  time : 16.3020 \n",
            "iter : 51800  lr : 0.00602   loss : 1.25625  time : 16.2893 \n",
            "iter : 51900  lr : 0.00601   loss : 0.98868  time : 16.3021 \n",
            "iter : 52000  lr : 0.00600   loss : 0.75104  time : 16.2977 \n",
            "iter : 52100  lr : 0.00599   loss : 1.49207  time : 16.3066 \n",
            "iter : 52200  lr : 0.00598   loss : 0.83562  time : 16.3037 \n",
            "iter : 52300  lr : 0.00598   loss : 0.97069  time : 16.3029 \n",
            "iter : 52400  lr : 0.00597   loss : 0.80737  time : 16.3087 \n",
            "iter : 52500  lr : 0.00596   loss : 0.97255  time : 16.2962 \n",
            "iter : 52600  lr : 0.00595   loss : 0.90018  time : 16.2928 \n",
            "iter : 52700  lr : 0.00595   loss : 1.14045  time : 16.3198 \n",
            "iter : 52800  lr : 0.00594   loss : 0.89612  time : 16.2974 \n",
            "iter : 52900  lr : 0.00593   loss : 1.08091  time : 16.2991 \n",
            "iter : 53000  lr : 0.00592   loss : 0.94215  time : 16.3012 \n",
            "iter : 53100  lr : 0.00592   loss : 0.94526  time : 16.3070 \n",
            "iter : 53200  lr : 0.00591   loss : 1.41780  time : 16.3062 \n",
            "iter : 53300  lr : 0.00590   loss : 1.03439  time : 16.3015 \n",
            "iter : 53400  lr : 0.00589   loss : 1.13953  time : 16.3061 \n",
            "iter : 53500  lr : 0.00588   loss : 0.91564  time : 16.3011 \n",
            "iter : 53600  lr : 0.00588   loss : 1.00575  time : 16.2973 \n",
            "iter : 53700  lr : 0.00587   loss : 0.91828  time : 16.3213 \n",
            "iter : 53800  lr : 0.00586   loss : 0.91552  time : 16.3204 \n",
            "iter : 53900  lr : 0.00585   loss : 1.24916  time : 16.3066 \n",
            "iter : 54000  lr : 0.00585   loss : 0.66382  time : 16.2975 \n",
            "iter : 54100  lr : 0.00584   loss : 1.41983  time : 16.2991 \n",
            "iter : 54200  lr : 0.00583   loss : 0.79454  time : 16.3004 \n",
            "iter : 54300  lr : 0.00582   loss : 0.66130  time : 16.3040 \n",
            "iter : 54400  lr : 0.00582   loss : 1.29016  time : 16.3122 \n",
            "iter : 54500  lr : 0.00581   loss : 1.15333  time : 16.3055 \n",
            "iter : 54600  lr : 0.00580   loss : 1.23022  time : 16.3069 \n",
            "iter : 54700  lr : 0.00579   loss : 0.96079  time : 16.3201 \n",
            "iter : 54800  lr : 0.00578   loss : 0.86200  time : 16.3099 \n",
            "iter : 54900  lr : 0.00578   loss : 1.10944  time : 16.3225 \n",
            "iter : 55000  lr : 0.00577   loss : 0.94757  time : 16.3013 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 66.530000    top5 : 87.280000 \n",
            "\n",
            "iter : 55100  lr : 0.00576   loss : 0.90847  time : 145.7846 \n",
            "iter : 55200  lr : 0.00575   loss : 1.00143  time : 16.3102 \n",
            "iter : 55300  lr : 0.00575   loss : 1.19405  time : 16.2986 \n",
            "iter : 55400  lr : 0.00574   loss : 1.35929  time : 16.3020 \n",
            "iter : 55500  lr : 0.00573   loss : 0.92462  time : 16.3056 \n",
            "iter : 55600  lr : 0.00572   loss : 1.20808  time : 16.2979 \n",
            "iter : 55700  lr : 0.00572   loss : 1.01243  time : 16.3070 \n",
            "iter : 55800  lr : 0.00571   loss : 0.90106  time : 16.3094 \n",
            "iter : 55900  lr : 0.00570   loss : 1.20247  time : 16.3051 \n",
            "iter : 56000  lr : 0.00569   loss : 1.08874  time : 16.3141 \n",
            "iter : 56100  lr : 0.00568   loss : 0.81752  time : 16.3093 \n",
            "iter : 56200  lr : 0.00568   loss : 0.76941  time : 16.3005 \n",
            "iter : 56300  lr : 0.00567   loss : 1.32356  time : 16.3283 \n",
            "iter : 56400  lr : 0.00566   loss : 0.82281  time : 16.3059 \n",
            "iter : 56500  lr : 0.00565   loss : 0.90020  time : 16.3187 \n",
            "iter : 56600  lr : 0.00565   loss : 0.84861  time : 16.3016 \n",
            "iter : 56700  lr : 0.00564   loss : 0.80341  time : 16.3148 \n",
            "iter : 56800  lr : 0.00563   loss : 0.80148  time : 16.3112 \n",
            "iter : 56900  lr : 0.00562   loss : 0.85522  time : 16.3063 \n",
            "iter : 57000  lr : 0.00562   loss : 1.17249  time : 16.2990 \n",
            "iter : 57100  lr : 0.00561   loss : 0.90724  time : 16.3073 \n",
            "iter : 57200  lr : 0.00560   loss : 1.09952  time : 16.3152 \n",
            "iter : 57300  lr : 0.00559   loss : 0.67892  time : 16.3151 \n",
            "iter : 57400  lr : 0.00558   loss : 1.23203  time : 16.3160 \n",
            "iter : 57500  lr : 0.00558   loss : 1.06251  time : 16.3027 \n",
            "iter : 57600  lr : 0.00557   loss : 0.97390  time : 16.3029 \n",
            "iter : 57700  lr : 0.00556   loss : 0.95830  time : 16.2968 \n",
            "iter : 57800  lr : 0.00555   loss : 0.80338  time : 16.3073 \n",
            "iter : 57900  lr : 0.00555   loss : 1.05792  time : 16.2929 \n",
            "iter : 58000  lr : 0.00554   loss : 0.90616  time : 16.3181 \n",
            "iter : 58100  lr : 0.00553   loss : 0.72061  time : 16.3115 \n",
            "iter : 58200  lr : 0.00552   loss : 1.04208  time : 16.3019 \n",
            "iter : 58300  lr : 0.00552   loss : 1.11268  time : 16.3126 \n",
            "iter : 58400  lr : 0.00551   loss : 0.85228  time : 16.3128 \n",
            "iter : 58500  lr : 0.00550   loss : 0.77944  time : 16.3127 \n",
            "iter : 58600  lr : 0.00549   loss : 0.81369  time : 16.3115 \n",
            "iter : 58700  lr : 0.00548   loss : 1.35134  time : 16.3036 \n",
            "iter : 58800  lr : 0.00548   loss : 1.35775  time : 16.3159 \n",
            "iter : 58900  lr : 0.00547   loss : 0.89207  time : 16.3263 \n",
            "iter : 59000  lr : 0.00546   loss : 0.75307  time : 16.3117 \n",
            "iter : 59100  lr : 0.00545   loss : 0.83587  time : 16.3009 \n",
            "iter : 59200  lr : 0.00545   loss : 1.29464  time : 16.3146 \n",
            "iter : 59300  lr : 0.00544   loss : 1.13169  time : 16.3106 \n",
            "iter : 59400  lr : 0.00543   loss : 1.08255  time : 16.3013 \n",
            "iter : 59500  lr : 0.00542   loss : 1.20961  time : 16.3083 \n",
            "iter : 59600  lr : 0.00542   loss : 1.38780  time : 16.3266 \n",
            "iter : 59700  lr : 0.00541   loss : 1.06189  time : 16.3155 \n",
            "iter : 59800  lr : 0.00540   loss : 1.12911  time : 16.3217 \n",
            "iter : 59900  lr : 0.00539   loss : 0.83352  time : 16.3011 \n",
            "iter : 60000  lr : 0.00538   loss : 0.72059  time : 16.3098 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 66.380000    top5 : 87.650000 \n",
            "\n",
            "iter : 60100  lr : 0.00538   loss : 0.74791  time : 145.6326 \n",
            "iter : 60200  lr : 0.00537   loss : 1.11622  time : 16.3150 \n",
            "iter : 60300  lr : 0.00536   loss : 0.84485  time : 16.3057 \n",
            "iter : 60400  lr : 0.00535   loss : 0.86772  time : 16.3243 \n",
            "iter : 60500  lr : 0.00535   loss : 1.04892  time : 16.3107 \n",
            "iter : 60600  lr : 0.00534   loss : 1.08320  time : 16.3101 \n",
            "iter : 60700  lr : 0.00533   loss : 0.79883  time : 16.3055 \n",
            "iter : 60800  lr : 0.00532   loss : 0.85408  time : 16.3323 \n",
            "iter : 60900  lr : 0.00532   loss : 1.03313  time : 16.3126 \n",
            "iter : 61000  lr : 0.00531   loss : 0.76020  time : 16.3215 \n",
            "iter : 61100  lr : 0.00530   loss : 1.04849  time : 16.3368 \n",
            "iter : 61200  lr : 0.00529   loss : 0.76980  time : 16.3236 \n",
            "iter : 61300  lr : 0.00528   loss : 0.68512  time : 16.3200 \n",
            "iter : 61400  lr : 0.00528   loss : 0.80299  time : 16.3121 \n",
            "iter : 61500  lr : 0.00527   loss : 0.92236  time : 16.3226 \n",
            "iter : 61600  lr : 0.00526   loss : 1.14302  time : 16.3193 \n",
            "iter : 61700  lr : 0.00525   loss : 0.79299  time : 16.3138 \n",
            "iter : 61800  lr : 0.00525   loss : 1.13393  time : 16.3159 \n",
            "iter : 61900  lr : 0.00524   loss : 0.91195  time : 16.3164 \n",
            "iter : 62000  lr : 0.00523   loss : 0.85918  time : 16.3021 \n",
            "iter : 62100  lr : 0.00522   loss : 1.05611  time : 16.3059 \n",
            "iter : 62200  lr : 0.00522   loss : 1.03582  time : 16.3251 \n",
            "iter : 62300  lr : 0.00521   loss : 0.79466  time : 16.3035 \n",
            "iter : 62400  lr : 0.00520   loss : 0.76462  time : 16.3114 \n",
            "iter : 62500  lr : 0.00519   loss : 0.94255  time : 16.2988 \n",
            "iter : 62600  lr : 0.00518   loss : 0.91313  time : 16.3193 \n",
            "iter : 62700  lr : 0.00518   loss : 0.99124  time : 16.3159 \n",
            "iter : 62800  lr : 0.00517   loss : 0.84638  time : 16.3182 \n",
            "iter : 62900  lr : 0.00516   loss : 0.97327  time : 16.3132 \n",
            "iter : 63000  lr : 0.00515   loss : 0.74037  time : 16.3131 \n",
            "iter : 63100  lr : 0.00515   loss : 1.20400  time : 16.3050 \n",
            "iter : 63200  lr : 0.00514   loss : 1.11076  time : 16.3168 \n",
            "iter : 63300  lr : 0.00513   loss : 0.67589  time : 16.3258 \n",
            "iter : 63400  lr : 0.00512   loss : 1.07828  time : 16.3087 \n",
            "iter : 63500  lr : 0.00512   loss : 0.67694  time : 16.3010 \n",
            "iter : 63600  lr : 0.00511   loss : 0.61896  time : 16.2995 \n",
            "iter : 63700  lr : 0.00510   loss : 1.06379  time : 16.2981 \n",
            "iter : 63800  lr : 0.00509   loss : 0.86600  time : 16.3043 \n",
            "iter : 63900  lr : 0.00508   loss : 0.81034  time : 16.3040 \n",
            "iter : 64000  lr : 0.00508   loss : 0.96519  time : 16.3129 \n",
            "iter : 64100  lr : 0.00507   loss : 0.79051  time : 16.3038 \n",
            "iter : 64200  lr : 0.00506   loss : 1.06574  time : 16.3140 \n",
            "iter : 64300  lr : 0.00505   loss : 1.04870  time : 16.3141 \n",
            "iter : 64400  lr : 0.00505   loss : 0.98173  time : 16.3135 \n",
            "iter : 64500  lr : 0.00504   loss : 0.52563  time : 16.3278 \n",
            "iter : 64600  lr : 0.00503   loss : 0.88479  time : 16.3210 \n",
            "iter : 64700  lr : 0.00502   loss : 1.01841  time : 16.3299 \n",
            "iter : 64800  lr : 0.00502   loss : 0.81340  time : 16.3197 \n",
            "iter : 64900  lr : 0.00501   loss : 0.58750  time : 16.3355 \n",
            "iter : 65000  lr : 0.00500   loss : 0.83069  time : 16.3154 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 66.440000    top5 : 87.460000 \n",
            "\n",
            "iter : 65100  lr : 0.00499   loss : 0.71239  time : 145.8014 \n",
            "iter : 65200  lr : 0.00498   loss : 0.98444  time : 16.3163 \n",
            "iter : 65300  lr : 0.00498   loss : 0.86068  time : 16.3420 \n",
            "iter : 65400  lr : 0.00497   loss : 0.90180  time : 16.3377 \n",
            "iter : 65500  lr : 0.00496   loss : 0.72965  time : 16.3555 \n",
            "iter : 65600  lr : 0.00495   loss : 1.21348  time : 16.3527 \n",
            "iter : 65700  lr : 0.00495   loss : 0.98472  time : 16.3215 \n",
            "iter : 65800  lr : 0.00494   loss : 1.08429  time : 16.3077 \n",
            "iter : 65900  lr : 0.00493   loss : 0.97285  time : 16.3218 \n",
            "iter : 66000  lr : 0.00492   loss : 0.65432  time : 16.3267 \n",
            "iter : 66100  lr : 0.00492   loss : 0.71078  time : 16.3112 \n",
            "iter : 66200  lr : 0.00491   loss : 0.83382  time : 16.3215 \n",
            "iter : 66300  lr : 0.00490   loss : 0.68057  time : 16.3098 \n",
            "iter : 66400  lr : 0.00489   loss : 0.56000  time : 16.2944 \n",
            "iter : 66500  lr : 0.00488   loss : 0.72543  time : 16.3225 \n",
            "iter : 66600  lr : 0.00488   loss : 0.98581  time : 16.3491 \n",
            "iter : 66700  lr : 0.00487   loss : 0.87123  time : 16.3445 \n",
            "iter : 66800  lr : 0.00486   loss : 0.96804  time : 16.3467 \n",
            "iter : 66900  lr : 0.00485   loss : 0.63565  time : 16.3507 \n",
            "iter : 67000  lr : 0.00485   loss : 0.78715  time : 16.3620 \n",
            "iter : 67100  lr : 0.00484   loss : 0.79076  time : 16.3664 \n",
            "iter : 67200  lr : 0.00483   loss : 0.78076  time : 16.3394 \n",
            "iter : 67300  lr : 0.00482   loss : 0.77053  time : 16.3406 \n",
            "iter : 67400  lr : 0.00482   loss : 1.07436  time : 16.3341 \n",
            "iter : 67500  lr : 0.00481   loss : 1.01457  time : 16.3429 \n",
            "iter : 67600  lr : 0.00480   loss : 1.00984  time : 16.3605 \n",
            "iter : 67700  lr : 0.00479   loss : 0.66069  time : 16.3340 \n",
            "iter : 67800  lr : 0.00478   loss : 1.01426  time : 16.3201 \n",
            "iter : 67900  lr : 0.00478   loss : 1.07451  time : 16.3242 \n",
            "iter : 68000  lr : 0.00477   loss : 0.80404  time : 16.3028 \n",
            "iter : 68100  lr : 0.00476   loss : 0.73551  time : 16.2835 \n",
            "iter : 68200  lr : 0.00475   loss : 0.74397  time : 16.2603 \n",
            "iter : 68300  lr : 0.00475   loss : 1.06547  time : 16.2759 \n",
            "iter : 68400  lr : 0.00474   loss : 0.74769  time : 16.2541 \n",
            "iter : 68500  lr : 0.00473   loss : 0.66617  time : 16.2622 \n",
            "iter : 68600  lr : 0.00472   loss : 0.99333  time : 16.3127 \n",
            "iter : 68700  lr : 0.00472   loss : 0.92469  time : 16.3747 \n",
            "iter : 68800  lr : 0.00471   loss : 1.11548  time : 16.3681 \n",
            "iter : 68900  lr : 0.00470   loss : 0.62695  time : 16.3759 \n",
            "iter : 69000  lr : 0.00469   loss : 0.72416  time : 16.3581 \n",
            "iter : 69100  lr : 0.00468   loss : 0.81710  time : 16.3205 \n",
            "iter : 69200  lr : 0.00468   loss : 0.82433  time : 16.3029 \n",
            "iter : 69300  lr : 0.00467   loss : 1.10275  time : 16.3027 \n",
            "iter : 69400  lr : 0.00466   loss : 0.69391  time : 16.3642 \n",
            "iter : 69500  lr : 0.00465   loss : 1.30207  time : 16.3783 \n",
            "iter : 69600  lr : 0.00465   loss : 0.84101  time : 16.3687 \n",
            "iter : 69700  lr : 0.00464   loss : 0.58506  time : 16.3409 \n",
            "iter : 69800  lr : 0.00463   loss : 0.75514  time : 16.3010 \n",
            "iter : 69900  lr : 0.00462   loss : 0.64869  time : 16.3047 \n",
            "iter : 70000  lr : 0.00462   loss : 1.03287  time : 16.2673 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 68.690000    top5 : 88.250000 \n",
            "\n",
            "iter : 70100  lr : 0.00461   loss : 1.10876  time : 145.5412 \n",
            "iter : 70200  lr : 0.00460   loss : 0.83199  time : 16.2906 \n",
            "iter : 70300  lr : 0.00459   loss : 0.88115  time : 16.2851 \n",
            "iter : 70400  lr : 0.00458   loss : 0.85207  time : 16.2599 \n",
            "iter : 70500  lr : 0.00458   loss : 0.81506  time : 16.2501 \n",
            "iter : 70600  lr : 0.00457   loss : 0.55248  time : 16.2648 \n",
            "iter : 70700  lr : 0.00456   loss : 0.91320  time : 16.2693 \n",
            "iter : 70800  lr : 0.00455   loss : 0.71892  time : 16.2944 \n",
            "iter : 70900  lr : 0.00455   loss : 0.61418  time : 16.3155 \n",
            "iter : 71000  lr : 0.00454   loss : 0.53251  time : 16.2831 \n",
            "iter : 71100  lr : 0.00453   loss : 0.65127  time : 16.3087 \n",
            "iter : 71200  lr : 0.00452   loss : 0.74762  time : 16.2915 \n",
            "iter : 71300  lr : 0.00452   loss : 0.76891  time : 16.2826 \n",
            "iter : 71400  lr : 0.00451   loss : 0.62072  time : 16.2871 \n",
            "iter : 71500  lr : 0.00450   loss : 0.91487  time : 16.2823 \n",
            "iter : 71600  lr : 0.00449   loss : 0.59389  time : 16.2962 \n",
            "iter : 71700  lr : 0.00448   loss : 0.75234  time : 16.3737 \n",
            "iter : 71800  lr : 0.00448   loss : 0.94546  time : 16.3644 \n",
            "iter : 71900  lr : 0.00447   loss : 0.65692  time : 16.3836 \n",
            "iter : 72000  lr : 0.00446   loss : 0.83899  time : 16.3986 \n",
            "iter : 72100  lr : 0.00445   loss : 0.82169  time : 16.4059 \n",
            "iter : 72200  lr : 0.00445   loss : 0.74516  time : 16.3967 \n",
            "iter : 72300  lr : 0.00444   loss : 0.69534  time : 16.3908 \n",
            "iter : 72400  lr : 0.00443   loss : 0.62813  time : 16.4028 \n",
            "iter : 72500  lr : 0.00442   loss : 1.06254  time : 16.3856 \n",
            "iter : 72600  lr : 0.00442   loss : 0.68123  time : 16.3937 \n",
            "iter : 72700  lr : 0.00441   loss : 1.04750  time : 16.3973 \n",
            "iter : 72800  lr : 0.00440   loss : 0.78236  time : 16.3884 \n",
            "iter : 72900  lr : 0.00439   loss : 0.64926  time : 16.3929 \n",
            "iter : 73000  lr : 0.00438   loss : 0.81797  time : 16.3901 \n",
            "iter : 73100  lr : 0.00438   loss : 0.88736  time : 16.3756 \n",
            "iter : 73200  lr : 0.00437   loss : 0.56991  time : 16.3738 \n",
            "iter : 73300  lr : 0.00436   loss : 0.59982  time : 16.3755 \n",
            "iter : 73400  lr : 0.00435   loss : 0.48904  time : 16.3649 \n",
            "iter : 73500  lr : 0.00435   loss : 0.65029  time : 16.3512 \n",
            "iter : 73600  lr : 0.00434   loss : 0.81639  time : 16.3528 \n",
            "iter : 73700  lr : 0.00433   loss : 0.61234  time : 16.3553 \n",
            "iter : 73800  lr : 0.00432   loss : 0.60592  time : 16.3446 \n",
            "iter : 73900  lr : 0.00432   loss : 0.86920  time : 16.3534 \n",
            "iter : 74000  lr : 0.00431   loss : 0.58286  time : 16.3617 \n",
            "iter : 74100  lr : 0.00430   loss : 0.70625  time : 16.3629 \n",
            "iter : 74200  lr : 0.00429   loss : 0.63524  time : 16.3474 \n",
            "iter : 74300  lr : 0.00428   loss : 0.69776  time : 16.3149 \n",
            "iter : 74400  lr : 0.00428   loss : 0.90112  time : 16.2722 \n",
            "iter : 74500  lr : 0.00427   loss : 0.73021  time : 16.2994 \n",
            "iter : 74600  lr : 0.00426   loss : 0.92238  time : 16.2432 \n",
            "iter : 74700  lr : 0.00425   loss : 0.75545  time : 16.2945 \n",
            "iter : 74800  lr : 0.00425   loss : 0.69184  time : 16.3164 \n",
            "iter : 74900  lr : 0.00424   loss : 0.77701  time : 16.3277 \n",
            "iter : 75000  lr : 0.00423   loss : 0.68195  time : 16.3166 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 69.080000    top5 : 88.530000 \n",
            "\n",
            "iter : 75100  lr : 0.00422   loss : 0.87811  time : 145.7687 \n",
            "iter : 75200  lr : 0.00422   loss : 0.83508  time : 16.3034 \n",
            "iter : 75300  lr : 0.00421   loss : 0.69146  time : 16.3064 \n",
            "iter : 75400  lr : 0.00420   loss : 0.55732  time : 16.3042 \n",
            "iter : 75500  lr : 0.00419   loss : 0.50422  time : 16.3000 \n",
            "iter : 75600  lr : 0.00418   loss : 0.71710  time : 16.2917 \n",
            "iter : 75700  lr : 0.00418   loss : 0.55028  time : 16.3066 \n",
            "iter : 75800  lr : 0.00417   loss : 0.53765  time : 16.2582 \n",
            "iter : 75900  lr : 0.00416   loss : 0.96486  time : 16.2520 \n",
            "iter : 76000  lr : 0.00415   loss : 0.89918  time : 16.2475 \n",
            "iter : 76100  lr : 0.00415   loss : 0.74937  time : 16.2701 \n",
            "iter : 76200  lr : 0.00414   loss : 0.83019  time : 16.2669 \n",
            "iter : 76300  lr : 0.00413   loss : 0.86677  time : 16.2645 \n",
            "iter : 76400  lr : 0.00412   loss : 0.83884  time : 16.2443 \n",
            "iter : 76500  lr : 0.00412   loss : 0.47462  time : 16.2438 \n",
            "iter : 76600  lr : 0.00411   loss : 0.66494  time : 16.2594 \n",
            "iter : 76700  lr : 0.00410   loss : 0.72661  time : 16.2480 \n",
            "iter : 76800  lr : 0.00409   loss : 0.92252  time : 16.2556 \n",
            "iter : 76900  lr : 0.00408   loss : 0.53706  time : 16.2568 \n",
            "iter : 77000  lr : 0.00408   loss : 0.63208  time : 16.2609 \n",
            "iter : 77100  lr : 0.00407   loss : 1.12115  time : 16.2389 \n",
            "iter : 77200  lr : 0.00406   loss : 0.47186  time : 16.2519 \n",
            "iter : 77300  lr : 0.00405   loss : 0.67292  time : 16.2808 \n",
            "iter : 77400  lr : 0.00405   loss : 0.76210  time : 16.2544 \n",
            "iter : 77500  lr : 0.00404   loss : 0.96215  time : 16.2558 \n",
            "iter : 77600  lr : 0.00403   loss : 0.55489  time : 16.2559 \n",
            "iter : 77700  lr : 0.00402   loss : 0.90010  time : 16.2582 \n",
            "iter : 77800  lr : 0.00402   loss : 0.77195  time : 16.2579 \n",
            "iter : 77900  lr : 0.00401   loss : 0.80454  time : 16.2548 \n",
            "iter : 78000  lr : 0.00400   loss : 0.35790  time : 16.2602 \n",
            "iter : 78100  lr : 0.00399   loss : 0.61277  time : 16.2543 \n",
            "iter : 78200  lr : 0.00398   loss : 0.64802  time : 16.2561 \n",
            "iter : 78300  lr : 0.00398   loss : 0.51828  time : 16.2698 \n",
            "iter : 78400  lr : 0.00397   loss : 0.95864  time : 16.2577 \n",
            "iter : 78500  lr : 0.00396   loss : 0.90296  time : 16.2689 \n",
            "iter : 78600  lr : 0.00395   loss : 0.69611  time : 16.2513 \n",
            "iter : 78700  lr : 0.00395   loss : 1.04667  time : 16.2957 \n",
            "iter : 78800  lr : 0.00394   loss : 0.77141  time : 16.2683 \n",
            "iter : 78900  lr : 0.00393   loss : 0.52683  time : 16.2692 \n",
            "iter : 79000  lr : 0.00392   loss : 0.75414  time : 16.2549 \n",
            "iter : 79100  lr : 0.00392   loss : 0.54079  time : 16.2353 \n",
            "iter : 79200  lr : 0.00391   loss : 0.69111  time : 16.2254 \n",
            "iter : 79300  lr : 0.00390   loss : 0.66214  time : 16.2559 \n",
            "iter : 79400  lr : 0.00389   loss : 0.60097  time : 16.2345 \n",
            "iter : 79500  lr : 0.00388   loss : 0.74935  time : 16.2705 \n",
            "iter : 79600  lr : 0.00388   loss : 0.48324  time : 16.3011 \n",
            "iter : 79700  lr : 0.00387   loss : 0.98559  time : 16.2969 \n",
            "iter : 79800  lr : 0.00386   loss : 0.56506  time : 16.2409 \n",
            "iter : 79900  lr : 0.00385   loss : 0.64359  time : 16.2406 \n",
            "iter : 80000  lr : 0.00385   loss : 0.70383  time : 16.2382 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 69.610000    top5 : 88.580000 \n",
            "\n",
            "iter : 80100  lr : 0.00384   loss : 0.74438  time : 145.6643 \n",
            "iter : 80200  lr : 0.00383   loss : 0.68105  time : 16.2400 \n",
            "iter : 80300  lr : 0.00382   loss : 0.69650  time : 16.2426 \n",
            "iter : 80400  lr : 0.00382   loss : 0.74100  time : 16.2384 \n",
            "iter : 80500  lr : 0.00381   loss : 0.81929  time : 16.2378 \n",
            "iter : 80600  lr : 0.00380   loss : 0.74670  time : 16.2463 \n",
            "iter : 80700  lr : 0.00379   loss : 0.95800  time : 16.2430 \n",
            "iter : 80800  lr : 0.00378   loss : 0.63833  time : 16.2633 \n",
            "iter : 80900  lr : 0.00378   loss : 0.47698  time : 16.2450 \n",
            "iter : 81000  lr : 0.00377   loss : 0.61889  time : 16.2464 \n",
            "iter : 81100  lr : 0.00376   loss : 0.98489  time : 16.2402 \n",
            "iter : 81200  lr : 0.00375   loss : 0.68961  time : 16.2452 \n",
            "iter : 81300  lr : 0.00375   loss : 0.62374  time : 16.2787 \n",
            "iter : 81400  lr : 0.00374   loss : 0.76492  time : 16.2359 \n",
            "iter : 81500  lr : 0.00373   loss : 0.49035  time : 16.2338 \n",
            "iter : 81600  lr : 0.00372   loss : 0.56366  time : 16.2371 \n",
            "iter : 81700  lr : 0.00372   loss : 0.82763  time : 16.2323 \n",
            "iter : 81800  lr : 0.00371   loss : 0.61870  time : 16.2332 \n",
            "iter : 81900  lr : 0.00370   loss : 0.62316  time : 16.2406 \n",
            "iter : 82000  lr : 0.00369   loss : 0.60122  time : 16.2294 \n",
            "iter : 82100  lr : 0.00368   loss : 0.43350  time : 16.2388 \n",
            "iter : 82200  lr : 0.00368   loss : 0.61012  time : 16.2384 \n",
            "iter : 82300  lr : 0.00367   loss : 0.78339  time : 16.2371 \n",
            "iter : 82400  lr : 0.00366   loss : 0.64858  time : 16.2452 \n",
            "iter : 82500  lr : 0.00365   loss : 0.63591  time : 16.2763 \n",
            "iter : 82600  lr : 0.00365   loss : 0.72036  time : 16.2585 \n",
            "iter : 82700  lr : 0.00364   loss : 0.83902  time : 16.2386 \n",
            "iter : 82800  lr : 0.00363   loss : 0.50608  time : 16.2479 \n",
            "iter : 82900  lr : 0.00362   loss : 0.74062  time : 16.2390 \n",
            "iter : 83000  lr : 0.00362   loss : 0.50341  time : 16.2385 \n",
            "iter : 83100  lr : 0.00361   loss : 0.64984  time : 16.2379 \n",
            "iter : 83200  lr : 0.00360   loss : 0.39114  time : 16.2460 \n",
            "iter : 83300  lr : 0.00359   loss : 0.44276  time : 16.2293 \n",
            "iter : 83400  lr : 0.00358   loss : 0.59683  time : 16.2504 \n",
            "iter : 83500  lr : 0.00358   loss : 0.51098  time : 16.2573 \n",
            "iter : 83600  lr : 0.00357   loss : 0.51652  time : 16.2366 \n",
            "iter : 83700  lr : 0.00356   loss : 0.62753  time : 16.2696 \n",
            "iter : 83800  lr : 0.00355   loss : 0.71182  time : 16.2430 \n",
            "iter : 83900  lr : 0.00355   loss : 0.56870  time : 16.2411 \n",
            "iter : 84000  lr : 0.00354   loss : 0.63191  time : 16.2342 \n",
            "iter : 84100  lr : 0.00353   loss : 0.59941  time : 16.2387 \n",
            "iter : 84200  lr : 0.00352   loss : 0.68459  time : 16.2616 \n",
            "iter : 84300  lr : 0.00352   loss : 0.50897  time : 16.2611 \n",
            "iter : 84400  lr : 0.00351   loss : 0.42491  time : 16.2580 \n",
            "iter : 84500  lr : 0.00350   loss : 0.43799  time : 16.2472 \n",
            "iter : 84600  lr : 0.00349   loss : 0.42979  time : 16.2623 \n",
            "iter : 84700  lr : 0.00348   loss : 0.43077  time : 16.2575 \n",
            "iter : 84800  lr : 0.00348   loss : 0.61035  time : 16.2583 \n",
            "iter : 84900  lr : 0.00347   loss : 0.41978  time : 16.2408 \n",
            "iter : 85000  lr : 0.00346   loss : 0.47621  time : 16.2669 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 69.970000    top5 : 88.850000 \n",
            "\n",
            "iter : 85100  lr : 0.00345   loss : 0.68085  time : 145.5579 \n",
            "iter : 85200  lr : 0.00345   loss : 0.34558  time : 16.2361 \n",
            "iter : 85300  lr : 0.00344   loss : 0.43435  time : 16.2311 \n",
            "iter : 85400  lr : 0.00343   loss : 0.79565  time : 16.2538 \n",
            "iter : 85500  lr : 0.00342   loss : 0.24192  time : 16.2361 \n",
            "iter : 85600  lr : 0.00342   loss : 0.62856  time : 16.2336 \n",
            "iter : 85700  lr : 0.00341   loss : 1.10437  time : 16.2358 \n",
            "iter : 85800  lr : 0.00340   loss : 0.48588  time : 16.2333 \n",
            "iter : 85900  lr : 0.00339   loss : 0.55151  time : 16.2298 \n",
            "iter : 86000  lr : 0.00338   loss : 0.53477  time : 16.2409 \n",
            "iter : 86100  lr : 0.00338   loss : 0.58912  time : 16.2395 \n",
            "iter : 86200  lr : 0.00337   loss : 0.48032  time : 16.2413 \n",
            "iter : 86300  lr : 0.00336   loss : 0.37466  time : 16.2389 \n",
            "iter : 86400  lr : 0.00335   loss : 0.60511  time : 16.2570 \n",
            "iter : 86500  lr : 0.00335   loss : 0.53597  time : 16.2993 \n",
            "iter : 86600  lr : 0.00334   loss : 0.74123  time : 16.2696 \n",
            "iter : 86700  lr : 0.00333   loss : 0.60698  time : 16.2398 \n",
            "iter : 86800  lr : 0.00332   loss : 0.58289  time : 16.2391 \n",
            "iter : 86900  lr : 0.00332   loss : 0.42339  time : 16.2539 \n",
            "iter : 87000  lr : 0.00331   loss : 0.54495  time : 16.2947 \n",
            "iter : 87100  lr : 0.00330   loss : 0.58978  time : 16.2814 \n",
            "iter : 87200  lr : 0.00329   loss : 0.69576  time : 16.2855 \n",
            "iter : 87300  lr : 0.00328   loss : 0.55156  time : 16.2810 \n",
            "iter : 87400  lr : 0.00328   loss : 0.60073  time : 16.3284 \n",
            "iter : 87500  lr : 0.00327   loss : 0.45926  time : 16.3688 \n",
            "iter : 87600  lr : 0.00326   loss : 0.39623  time : 16.3478 \n",
            "iter : 87700  lr : 0.00325   loss : 0.44426  time : 16.3779 \n",
            "iter : 87800  lr : 0.00325   loss : 0.68908  time : 16.3635 \n",
            "iter : 87900  lr : 0.00324   loss : 0.43382  time : 16.3735 \n",
            "iter : 88000  lr : 0.00323   loss : 0.41404  time : 16.3066 \n",
            "iter : 88100  lr : 0.00322   loss : 0.37474  time : 16.2855 \n",
            "iter : 88200  lr : 0.00322   loss : 0.34908  time : 16.2745 \n",
            "iter : 88300  lr : 0.00321   loss : 0.55325  time : 16.2845 \n",
            "iter : 88400  lr : 0.00320   loss : 0.40827  time : 16.2981 \n",
            "iter : 88500  lr : 0.00319   loss : 0.40061  time : 16.2796 \n",
            "iter : 88600  lr : 0.00318   loss : 0.76054  time : 16.2837 \n",
            "iter : 88700  lr : 0.00318   loss : 0.66861  time : 16.2734 \n",
            "iter : 88800  lr : 0.00317   loss : 0.63941  time : 16.2703 \n",
            "iter : 88900  lr : 0.00316   loss : 0.64511  time : 16.2653 \n",
            "iter : 89000  lr : 0.00315   loss : 0.39141  time : 16.2643 \n",
            "iter : 89100  lr : 0.00315   loss : 0.79411  time : 16.2991 \n",
            "iter : 89200  lr : 0.00314   loss : 0.50716  time : 16.2787 \n",
            "iter : 89300  lr : 0.00313   loss : 0.69448  time : 16.2783 \n",
            "iter : 89400  lr : 0.00312   loss : 0.42021  time : 16.2726 \n",
            "iter : 89500  lr : 0.00312   loss : 0.51676  time : 16.2664 \n",
            "iter : 89600  lr : 0.00311   loss : 0.70779  time : 16.2591 \n",
            "iter : 89700  lr : 0.00310   loss : 0.69636  time : 16.2574 \n",
            "iter : 89800  lr : 0.00309   loss : 0.26951  time : 16.2602 \n",
            "iter : 89900  lr : 0.00308   loss : 0.42810  time : 16.2472 \n",
            "iter : 90000  lr : 0.00308   loss : 0.42380  time : 16.2392 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 71.650000    top5 : 89.740000 \n",
            "\n",
            "iter : 90100  lr : 0.00307   loss : 0.77364  time : 145.4832 \n",
            "iter : 90200  lr : 0.00306   loss : 0.44621  time : 16.2460 \n",
            "iter : 90300  lr : 0.00305   loss : 0.79220  time : 16.2721 \n",
            "iter : 90400  lr : 0.00305   loss : 0.49132  time : 16.2527 \n",
            "iter : 90500  lr : 0.00304   loss : 0.28938  time : 16.2520 \n",
            "iter : 90600  lr : 0.00303   loss : 0.37990  time : 16.2437 \n",
            "iter : 90700  lr : 0.00302   loss : 0.56282  time : 16.2452 \n",
            "iter : 90800  lr : 0.00302   loss : 0.58455  time : 16.2451 \n",
            "iter : 90900  lr : 0.00301   loss : 0.54521  time : 16.2573 \n",
            "iter : 91000  lr : 0.00300   loss : 0.33695  time : 16.2482 \n",
            "iter : 91100  lr : 0.00299   loss : 0.57585  time : 16.2600 \n",
            "iter : 91200  lr : 0.00298   loss : 0.72922  time : 16.2486 \n",
            "iter : 91300  lr : 0.00298   loss : 0.41916  time : 16.2462 \n",
            "iter : 91400  lr : 0.00297   loss : 0.48589  time : 16.2501 \n",
            "iter : 91500  lr : 0.00296   loss : 0.51137  time : 16.2416 \n",
            "iter : 91600  lr : 0.00295   loss : 0.64299  time : 16.2458 \n",
            "iter : 91700  lr : 0.00295   loss : 0.49510  time : 16.2498 \n",
            "iter : 91800  lr : 0.00294   loss : 0.47430  time : 16.2445 \n",
            "iter : 91900  lr : 0.00293   loss : 0.42654  time : 16.3298 \n",
            "iter : 92000  lr : 0.00292   loss : 0.45089  time : 16.2780 \n",
            "iter : 92100  lr : 0.00292   loss : 0.47022  time : 16.2461 \n",
            "iter : 92200  lr : 0.00291   loss : 0.44769  time : 16.2752 \n",
            "iter : 92300  lr : 0.00290   loss : 0.43802  time : 16.2511 \n",
            "iter : 92400  lr : 0.00289   loss : 0.46818  time : 16.2511 \n",
            "iter : 92500  lr : 0.00288   loss : 0.47703  time : 16.2431 \n",
            "iter : 92600  lr : 0.00288   loss : 0.57222  time : 16.2423 \n",
            "iter : 92700  lr : 0.00287   loss : 0.29254  time : 16.2421 \n",
            "iter : 92800  lr : 0.00286   loss : 0.43379  time : 16.2497 \n",
            "iter : 92900  lr : 0.00285   loss : 0.38180  time : 16.2521 \n",
            "iter : 93000  lr : 0.00285   loss : 0.44523  time : 16.2627 \n",
            "iter : 93100  lr : 0.00284   loss : 0.61330  time : 16.2464 \n",
            "iter : 93200  lr : 0.00283   loss : 0.33559  time : 16.2522 \n",
            "iter : 93300  lr : 0.00282   loss : 0.45510  time : 16.2477 \n",
            "iter : 93400  lr : 0.00282   loss : 0.57759  time : 16.2585 \n",
            "iter : 93500  lr : 0.00281   loss : 0.40029  time : 16.2679 \n",
            "iter : 93600  lr : 0.00280   loss : 0.40582  time : 16.2406 \n",
            "iter : 93700  lr : 0.00279   loss : 0.49607  time : 16.2443 \n",
            "iter : 93800  lr : 0.00278   loss : 0.40907  time : 16.2404 \n",
            "iter : 93900  lr : 0.00278   loss : 0.38625  time : 16.2442 \n",
            "iter : 94000  lr : 0.00277   loss : 0.39024  time : 16.2469 \n",
            "iter : 94100  lr : 0.00276   loss : 0.46865  time : 16.2569 \n",
            "iter : 94200  lr : 0.00275   loss : 0.31129  time : 16.2390 \n",
            "iter : 94300  lr : 0.00275   loss : 0.32562  time : 16.2385 \n",
            "iter : 94400  lr : 0.00274   loss : 0.43387  time : 16.2474 \n",
            "iter : 94500  lr : 0.00273   loss : 0.52717  time : 16.2594 \n",
            "iter : 94600  lr : 0.00272   loss : 0.25058  time : 16.2553 \n",
            "iter : 94700  lr : 0.00272   loss : 0.48937  time : 16.2422 \n",
            "iter : 94800  lr : 0.00271   loss : 0.43185  time : 16.2492 \n",
            "iter : 94900  lr : 0.00270   loss : 0.56727  time : 16.3047 \n",
            "iter : 95000  lr : 0.00269   loss : 0.53007  time : 16.2939 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 71.890000    top5 : 89.460000 \n",
            "\n",
            "iter : 95100  lr : 0.00268   loss : 0.35337  time : 145.4934 \n",
            "iter : 95200  lr : 0.00268   loss : 0.52331  time : 16.2411 \n",
            "iter : 95300  lr : 0.00267   loss : 0.35906  time : 16.2321 \n",
            "iter : 95400  lr : 0.00266   loss : 0.27113  time : 16.2740 \n",
            "iter : 95500  lr : 0.00265   loss : 0.53695  time : 16.3239 \n",
            "iter : 95600  lr : 0.00265   loss : 0.34463  time : 16.3155 \n",
            "iter : 95700  lr : 0.00264   loss : 0.50273  time : 16.3412 \n",
            "iter : 95800  lr : 0.00263   loss : 0.49317  time : 16.3289 \n",
            "iter : 95900  lr : 0.00262   loss : 0.37037  time : 16.3178 \n",
            "iter : 96000  lr : 0.00262   loss : 0.27832  time : 16.3220 \n",
            "iter : 96100  lr : 0.00261   loss : 0.33872  time : 16.3071 \n",
            "iter : 96200  lr : 0.00260   loss : 0.54507  time : 16.2888 \n",
            "iter : 96300  lr : 0.00259   loss : 0.40702  time : 16.3009 \n",
            "iter : 96400  lr : 0.00258   loss : 0.36683  time : 16.2807 \n",
            "iter : 96500  lr : 0.00258   loss : 0.37111  time : 16.2513 \n",
            "iter : 96600  lr : 0.00257   loss : 0.40134  time : 16.2747 \n",
            "iter : 96700  lr : 0.00256   loss : 0.42074  time : 16.3283 \n",
            "iter : 96800  lr : 0.00255   loss : 0.27081  time : 16.2923 \n",
            "iter : 96900  lr : 0.00255   loss : 0.42921  time : 16.2842 \n",
            "iter : 97000  lr : 0.00254   loss : 0.39854  time : 16.2842 \n",
            "iter : 97100  lr : 0.00253   loss : 0.39139  time : 16.2799 \n",
            "iter : 97200  lr : 0.00252   loss : 0.42423  time : 16.2818 \n",
            "iter : 97300  lr : 0.00252   loss : 0.16037  time : 16.2826 \n",
            "iter : 97400  lr : 0.00251   loss : 0.38648  time : 16.2814 \n",
            "iter : 97500  lr : 0.00250   loss : 0.32306  time : 16.3023 \n",
            "iter : 97600  lr : 0.00249   loss : 0.46862  time : 16.3325 \n",
            "iter : 97700  lr : 0.00248   loss : 0.45573  time : 16.2927 \n",
            "iter : 97800  lr : 0.00248   loss : 0.46742  time : 16.2964 \n",
            "iter : 97900  lr : 0.00247   loss : 0.35305  time : 16.2932 \n",
            "iter : 98000  lr : 0.00246   loss : 0.29386  time : 16.2889 \n",
            "iter : 98100  lr : 0.00245   loss : 0.23680  time : 16.2879 \n",
            "iter : 98200  lr : 0.00245   loss : 0.40735  time : 16.3031 \n",
            "iter : 98300  lr : 0.00244   loss : 0.48159  time : 16.2876 \n",
            "iter : 98400  lr : 0.00243   loss : 0.28548  time : 16.3089 \n",
            "iter : 98500  lr : 0.00242   loss : 0.52788  time : 16.3280 \n",
            "iter : 98600  lr : 0.00242   loss : 0.31793  time : 16.2944 \n",
            "iter : 98700  lr : 0.00241   loss : 0.32312  time : 16.2792 \n",
            "iter : 98800  lr : 0.00240   loss : 0.30808  time : 16.2818 \n",
            "iter : 98900  lr : 0.00239   loss : 0.34222  time : 16.2827 \n",
            "iter : 99000  lr : 0.00238   loss : 0.29078  time : 16.3008 \n",
            "iter : 99100  lr : 0.00238   loss : 0.45375  time : 16.2822 \n",
            "iter : 99200  lr : 0.00237   loss : 0.24338  time : 16.2914 \n",
            "iter : 99300  lr : 0.00236   loss : 0.50892  time : 16.3419 \n",
            "iter : 99400  lr : 0.00235   loss : 0.43063  time : 16.2805 \n",
            "iter : 99500  lr : 0.00235   loss : 0.37565  time : 16.3215 \n",
            "iter : 99600  lr : 0.00234   loss : 0.43100  time : 16.3196 \n",
            "iter : 99700  lr : 0.00233   loss : 0.33327  time : 16.3050 \n",
            "iter : 99800  lr : 0.00232   loss : 0.37514  time : 16.2955 \n",
            "iter : 99900  lr : 0.00232   loss : 0.42876  time : 16.2861 \n",
            "iter : 100000  lr : 0.00231   loss : 0.37442  time : 16.2946 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 72.020000    top5 : 89.510000 \n",
            "\n",
            "iter : 100100  lr : 0.00230   loss : 0.55873  time : 146.1115 \n",
            "iter : 100200  lr : 0.00229   loss : 0.33945  time : 16.3287 \n",
            "iter : 100300  lr : 0.00228   loss : 0.39987  time : 16.2818 \n",
            "iter : 100400  lr : 0.00228   loss : 0.38501  time : 16.2770 \n",
            "iter : 100500  lr : 0.00227   loss : 0.24445  time : 16.2822 \n",
            "iter : 100600  lr : 0.00226   loss : 0.48073  time : 16.3002 \n",
            "iter : 100700  lr : 0.00225   loss : 0.28098  time : 16.2844 \n",
            "iter : 100800  lr : 0.00225   loss : 0.38772  time : 16.2835 \n",
            "iter : 100900  lr : 0.00224   loss : 0.38205  time : 16.2932 \n",
            "iter : 101000  lr : 0.00223   loss : 0.49942  time : 16.3583 \n",
            "iter : 101100  lr : 0.00222   loss : 0.28943  time : 16.3381 \n",
            "iter : 101200  lr : 0.00222   loss : 0.28591  time : 16.2873 \n",
            "iter : 101300  lr : 0.00221   loss : 0.26159  time : 16.3388 \n",
            "iter : 101400  lr : 0.00220   loss : 0.46313  time : 16.3812 \n",
            "iter : 101500  lr : 0.00219   loss : 0.39253  time : 16.3707 \n",
            "iter : 101600  lr : 0.00218   loss : 0.24130  time : 16.3778 \n",
            "iter : 101700  lr : 0.00218   loss : 0.33704  time : 16.3847 \n",
            "iter : 101800  lr : 0.00217   loss : 0.28705  time : 16.3720 \n",
            "iter : 101900  lr : 0.00216   loss : 0.44012  time : 16.3695 \n",
            "iter : 102000  lr : 0.00215   loss : 0.28872  time : 16.3737 \n",
            "iter : 102100  lr : 0.00215   loss : 0.30048  time : 16.3914 \n",
            "iter : 102200  lr : 0.00214   loss : 0.19769  time : 16.4001 \n",
            "iter : 102300  lr : 0.00213   loss : 0.24376  time : 16.4068 \n",
            "iter : 102400  lr : 0.00212   loss : 0.22169  time : 16.4104 \n",
            "iter : 102500  lr : 0.00212   loss : 0.41247  time : 16.3979 \n",
            "iter : 102600  lr : 0.00211   loss : 0.40081  time : 16.3778 \n",
            "iter : 102700  lr : 0.00210   loss : 0.48264  time : 16.3940 \n",
            "iter : 102800  lr : 0.00209   loss : 0.29862  time : 16.4034 \n",
            "iter : 102900  lr : 0.00208   loss : 0.36636  time : 16.3399 \n",
            "iter : 103000  lr : 0.00208   loss : 0.29403  time : 16.2681 \n",
            "iter : 103100  lr : 0.00207   loss : 0.07891  time : 16.2451 \n",
            "iter : 103200  lr : 0.00206   loss : 0.37403  time : 16.2568 \n",
            "iter : 103300  lr : 0.00205   loss : 0.44947  time : 16.2470 \n",
            "iter : 103400  lr : 0.00205   loss : 0.31096  time : 16.2515 \n",
            "iter : 103500  lr : 0.00204   loss : 0.30337  time : 16.2325 \n",
            "iter : 103600  lr : 0.00203   loss : 0.37091  time : 16.2897 \n",
            "iter : 103700  lr : 0.00202   loss : 0.43011  time : 16.3072 \n",
            "iter : 103800  lr : 0.00202   loss : 0.22514  time : 16.3085 \n",
            "iter : 103900  lr : 0.00201   loss : 0.56432  time : 16.3277 \n",
            "iter : 104000  lr : 0.00200   loss : 0.24728  time : 16.2566 \n",
            "iter : 104100  lr : 0.00199   loss : 0.42222  time : 16.2674 \n",
            "iter : 104200  lr : 0.00198   loss : 0.44874  time : 16.2668 \n",
            "iter : 104300  lr : 0.00198   loss : 0.19531  time : 16.2852 \n",
            "iter : 104400  lr : 0.00197   loss : 0.21397  time : 16.3315 \n",
            "iter : 104500  lr : 0.00196   loss : 0.29417  time : 16.3173 \n",
            "iter : 104600  lr : 0.00195   loss : 0.29886  time : 16.2972 \n",
            "iter : 104700  lr : 0.00195   loss : 0.16771  time : 16.3057 \n",
            "iter : 104800  lr : 0.00194   loss : 0.22249  time : 16.2774 \n",
            "iter : 104900  lr : 0.00193   loss : 0.25625  time : 16.3133 \n",
            "iter : 105000  lr : 0.00192   loss : 0.38982  time : 16.3005 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 72.730000    top5 : 89.950000 \n",
            "\n",
            "iter : 105100  lr : 0.00192   loss : 0.19849  time : 145.4129 \n",
            "iter : 105200  lr : 0.00191   loss : 0.31735  time : 16.2530 \n",
            "iter : 105300  lr : 0.00190   loss : 0.46584  time : 16.2968 \n",
            "iter : 105400  lr : 0.00189   loss : 0.26076  time : 16.3265 \n",
            "iter : 105500  lr : 0.00188   loss : 0.29543  time : 16.3065 \n",
            "iter : 105600  lr : 0.00188   loss : 0.28754  time : 16.3000 \n",
            "iter : 105700  lr : 0.00187   loss : 0.21951  time : 16.3245 \n",
            "iter : 105800  lr : 0.00186   loss : 0.29159  time : 16.3264 \n",
            "iter : 105900  lr : 0.00185   loss : 0.33639  time : 16.2692 \n",
            "iter : 106000  lr : 0.00185   loss : 0.42235  time : 16.3423 \n",
            "iter : 106100  lr : 0.00184   loss : 0.34609  time : 16.3301 \n",
            "iter : 106200  lr : 0.00183   loss : 0.46283  time : 16.3047 \n",
            "iter : 106300  lr : 0.00182   loss : 0.21137  time : 16.2834 \n",
            "iter : 106400  lr : 0.00182   loss : 0.20447  time : 16.2699 \n",
            "iter : 106500  lr : 0.00181   loss : 0.25931  time : 16.2748 \n",
            "iter : 106600  lr : 0.00180   loss : 0.54684  time : 16.2480 \n",
            "iter : 106700  lr : 0.00179   loss : 0.23966  time : 16.2591 \n",
            "iter : 106800  lr : 0.00178   loss : 0.32973  time : 16.2870 \n",
            "iter : 106900  lr : 0.00178   loss : 0.23735  time : 16.2984 \n",
            "iter : 107000  lr : 0.00177   loss : 0.24787  time : 16.2622 \n",
            "iter : 107100  lr : 0.00176   loss : 0.27358  time : 16.3185 \n",
            "iter : 107200  lr : 0.00175   loss : 0.38242  time : 16.3417 \n",
            "iter : 107300  lr : 0.00175   loss : 0.36574  time : 16.3508 \n",
            "iter : 107400  lr : 0.00174   loss : 0.38742  time : 16.3560 \n",
            "iter : 107500  lr : 0.00173   loss : 0.26960  time : 16.3489 \n",
            "iter : 107600  lr : 0.00172   loss : 0.31059  time : 16.3698 \n",
            "iter : 107700  lr : 0.00172   loss : 0.43376  time : 16.3700 \n",
            "iter : 107800  lr : 0.00171   loss : 0.14734  time : 16.3315 \n",
            "iter : 107900  lr : 0.00170   loss : 0.51810  time : 16.3009 \n",
            "iter : 108000  lr : 0.00169   loss : 0.23001  time : 16.3054 \n",
            "iter : 108100  lr : 0.00168   loss : 0.47261  time : 16.3329 \n",
            "iter : 108200  lr : 0.00168   loss : 0.33954  time : 16.3274 \n",
            "iter : 108300  lr : 0.00167   loss : 0.41905  time : 16.3216 \n",
            "iter : 108400  lr : 0.00166   loss : 0.36605  time : 16.3326 \n",
            "iter : 108500  lr : 0.00165   loss : 0.28460  time : 16.3303 \n",
            "iter : 108600  lr : 0.00165   loss : 0.24453  time : 16.3368 \n",
            "iter : 108700  lr : 0.00164   loss : 0.26142  time : 16.3321 \n",
            "iter : 108800  lr : 0.00163   loss : 0.32828  time : 16.3101 \n",
            "iter : 108900  lr : 0.00162   loss : 0.32219  time : 16.3277 \n",
            "iter : 109000  lr : 0.00162   loss : 0.15634  time : 16.3268 \n",
            "iter : 109100  lr : 0.00161   loss : 0.24287  time : 16.3241 \n",
            "iter : 109200  lr : 0.00160   loss : 0.28420  time : 16.2885 \n",
            "iter : 109300  lr : 0.00159   loss : 0.20973  time : 16.3205 \n",
            "iter : 109400  lr : 0.00158   loss : 0.19920  time : 16.3045 \n",
            "iter : 109500  lr : 0.00158   loss : 0.16197  time : 16.3144 \n",
            "iter : 109600  lr : 0.00157   loss : 0.22894  time : 16.3085 \n",
            "iter : 109700  lr : 0.00156   loss : 0.14740  time : 16.3264 \n",
            "iter : 109800  lr : 0.00155   loss : 0.13007  time : 16.2651 \n",
            "iter : 109900  lr : 0.00155   loss : 0.31657  time : 16.3025 \n",
            "iter : 110000  lr : 0.00154   loss : 0.26892  time : 16.2882 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 72.620000    top5 : 90.130000 \n",
            "\n",
            "iter : 110100  lr : 0.00153   loss : 0.19863  time : 145.4216 \n",
            "iter : 110200  lr : 0.00152   loss : 0.30830  time : 16.2697 \n",
            "iter : 110300  lr : 0.00152   loss : 0.24257  time : 16.2814 \n",
            "iter : 110400  lr : 0.00151   loss : 0.16685  time : 16.2914 \n",
            "iter : 110500  lr : 0.00150   loss : 0.31596  time : 16.2672 \n",
            "iter : 110600  lr : 0.00149   loss : 0.36666  time : 16.2956 \n",
            "iter : 110700  lr : 0.00148   loss : 0.29767  time : 16.3012 \n",
            "iter : 110800  lr : 0.00148   loss : 0.20796  time : 16.2966 \n",
            "iter : 110900  lr : 0.00147   loss : 0.18002  time : 16.2869 \n",
            "iter : 111000  lr : 0.00146   loss : 0.32911  time : 16.2724 \n",
            "iter : 111100  lr : 0.00145   loss : 0.21415  time : 16.2777 \n",
            "iter : 111200  lr : 0.00145   loss : 0.09847  time : 16.2773 \n",
            "iter : 111300  lr : 0.00144   loss : 0.22906  time : 16.3000 \n",
            "iter : 111400  lr : 0.00143   loss : 0.20418  time : 16.3203 \n",
            "iter : 111500  lr : 0.00142   loss : 0.13560  time : 16.2860 \n",
            "iter : 111600  lr : 0.00142   loss : 0.18723  time : 16.3056 \n",
            "iter : 111700  lr : 0.00141   loss : 0.11562  time : 16.2907 \n",
            "iter : 111800  lr : 0.00140   loss : 0.17560  time : 16.2723 \n",
            "iter : 111900  lr : 0.00139   loss : 0.18485  time : 16.3036 \n",
            "iter : 112000  lr : 0.00138   loss : 0.26114  time : 16.3118 \n",
            "iter : 112100  lr : 0.00138   loss : 0.18967  time : 16.2867 \n",
            "iter : 112200  lr : 0.00137   loss : 0.08711  time : 16.2971 \n",
            "iter : 112300  lr : 0.00136   loss : 0.21659  time : 16.3103 \n",
            "iter : 112400  lr : 0.00135   loss : 0.23946  time : 16.2762 \n",
            "iter : 112500  lr : 0.00135   loss : 0.20971  time : 16.2780 \n",
            "iter : 112600  lr : 0.00134   loss : 0.21015  time : 16.3037 \n",
            "iter : 112700  lr : 0.00133   loss : 0.09904  time : 16.2801 \n",
            "iter : 112800  lr : 0.00132   loss : 0.17034  time : 16.2811 \n",
            "iter : 112900  lr : 0.00132   loss : 0.06202  time : 16.2775 \n",
            "iter : 113000  lr : 0.00131   loss : 0.17477  time : 16.2562 \n",
            "iter : 113100  lr : 0.00130   loss : 0.19540  time : 16.2474 \n",
            "iter : 113200  lr : 0.00129   loss : 0.10906  time : 16.2778 \n",
            "iter : 113300  lr : 0.00128   loss : 0.14038  time : 16.2544 \n",
            "iter : 113400  lr : 0.00128   loss : 0.15753  time : 16.2488 \n",
            "iter : 113500  lr : 0.00127   loss : 0.14879  time : 16.3184 \n",
            "iter : 113600  lr : 0.00126   loss : 0.14242  time : 16.3360 \n",
            "iter : 113700  lr : 0.00125   loss : 0.14070  time : 16.2987 \n",
            "iter : 113800  lr : 0.00125   loss : 0.27492  time : 16.2624 \n",
            "iter : 113900  lr : 0.00124   loss : 0.14362  time : 16.2674 \n",
            "iter : 114000  lr : 0.00123   loss : 0.20589  time : 16.2908 \n",
            "iter : 114100  lr : 0.00122   loss : 0.07879  time : 16.2816 \n",
            "iter : 114200  lr : 0.00122   loss : 0.22033  time : 16.2784 \n",
            "iter : 114300  lr : 0.00121   loss : 0.08148  time : 16.3164 \n",
            "iter : 114400  lr : 0.00120   loss : 0.18642  time : 16.3232 \n",
            "iter : 114500  lr : 0.00119   loss : 0.20943  time : 16.2863 \n",
            "iter : 114600  lr : 0.00118   loss : 0.18556  time : 16.3367 \n",
            "iter : 114700  lr : 0.00118   loss : 0.16750  time : 16.3441 \n",
            "iter : 114800  lr : 0.00117   loss : 0.16654  time : 16.3564 \n",
            "iter : 114900  lr : 0.00116   loss : 0.07674  time : 16.3592 \n",
            "iter : 115000  lr : 0.00115   loss : 0.22477  time : 16.3328 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 73.010000    top5 : 90.440000 \n",
            "\n",
            "iter : 115100  lr : 0.00115   loss : 0.11950  time : 146.0503 \n",
            "iter : 115200  lr : 0.00114   loss : 0.23626  time : 16.2835 \n",
            "iter : 115300  lr : 0.00113   loss : 0.14958  time : 16.2802 \n",
            "iter : 115400  lr : 0.00112   loss : 0.24191  time : 16.2654 \n",
            "iter : 115500  lr : 0.00112   loss : 0.06019  time : 16.2918 \n",
            "iter : 115600  lr : 0.00111   loss : 0.19971  time : 16.3154 \n",
            "iter : 115700  lr : 0.00110   loss : 0.11317  time : 16.2797 \n",
            "iter : 115800  lr : 0.00109   loss : 0.07525  time : 16.2866 \n",
            "iter : 115900  lr : 0.00108   loss : 0.08938  time : 16.2722 \n",
            "iter : 116000  lr : 0.00108   loss : 0.24943  time : 16.2666 \n",
            "iter : 116100  lr : 0.00107   loss : 0.15126  time : 16.2654 \n",
            "iter : 116200  lr : 0.00106   loss : 0.13712  time : 16.2696 \n",
            "iter : 116300  lr : 0.00105   loss : 0.06020  time : 16.2748 \n",
            "iter : 116400  lr : 0.00105   loss : 0.21606  time : 16.2652 \n",
            "iter : 116500  lr : 0.00104   loss : 0.09276  time : 16.2658 \n",
            "iter : 116600  lr : 0.00103   loss : 0.24652  time : 16.2731 \n",
            "iter : 116700  lr : 0.00102   loss : 0.22100  time : 16.2724 \n",
            "iter : 116800  lr : 0.00102   loss : 0.31070  time : 16.2605 \n",
            "iter : 116900  lr : 0.00101   loss : 0.08680  time : 16.2691 \n",
            "iter : 117000  lr : 0.00100   loss : 0.18078  time : 16.2629 \n",
            "iter : 117100  lr : 0.00099   loss : 0.11326  time : 16.2810 \n",
            "iter : 117200  lr : 0.00098   loss : 0.10025  time : 16.3073 \n",
            "iter : 117300  lr : 0.00098   loss : 0.08127  time : 16.2680 \n",
            "iter : 117400  lr : 0.00097   loss : 0.12821  time : 16.3420 \n",
            "iter : 117500  lr : 0.00096   loss : 0.15291  time : 16.3289 \n",
            "iter : 117600  lr : 0.00095   loss : 0.09050  time : 16.3260 \n",
            "iter : 117700  lr : 0.00095   loss : 0.20054  time : 16.3169 \n",
            "iter : 117800  lr : 0.00094   loss : 0.13309  time : 16.3338 \n",
            "iter : 117900  lr : 0.00093   loss : 0.13638  time : 16.3189 \n",
            "iter : 118000  lr : 0.00092   loss : 0.10903  time : 16.3114 \n",
            "iter : 118100  lr : 0.00092   loss : 0.11732  time : 16.3079 \n",
            "iter : 118200  lr : 0.00091   loss : 0.06617  time : 16.3091 \n",
            "iter : 118300  lr : 0.00090   loss : 0.15426  time : 16.3056 \n",
            "iter : 118400  lr : 0.00089   loss : 0.14642  time : 16.3076 \n",
            "iter : 118500  lr : 0.00088   loss : 0.15528  time : 16.2993 \n",
            "iter : 118600  lr : 0.00088   loss : 0.19006  time : 16.3002 \n",
            "iter : 118700  lr : 0.00087   loss : 0.14483  time : 16.3087 \n",
            "iter : 118800  lr : 0.00086   loss : 0.16693  time : 16.3101 \n",
            "iter : 118900  lr : 0.00085   loss : 0.15558  time : 16.3024 \n",
            "iter : 119000  lr : 0.00085   loss : 0.23367  time : 16.2874 \n",
            "iter : 119100  lr : 0.00084   loss : 0.11845  time : 16.3210 \n",
            "iter : 119200  lr : 0.00083   loss : 0.16415  time : 16.3392 \n",
            "iter : 119300  lr : 0.00082   loss : 0.33062  time : 16.3531 \n",
            "iter : 119400  lr : 0.00082   loss : 0.10185  time : 16.3448 \n",
            "iter : 119500  lr : 0.00081   loss : 0.13452  time : 16.3035 \n",
            "iter : 119600  lr : 0.00080   loss : 0.16557  time : 16.3103 \n",
            "iter : 119700  lr : 0.00079   loss : 0.10425  time : 16.3086 \n",
            "iter : 119800  lr : 0.00078   loss : 0.14749  time : 16.3035 \n",
            "iter : 119900  lr : 0.00078   loss : 0.20020  time : 16.3075 \n",
            "iter : 120000  lr : 0.00077   loss : 0.13349  time : 16.2994 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 73.460000    top5 : 90.150000 \n",
            "\n",
            "iter : 120100  lr : 0.00076   loss : 0.11691  time : 145.4061 \n",
            "iter : 120200  lr : 0.00075   loss : 0.19549  time : 16.3105 \n",
            "iter : 120300  lr : 0.00075   loss : 0.21432  time : 16.3171 \n",
            "iter : 120400  lr : 0.00074   loss : 0.26579  time : 16.3183 \n",
            "iter : 120500  lr : 0.00073   loss : 0.32210  time : 16.3026 \n",
            "iter : 120600  lr : 0.00072   loss : 0.22329  time : 16.3028 \n",
            "iter : 120700  lr : 0.00072   loss : 0.15441  time : 16.3034 \n",
            "iter : 120800  lr : 0.00071   loss : 0.15649  time : 16.3026 \n",
            "iter : 120900  lr : 0.00070   loss : 0.10947  time : 16.2999 \n",
            "iter : 121000  lr : 0.00069   loss : 0.26849  time : 16.3012 \n",
            "iter : 121100  lr : 0.00068   loss : 0.06598  time : 16.3150 \n",
            "iter : 121200  lr : 0.00068   loss : 0.10733  time : 16.3281 \n",
            "iter : 121300  lr : 0.00067   loss : 0.05246  time : 16.3182 \n",
            "iter : 121400  lr : 0.00066   loss : 0.18593  time : 16.3308 \n",
            "iter : 121500  lr : 0.00065   loss : 0.16684  time : 16.3092 \n",
            "iter : 121600  lr : 0.00065   loss : 0.13827  time : 16.3167 \n",
            "iter : 121700  lr : 0.00064   loss : 0.06917  time : 16.3287 \n",
            "iter : 121800  lr : 0.00063   loss : 0.10244  time : 16.3188 \n",
            "iter : 121900  lr : 0.00062   loss : 0.10605  time : 16.3250 \n",
            "iter : 122000  lr : 0.00062   loss : 0.18259  time : 16.3467 \n",
            "iter : 122100  lr : 0.00061   loss : 0.07148  time : 16.3199 \n",
            "iter : 122200  lr : 0.00060   loss : 0.15396  time : 16.3108 \n",
            "iter : 122300  lr : 0.00059   loss : 0.11137  time : 16.3179 \n",
            "iter : 122400  lr : 0.00058   loss : 0.06961  time : 16.3229 \n",
            "iter : 122500  lr : 0.00058   loss : 0.07074  time : 16.3115 \n",
            "iter : 122600  lr : 0.00057   loss : 0.08384  time : 16.3109 \n",
            "iter : 122700  lr : 0.00056   loss : 0.11231  time : 16.3208 \n",
            "iter : 122800  lr : 0.00055   loss : 0.14639  time : 16.3235 \n",
            "iter : 122900  lr : 0.00055   loss : 0.10660  time : 16.3045 \n",
            "iter : 123000  lr : 0.00054   loss : 0.08431  time : 16.3213 \n",
            "iter : 123100  lr : 0.00053   loss : 0.24605  time : 16.3229 \n",
            "iter : 123200  lr : 0.00052   loss : 0.10133  time : 16.3158 \n",
            "iter : 123300  lr : 0.00052   loss : 0.02823  time : 16.3261 \n",
            "iter : 123400  lr : 0.00051   loss : 0.15678  time : 16.3175 \n",
            "iter : 123500  lr : 0.00050   loss : 0.02913  time : 16.3216 \n",
            "iter : 123600  lr : 0.00049   loss : 0.08182  time : 16.3092 \n",
            "iter : 123700  lr : 0.00048   loss : 0.12652  time : 16.3447 \n",
            "iter : 123800  lr : 0.00048   loss : 0.06203  time : 16.2982 \n",
            "iter : 123900  lr : 0.00047   loss : 0.25886  time : 16.2916 \n",
            "iter : 124000  lr : 0.00046   loss : 0.09911  time : 16.2928 \n",
            "iter : 124100  lr : 0.00045   loss : 0.13047  time : 16.2907 \n",
            "iter : 124200  lr : 0.00045   loss : 0.14053  time : 16.3136 \n",
            "iter : 124300  lr : 0.00044   loss : 0.15149  time : 16.3064 \n",
            "iter : 124400  lr : 0.00043   loss : 0.07072  time : 16.3141 \n",
            "iter : 124500  lr : 0.00042   loss : 0.10419  time : 16.2906 \n",
            "iter : 124600  lr : 0.00042   loss : 0.03005  time : 16.2940 \n",
            "iter : 124700  lr : 0.00041   loss : 0.13415  time : 16.2902 \n",
            "iter : 124800  lr : 0.00040   loss : 0.10751  time : 16.2944 \n",
            "iter : 124900  lr : 0.00039   loss : 0.10416  time : 16.2981 \n",
            "iter : 125000  lr : 0.00038   loss : 0.16408  time : 16.3019 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 74.330000    top5 : 90.510000 \n",
            "\n",
            "iter : 125100  lr : 0.00038   loss : 0.05287  time : 145.5029 \n",
            "iter : 125200  lr : 0.00037   loss : 0.16727  time : 16.2963 \n",
            "iter : 125300  lr : 0.00036   loss : 0.09730  time : 16.2943 \n",
            "iter : 125400  lr : 0.00035   loss : 0.06528  time : 16.2766 \n",
            "iter : 125500  lr : 0.00035   loss : 0.21041  time : 16.2824 \n",
            "iter : 125600  lr : 0.00034   loss : 0.09290  time : 16.2982 \n",
            "iter : 125700  lr : 0.00033   loss : 0.25478  time : 16.3085 \n",
            "iter : 125800  lr : 0.00032   loss : 0.11975  time : 16.2963 \n",
            "iter : 125900  lr : 0.00032   loss : 0.06897  time : 16.2966 \n",
            "iter : 126000  lr : 0.00031   loss : 0.11323  time : 16.3040 \n",
            "iter : 126100  lr : 0.00030   loss : 0.14997  time : 16.3062 \n",
            "iter : 126200  lr : 0.00029   loss : 0.12723  time : 16.3010 \n",
            "iter : 126300  lr : 0.00028   loss : 0.05658  time : 16.2888 \n",
            "iter : 126400  lr : 0.00028   loss : 0.12536  time : 16.2917 \n",
            "iter : 126500  lr : 0.00027   loss : 0.19322  time : 16.3012 \n",
            "iter : 126600  lr : 0.00026   loss : 0.08723  time : 16.2868 \n",
            "iter : 126700  lr : 0.00025   loss : 0.12117  time : 16.3033 \n",
            "iter : 126800  lr : 0.00025   loss : 0.09246  time : 16.3157 \n",
            "iter : 126900  lr : 0.00024   loss : 0.12645  time : 16.2872 \n",
            "iter : 127000  lr : 0.00023   loss : 0.02760  time : 16.2923 \n",
            "iter : 127100  lr : 0.00022   loss : 0.11856  time : 16.3169 \n",
            "iter : 127200  lr : 0.00022   loss : 0.19651  time : 16.3035 \n",
            "iter : 127300  lr : 0.00021   loss : 0.15549  time : 16.3072 \n",
            "iter : 127400  lr : 0.00020   loss : 0.03248  time : 16.3125 \n",
            "iter : 127500  lr : 0.00019   loss : 0.13606  time : 16.2955 \n",
            "iter : 127600  lr : 0.00018   loss : 0.16001  time : 16.2991 \n",
            "iter : 127700  lr : 0.00018   loss : 0.13816  time : 16.3246 \n",
            "iter : 127800  lr : 0.00017   loss : 0.07972  time : 16.2985 \n",
            "iter : 127900  lr : 0.00016   loss : 0.08288  time : 16.3136 \n",
            "iter : 128000  lr : 0.00015   loss : 0.10537  time : 16.3107 \n",
            "iter : 128100  lr : 0.00015   loss : 0.09311  time : 16.2895 \n",
            "iter : 128200  lr : 0.00014   loss : 0.11246  time : 16.3085 \n",
            "iter : 128300  lr : 0.00013   loss : 0.15764  time : 16.2762 \n",
            "iter : 128400  lr : 0.00012   loss : 0.09677  time : 16.2842 \n",
            "iter : 128500  lr : 0.00012   loss : 0.12561  time : 16.3075 \n",
            "iter : 128600  lr : 0.00011   loss : 0.13149  time : 16.3188 \n",
            "iter : 128700  lr : 0.00010   loss : 0.12722  time : 16.2914 \n",
            "iter : 128800  lr : 0.00009   loss : 0.10154  time : 16.2934 \n",
            "iter : 128900  lr : 0.00008   loss : 0.09717  time : 16.2924 \n",
            "iter : 129000  lr : 0.00008   loss : 0.08624  time : 16.3027 \n",
            "iter : 129100  lr : 0.00007   loss : 0.07433  time : 16.2848 \n",
            "iter : 129200  lr : 0.00006   loss : 0.07233  time : 16.2779 \n",
            "iter : 129300  lr : 0.00005   loss : 0.05292  time : 16.3009 \n",
            "iter : 129400  lr : 0.00005   loss : 0.11067  time : 16.3143 \n",
            "iter : 129500  lr : 0.00004   loss : 0.14503  time : 16.2910 \n",
            "iter : 129600  lr : 0.00003   loss : 0.15792  time : 16.3010 \n",
            "iter : 129700  lr : 0.00002   loss : 0.08162  time : 16.3180 \n",
            "iter : 129800  lr : 0.00002   loss : 0.06209  time : 16.3100 \n",
            "iter : 129900  lr : 0.00001   loss : 0.06226  time : 16.2997 \n",
            "iter : 130000  lr : 0.00000   loss : 0.06537  time : 16.3106 \n",
            "SAVING MODEL\n",
            "SAVING MODEL FINISH\n",
            "START TEST\n",
            "top1 : 74.470000    top5 : 90.500000 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "\n",
        "# 1. Paramters setting\n",
        "num_class = 200\n",
        "model_save_path = './model/Resnet50_SGD_b64_v2/'\n",
        "restore_iter = 130000\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(DEVICE)\n",
        "\n",
        "# ----- 2. data load\n",
        "print('DATA LOAD')\n",
        "test_images = image_load(\"/content/AI_Programming/test/\")\n",
        "print('DATA LOAD FINISH')\n",
        "\n",
        "# ---- 3. network build (restore model if necessary)\n",
        "model = ResNet_50().to(DEVICE)\n",
        "model.load_state_dict(torch.load(model_save_path + 'model_%d.pt' % restore_iter))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print('START TEST')\n",
        "count = 0\n",
        "for itest in range(test_images.shape[0]):\n",
        "    if itest % 100 == 0:\n",
        "        print('%d / %d ' %(itest, test_images.shape[0]))\n",
        "\n",
        "    test_img = test_images[itest:itest+1, :, :, :].astype(np.float32)\n",
        "    test_img = (test_img / 255.0 * 2.0) - 1.0\n",
        "\n",
        "    test_img = np.transpose(test_img, (0, 3, 1, 2))\n",
        "    with torch.no_grad():\n",
        "        pred = model(torch.from_numpy(test_img).to(DEVICE))\n",
        "\n",
        "    pred = pred.cpu().numpy()  # [1, 200]\n",
        "    pred = np.reshape(pred, num_class)\n",
        "\n",
        "    f = open('딥머닝_Resnet50_SGD_b64_v2.txt', 'a+')\n",
        "\n",
        "    for ik in range(5):\n",
        "        max_index = np.argmax(pred)\n",
        "        f.write('%d ' % (int(max_index)))\n",
        "        pred[max_index] = -9999\n",
        "\n",
        "    f.write('\\n')\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "UFEzzVTjVZyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cdf8a89-551a-47a4-b6ba-306d26698bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "DATA LOAD\n",
            "img num : 51752 \n",
            "DATA LOAD FINISH\n",
            "START TEST\n",
            "0 / 51752 \n",
            "100 / 51752 \n",
            "200 / 51752 \n",
            "300 / 51752 \n",
            "400 / 51752 \n",
            "500 / 51752 \n",
            "600 / 51752 \n",
            "700 / 51752 \n",
            "800 / 51752 \n",
            "900 / 51752 \n",
            "1000 / 51752 \n",
            "1100 / 51752 \n",
            "1200 / 51752 \n",
            "1300 / 51752 \n",
            "1400 / 51752 \n",
            "1500 / 51752 \n",
            "1600 / 51752 \n",
            "1700 / 51752 \n",
            "1800 / 51752 \n",
            "1900 / 51752 \n",
            "2000 / 51752 \n",
            "2100 / 51752 \n",
            "2200 / 51752 \n",
            "2300 / 51752 \n",
            "2400 / 51752 \n",
            "2500 / 51752 \n",
            "2600 / 51752 \n",
            "2700 / 51752 \n",
            "2800 / 51752 \n",
            "2900 / 51752 \n",
            "3000 / 51752 \n",
            "3100 / 51752 \n",
            "3200 / 51752 \n",
            "3300 / 51752 \n",
            "3400 / 51752 \n",
            "3500 / 51752 \n",
            "3600 / 51752 \n",
            "3700 / 51752 \n",
            "3800 / 51752 \n",
            "3900 / 51752 \n",
            "4000 / 51752 \n",
            "4100 / 51752 \n",
            "4200 / 51752 \n",
            "4300 / 51752 \n",
            "4400 / 51752 \n",
            "4500 / 51752 \n",
            "4600 / 51752 \n",
            "4700 / 51752 \n",
            "4800 / 51752 \n",
            "4900 / 51752 \n",
            "5000 / 51752 \n",
            "5100 / 51752 \n",
            "5200 / 51752 \n",
            "5300 / 51752 \n",
            "5400 / 51752 \n",
            "5500 / 51752 \n",
            "5600 / 51752 \n",
            "5700 / 51752 \n",
            "5800 / 51752 \n",
            "5900 / 51752 \n",
            "6000 / 51752 \n",
            "6100 / 51752 \n",
            "6200 / 51752 \n",
            "6300 / 51752 \n",
            "6400 / 51752 \n",
            "6500 / 51752 \n",
            "6600 / 51752 \n",
            "6700 / 51752 \n",
            "6800 / 51752 \n",
            "6900 / 51752 \n",
            "7000 / 51752 \n",
            "7100 / 51752 \n",
            "7200 / 51752 \n",
            "7300 / 51752 \n",
            "7400 / 51752 \n",
            "7500 / 51752 \n",
            "7600 / 51752 \n",
            "7700 / 51752 \n",
            "7800 / 51752 \n",
            "7900 / 51752 \n",
            "8000 / 51752 \n",
            "8100 / 51752 \n",
            "8200 / 51752 \n",
            "8300 / 51752 \n",
            "8400 / 51752 \n",
            "8500 / 51752 \n",
            "8600 / 51752 \n",
            "8700 / 51752 \n",
            "8800 / 51752 \n",
            "8900 / 51752 \n",
            "9000 / 51752 \n",
            "9100 / 51752 \n",
            "9200 / 51752 \n",
            "9300 / 51752 \n",
            "9400 / 51752 \n",
            "9500 / 51752 \n",
            "9600 / 51752 \n",
            "9700 / 51752 \n",
            "9800 / 51752 \n",
            "9900 / 51752 \n",
            "10000 / 51752 \n",
            "10100 / 51752 \n",
            "10200 / 51752 \n",
            "10300 / 51752 \n",
            "10400 / 51752 \n",
            "10500 / 51752 \n",
            "10600 / 51752 \n",
            "10700 / 51752 \n",
            "10800 / 51752 \n",
            "10900 / 51752 \n",
            "11000 / 51752 \n",
            "11100 / 51752 \n",
            "11200 / 51752 \n",
            "11300 / 51752 \n",
            "11400 / 51752 \n",
            "11500 / 51752 \n",
            "11600 / 51752 \n",
            "11700 / 51752 \n",
            "11800 / 51752 \n",
            "11900 / 51752 \n",
            "12000 / 51752 \n",
            "12100 / 51752 \n",
            "12200 / 51752 \n",
            "12300 / 51752 \n",
            "12400 / 51752 \n",
            "12500 / 51752 \n",
            "12600 / 51752 \n",
            "12700 / 51752 \n",
            "12800 / 51752 \n",
            "12900 / 51752 \n",
            "13000 / 51752 \n",
            "13100 / 51752 \n",
            "13200 / 51752 \n",
            "13300 / 51752 \n",
            "13400 / 51752 \n",
            "13500 / 51752 \n",
            "13600 / 51752 \n",
            "13700 / 51752 \n",
            "13800 / 51752 \n",
            "13900 / 51752 \n",
            "14000 / 51752 \n",
            "14100 / 51752 \n",
            "14200 / 51752 \n",
            "14300 / 51752 \n",
            "14400 / 51752 \n",
            "14500 / 51752 \n",
            "14600 / 51752 \n",
            "14700 / 51752 \n",
            "14800 / 51752 \n",
            "14900 / 51752 \n",
            "15000 / 51752 \n",
            "15100 / 51752 \n",
            "15200 / 51752 \n",
            "15300 / 51752 \n",
            "15400 / 51752 \n",
            "15500 / 51752 \n",
            "15600 / 51752 \n",
            "15700 / 51752 \n",
            "15800 / 51752 \n",
            "15900 / 51752 \n",
            "16000 / 51752 \n",
            "16100 / 51752 \n",
            "16200 / 51752 \n",
            "16300 / 51752 \n",
            "16400 / 51752 \n",
            "16500 / 51752 \n",
            "16600 / 51752 \n",
            "16700 / 51752 \n",
            "16800 / 51752 \n",
            "16900 / 51752 \n",
            "17000 / 51752 \n",
            "17100 / 51752 \n",
            "17200 / 51752 \n",
            "17300 / 51752 \n",
            "17400 / 51752 \n",
            "17500 / 51752 \n",
            "17600 / 51752 \n",
            "17700 / 51752 \n",
            "17800 / 51752 \n",
            "17900 / 51752 \n",
            "18000 / 51752 \n",
            "18100 / 51752 \n",
            "18200 / 51752 \n",
            "18300 / 51752 \n",
            "18400 / 51752 \n",
            "18500 / 51752 \n",
            "18600 / 51752 \n",
            "18700 / 51752 \n",
            "18800 / 51752 \n",
            "18900 / 51752 \n",
            "19000 / 51752 \n",
            "19100 / 51752 \n",
            "19200 / 51752 \n",
            "19300 / 51752 \n",
            "19400 / 51752 \n",
            "19500 / 51752 \n",
            "19600 / 51752 \n",
            "19700 / 51752 \n",
            "19800 / 51752 \n",
            "19900 / 51752 \n",
            "20000 / 51752 \n",
            "20100 / 51752 \n",
            "20200 / 51752 \n",
            "20300 / 51752 \n",
            "20400 / 51752 \n",
            "20500 / 51752 \n",
            "20600 / 51752 \n",
            "20700 / 51752 \n",
            "20800 / 51752 \n",
            "20900 / 51752 \n",
            "21000 / 51752 \n",
            "21100 / 51752 \n",
            "21200 / 51752 \n",
            "21300 / 51752 \n",
            "21400 / 51752 \n",
            "21500 / 51752 \n",
            "21600 / 51752 \n",
            "21700 / 51752 \n",
            "21800 / 51752 \n",
            "21900 / 51752 \n",
            "22000 / 51752 \n",
            "22100 / 51752 \n",
            "22200 / 51752 \n",
            "22300 / 51752 \n",
            "22400 / 51752 \n",
            "22500 / 51752 \n",
            "22600 / 51752 \n",
            "22700 / 51752 \n",
            "22800 / 51752 \n",
            "22900 / 51752 \n",
            "23000 / 51752 \n",
            "23100 / 51752 \n",
            "23200 / 51752 \n",
            "23300 / 51752 \n",
            "23400 / 51752 \n",
            "23500 / 51752 \n",
            "23600 / 51752 \n",
            "23700 / 51752 \n",
            "23800 / 51752 \n",
            "23900 / 51752 \n",
            "24000 / 51752 \n",
            "24100 / 51752 \n",
            "24200 / 51752 \n",
            "24300 / 51752 \n",
            "24400 / 51752 \n",
            "24500 / 51752 \n",
            "24600 / 51752 \n",
            "24700 / 51752 \n",
            "24800 / 51752 \n",
            "24900 / 51752 \n",
            "25000 / 51752 \n",
            "25100 / 51752 \n",
            "25200 / 51752 \n",
            "25300 / 51752 \n",
            "25400 / 51752 \n",
            "25500 / 51752 \n",
            "25600 / 51752 \n",
            "25700 / 51752 \n",
            "25800 / 51752 \n",
            "25900 / 51752 \n",
            "26000 / 51752 \n",
            "26100 / 51752 \n",
            "26200 / 51752 \n",
            "26300 / 51752 \n",
            "26400 / 51752 \n",
            "26500 / 51752 \n",
            "26600 / 51752 \n",
            "26700 / 51752 \n",
            "26800 / 51752 \n",
            "26900 / 51752 \n",
            "27000 / 51752 \n",
            "27100 / 51752 \n",
            "27200 / 51752 \n",
            "27300 / 51752 \n",
            "27400 / 51752 \n",
            "27500 / 51752 \n",
            "27600 / 51752 \n",
            "27700 / 51752 \n",
            "27800 / 51752 \n",
            "27900 / 51752 \n",
            "28000 / 51752 \n",
            "28100 / 51752 \n",
            "28200 / 51752 \n",
            "28300 / 51752 \n",
            "28400 / 51752 \n",
            "28500 / 51752 \n",
            "28600 / 51752 \n",
            "28700 / 51752 \n",
            "28800 / 51752 \n",
            "28900 / 51752 \n",
            "29000 / 51752 \n",
            "29100 / 51752 \n",
            "29200 / 51752 \n",
            "29300 / 51752 \n",
            "29400 / 51752 \n",
            "29500 / 51752 \n",
            "29600 / 51752 \n",
            "29700 / 51752 \n",
            "29800 / 51752 \n",
            "29900 / 51752 \n",
            "30000 / 51752 \n",
            "30100 / 51752 \n",
            "30200 / 51752 \n",
            "30300 / 51752 \n",
            "30400 / 51752 \n",
            "30500 / 51752 \n",
            "30600 / 51752 \n",
            "30700 / 51752 \n",
            "30800 / 51752 \n",
            "30900 / 51752 \n",
            "31000 / 51752 \n",
            "31100 / 51752 \n",
            "31200 / 51752 \n",
            "31300 / 51752 \n",
            "31400 / 51752 \n",
            "31500 / 51752 \n",
            "31600 / 51752 \n",
            "31700 / 51752 \n",
            "31800 / 51752 \n",
            "31900 / 51752 \n",
            "32000 / 51752 \n",
            "32100 / 51752 \n",
            "32200 / 51752 \n",
            "32300 / 51752 \n",
            "32400 / 51752 \n",
            "32500 / 51752 \n",
            "32600 / 51752 \n",
            "32700 / 51752 \n",
            "32800 / 51752 \n",
            "32900 / 51752 \n",
            "33000 / 51752 \n",
            "33100 / 51752 \n",
            "33200 / 51752 \n",
            "33300 / 51752 \n",
            "33400 / 51752 \n",
            "33500 / 51752 \n",
            "33600 / 51752 \n",
            "33700 / 51752 \n",
            "33800 / 51752 \n",
            "33900 / 51752 \n",
            "34000 / 51752 \n",
            "34100 / 51752 \n",
            "34200 / 51752 \n",
            "34300 / 51752 \n",
            "34400 / 51752 \n",
            "34500 / 51752 \n",
            "34600 / 51752 \n",
            "34700 / 51752 \n",
            "34800 / 51752 \n",
            "34900 / 51752 \n",
            "35000 / 51752 \n",
            "35100 / 51752 \n",
            "35200 / 51752 \n",
            "35300 / 51752 \n",
            "35400 / 51752 \n",
            "35500 / 51752 \n",
            "35600 / 51752 \n",
            "35700 / 51752 \n",
            "35800 / 51752 \n",
            "35900 / 51752 \n",
            "36000 / 51752 \n",
            "36100 / 51752 \n",
            "36200 / 51752 \n",
            "36300 / 51752 \n",
            "36400 / 51752 \n",
            "36500 / 51752 \n",
            "36600 / 51752 \n",
            "36700 / 51752 \n",
            "36800 / 51752 \n",
            "36900 / 51752 \n",
            "37000 / 51752 \n",
            "37100 / 51752 \n",
            "37200 / 51752 \n",
            "37300 / 51752 \n",
            "37400 / 51752 \n",
            "37500 / 51752 \n",
            "37600 / 51752 \n",
            "37700 / 51752 \n",
            "37800 / 51752 \n",
            "37900 / 51752 \n",
            "38000 / 51752 \n",
            "38100 / 51752 \n",
            "38200 / 51752 \n",
            "38300 / 51752 \n",
            "38400 / 51752 \n",
            "38500 / 51752 \n",
            "38600 / 51752 \n",
            "38700 / 51752 \n",
            "38800 / 51752 \n",
            "38900 / 51752 \n",
            "39000 / 51752 \n",
            "39100 / 51752 \n",
            "39200 / 51752 \n",
            "39300 / 51752 \n",
            "39400 / 51752 \n",
            "39500 / 51752 \n",
            "39600 / 51752 \n",
            "39700 / 51752 \n",
            "39800 / 51752 \n",
            "39900 / 51752 \n",
            "40000 / 51752 \n",
            "40100 / 51752 \n",
            "40200 / 51752 \n",
            "40300 / 51752 \n",
            "40400 / 51752 \n",
            "40500 / 51752 \n",
            "40600 / 51752 \n",
            "40700 / 51752 \n",
            "40800 / 51752 \n",
            "40900 / 51752 \n",
            "41000 / 51752 \n",
            "41100 / 51752 \n",
            "41200 / 51752 \n",
            "41300 / 51752 \n",
            "41400 / 51752 \n",
            "41500 / 51752 \n",
            "41600 / 51752 \n",
            "41700 / 51752 \n",
            "41800 / 51752 \n",
            "41900 / 51752 \n",
            "42000 / 51752 \n",
            "42100 / 51752 \n",
            "42200 / 51752 \n",
            "42300 / 51752 \n",
            "42400 / 51752 \n",
            "42500 / 51752 \n",
            "42600 / 51752 \n",
            "42700 / 51752 \n",
            "42800 / 51752 \n",
            "42900 / 51752 \n",
            "43000 / 51752 \n",
            "43100 / 51752 \n",
            "43200 / 51752 \n",
            "43300 / 51752 \n",
            "43400 / 51752 \n",
            "43500 / 51752 \n",
            "43600 / 51752 \n",
            "43700 / 51752 \n",
            "43800 / 51752 \n",
            "43900 / 51752 \n",
            "44000 / 51752 \n",
            "44100 / 51752 \n",
            "44200 / 51752 \n",
            "44300 / 51752 \n",
            "44400 / 51752 \n",
            "44500 / 51752 \n",
            "44600 / 51752 \n",
            "44700 / 51752 \n",
            "44800 / 51752 \n",
            "44900 / 51752 \n",
            "45000 / 51752 \n",
            "45100 / 51752 \n",
            "45200 / 51752 \n",
            "45300 / 51752 \n",
            "45400 / 51752 \n",
            "45500 / 51752 \n",
            "45600 / 51752 \n",
            "45700 / 51752 \n",
            "45800 / 51752 \n",
            "45900 / 51752 \n",
            "46000 / 51752 \n",
            "46100 / 51752 \n",
            "46200 / 51752 \n",
            "46300 / 51752 \n",
            "46400 / 51752 \n",
            "46500 / 51752 \n",
            "46600 / 51752 \n",
            "46700 / 51752 \n",
            "46800 / 51752 \n",
            "46900 / 51752 \n",
            "47000 / 51752 \n",
            "47100 / 51752 \n",
            "47200 / 51752 \n",
            "47300 / 51752 \n",
            "47400 / 51752 \n",
            "47500 / 51752 \n",
            "47600 / 51752 \n",
            "47700 / 51752 \n",
            "47800 / 51752 \n",
            "47900 / 51752 \n",
            "48000 / 51752 \n",
            "48100 / 51752 \n",
            "48200 / 51752 \n",
            "48300 / 51752 \n",
            "48400 / 51752 \n",
            "48500 / 51752 \n",
            "48600 / 51752 \n",
            "48700 / 51752 \n",
            "48800 / 51752 \n",
            "48900 / 51752 \n",
            "49000 / 51752 \n",
            "49100 / 51752 \n",
            "49200 / 51752 \n",
            "49300 / 51752 \n",
            "49400 / 51752 \n",
            "49500 / 51752 \n",
            "49600 / 51752 \n",
            "49700 / 51752 \n",
            "49800 / 51752 \n",
            "49900 / 51752 \n",
            "50000 / 51752 \n",
            "50100 / 51752 \n",
            "50200 / 51752 \n",
            "50300 / 51752 \n",
            "50400 / 51752 \n",
            "50500 / 51752 \n",
            "50600 / 51752 \n",
            "50700 / 51752 \n",
            "50800 / 51752 \n",
            "50900 / 51752 \n",
            "51000 / 51752 \n",
            "51100 / 51752 \n",
            "51200 / 51752 \n",
            "51300 / 51752 \n",
            "51400 / 51752 \n",
            "51500 / 51752 \n",
            "51600 / 51752 \n",
            "51700 / 51752 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uCq0BfKCe5nT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}